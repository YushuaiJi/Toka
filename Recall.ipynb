{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaOq-HhZVcTD",
        "outputId": "c691c2d0-77df-4af9-ddb8-78100591515b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Running Lloyd on Huatuo_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.628\n",
            "Running Lloyd on LiveChat_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.434\n",
            "Running Lloyd on deep_96d_10k (d=96, topk=5)...\n",
            "recall: 0.534\n",
            "Running Lloyd on glove_300d_10k (d=3000, topk=5)...\n",
            "recall: 0.42400000000000004\n",
            "Running Lloyd on sift_128d_10k (d=128, topk=5)...\n",
            "recall: 0.538\n",
            "Running CDKM on Huatuo_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.598\n",
            "Running CDKM on LiveChat_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.43199999999999994\n",
            "Running CDKM on deep_96d_10k (d=96, topk=5)...\n",
            "recall: 0.622\n",
            "Running CDKM on glove_300d_10k (d=3000, topk=5)...\n",
            "recall: 0.39399999999999996\n",
            "Running CDKM on sift_128d_10k (d=128, topk=5)...\n",
            "recall: 0.5720000000000001\n",
            "Running BCLS on Huatuo_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.254\n",
            "Running BCLS on LiveChat_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.14400000000000002\n",
            "Running BCLS on deep_96d_10k (d=96, topk=5)...\n",
            "recall: 0.256\n",
            "Running BCLS on glove_300d_10k (d=3000, topk=5)...\n",
            "recall: 0.184\n",
            "Running BCLS on sift_128d_10k (d=128, topk=5)...\n",
            "recall: 0.146\n",
            "Running FCFC on Huatuo_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.10999999999999999\n",
            "Running FCFC on LiveChat_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.07800000000000001\n",
            "Running FCFC on deep_96d_10k (d=96, topk=5)...\n",
            "recall: 0.07600000000000001\n",
            "Running FCFC on glove_300d_10k (d=3000, topk=5)...\n",
            "recall: 0.19400000000000003\n",
            "Running FCFC on sift_128d_10k (d=128, topk=5)...\n",
            "recall: 0.508\n",
            "Running BKNC on Huatuo_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.648\n",
            "Running BKNC on LiveChat_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.412\n",
            "Running BKNC on deep_96d_10k (d=96, topk=5)...\n",
            "recall: 0.53\n",
            "Running BKNC on glove_300d_10k (d=3000, topk=5)...\n",
            "recall: 0.4060000000000001\n",
            "Running BKNC on sift_128d_10k (d=128, topk=5)...\n",
            "recall: 0.54\n",
            "Running MyKMeans on Huatuo_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.10599999999999998\n",
            "Running MyKMeans on LiveChat_1024d_10k (d=1024, topk=5)...\n",
            "recall: 0.094\n",
            "Running MyKMeans on deep_96d_10k (d=96, topk=5)...\n",
            "recall: 0.258\n",
            "Running MyKMeans on glove_300d_10k (d=3000, topk=5)...\n",
            "recall: 0.41800000000000004\n",
            "Running MyKMeans on sift_128d_10k (d=128, topk=5)...\n",
            "recall: 0.546\n",
            "Running Lloyd on Huatuo_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.6249999999999999\n",
            "Running Lloyd on LiveChat_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.41999999999999993\n",
            "Running Lloyd on deep_96d_10k (d=96, topk=10)...\n",
            "recall: 0.536\n",
            "Running Lloyd on glove_300d_10k (d=3000, topk=10)...\n",
            "recall: 0.36900000000000005\n",
            "Running Lloyd on sift_128d_10k (d=128, topk=10)...\n",
            "recall: 0.525\n",
            "Running CDKM on Huatuo_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.586\n",
            "Running CDKM on LiveChat_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.433\n",
            "Running CDKM on deep_96d_10k (d=96, topk=10)...\n",
            "recall: 0.602\n",
            "Running CDKM on glove_300d_10k (d=3000, topk=10)...\n",
            "recall: 0.374\n",
            "Running CDKM on sift_128d_10k (d=128, topk=10)...\n",
            "recall: 0.5589999999999999\n",
            "Running BCLS on Huatuo_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.215\n",
            "Running BCLS on LiveChat_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.14\n",
            "Running BCLS on deep_96d_10k (d=96, topk=10)...\n",
            "recall: 0.251\n",
            "Running BCLS on glove_300d_10k (d=3000, topk=10)...\n",
            "recall: 0.166\n",
            "Running BCLS on sift_128d_10k (d=128, topk=10)...\n",
            "recall: 0.15500000000000003\n",
            "Running FCFC on Huatuo_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.088\n",
            "Running FCFC on LiveChat_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.08599999999999998\n",
            "Running FCFC on deep_96d_10k (d=96, topk=10)...\n",
            "recall: 0.08299999999999999\n",
            "Running FCFC on glove_300d_10k (d=3000, topk=10)...\n",
            "recall: 0.17800000000000002\n",
            "Running FCFC on sift_128d_10k (d=128, topk=10)...\n",
            "recall: 0.514\n",
            "Running BKNC on Huatuo_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.629\n",
            "Running BKNC on LiveChat_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.41200000000000003\n",
            "Running BKNC on deep_96d_10k (d=96, topk=10)...\n",
            "recall: 0.5089999999999999\n",
            "Running BKNC on glove_300d_10k (d=3000, topk=10)...\n",
            "recall: 0.389\n",
            "Running BKNC on sift_128d_10k (d=128, topk=10)...\n",
            "recall: 0.5089999999999999\n",
            "Running MyKMeans on Huatuo_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.09299999999999999\n",
            "Running MyKMeans on LiveChat_1024d_10k (d=1024, topk=10)...\n",
            "recall: 0.081\n",
            "Running MyKMeans on deep_96d_10k (d=96, topk=10)...\n",
            "recall: 0.264\n",
            "Running MyKMeans on glove_300d_10k (d=3000, topk=10)...\n",
            "recall: 0.364\n",
            "Running MyKMeans on sift_128d_10k (d=128, topk=10)...\n",
            "recall: 0.509\n",
            "Running Lloyd on Huatuo_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.6090000000000001\n",
            "Running Lloyd on LiveChat_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.3960000000000001\n",
            "Running Lloyd on deep_96d_10k (d=96, topk=20)...\n",
            "recall: 0.5314999999999999\n",
            "Running Lloyd on glove_300d_10k (d=3000, topk=20)...\n",
            "recall: 0.3395000000000001\n",
            "Running Lloyd on sift_128d_10k (d=128, topk=20)...\n",
            "recall: 0.5025\n",
            "Running CDKM on Huatuo_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.5725\n",
            "Running CDKM on LiveChat_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.4260000000000001\n",
            "Running CDKM on deep_96d_10k (d=96, topk=20)...\n",
            "recall: 0.5945\n",
            "Running CDKM on glove_300d_10k (d=3000, topk=20)...\n",
            "recall: 0.33899999999999997\n",
            "Running CDKM on sift_128d_10k (d=128, topk=20)...\n",
            "recall: 0.5419999999999999\n",
            "Running BCLS on Huatuo_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.19250000000000003\n",
            "Running BCLS on LiveChat_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.12999999999999998\n",
            "Running BCLS on deep_96d_10k (d=96, topk=20)...\n",
            "recall: 0.23550000000000001\n",
            "Running BCLS on glove_300d_10k (d=3000, topk=20)...\n",
            "recall: 0.16099999999999998\n",
            "Running BCLS on sift_128d_10k (d=128, topk=20)...\n",
            "recall: 0.14650000000000002\n",
            "Running FCFC on Huatuo_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.081\n",
            "Running FCFC on LiveChat_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.08650000000000002\n",
            "Running FCFC on deep_96d_10k (d=96, topk=20)...\n",
            "recall: 0.0785\n",
            "Running FCFC on glove_300d_10k (d=3000, topk=20)...\n",
            "recall: 0.1605\n",
            "Running FCFC on sift_128d_10k (d=128, topk=20)...\n",
            "recall: 0.5005000000000002\n",
            "Running BKNC on Huatuo_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.592\n",
            "Running BKNC on LiveChat_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.406\n",
            "Running BKNC on deep_96d_10k (d=96, topk=20)...\n",
            "recall: 0.4975\n",
            "Running BKNC on glove_300d_10k (d=3000, topk=20)...\n",
            "recall: 0.33949999999999997\n",
            "Running BKNC on sift_128d_10k (d=128, topk=20)...\n",
            "recall: 0.4815\n",
            "Running MyKMeans on Huatuo_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.09050000000000002\n",
            "Running MyKMeans on LiveChat_1024d_10k (d=1024, topk=20)...\n",
            "recall: 0.079\n",
            "Running MyKMeans on deep_96d_10k (d=96, topk=20)...\n",
            "recall: 0.2635\n",
            "Running MyKMeans on glove_300d_10k (d=3000, topk=20)...\n",
            "recall: 0.321\n",
            "Running MyKMeans on sift_128d_10k (d=128, topk=20)...\n",
            "recall: 0.48350000000000004\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from scipy.linalg import orth # For creating orthogonal matrices\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeout\n",
        "\n",
        "class FCFC:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data\n",
        "        # k: number of clusters\n",
        "        self.d_features = d # Renamed for clarity, consistent with other classes\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.max_iter = niter # Keep for consistency with existing loop\n",
        "\n",
        "        # Other parameters (some might not be used by this specific FCFC logic but kept for interface)\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu # This FCFC implementation is CPU-based\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "        self.lambda_ = lambda_  # Balance parameter for the objective function in get_distance\n",
        "\n",
        "        # Results storage\n",
        "        self.centroids = None               # Final cluster centroids (k, d_features)\n",
        "        self.labels_ = None                 # Final cluster assignments for each point (n_samples,)\n",
        "        self.runtime_ = None                # Total training time\n",
        "\n",
        "        self.objective_history_ = None      # History of sum_dis (sum of D(i,j) values)\n",
        "        self.sse_history_ = None            # History of Sum of Squared Errors per iteration\n",
        "        self.balance_loss_history_ = None   # History of Balance Loss per iteration\n",
        "\n",
        "        self.final_objective_ = None        # Final value from objective_history_\n",
        "        self.final_sse_ = None              # Final Sum of Squared Errors\n",
        "        self.final_balance_loss_ = None     # Final Balance Loss\n",
        "        self.final_cluster_sizes_ = None    # Final size of each cluster (k,)\n",
        "\n",
        "        self.sse_ = 0\n",
        "        self.balance_loss_ = 0\n",
        "\n",
        "        # For compatibility, self.obj can point to the primary objective history\n",
        "        self.obj = None\n",
        "\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids_arg=None): # Renamed init_centroids to avoid conflict\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility\n",
        "        start_time = time.time()\n",
        "\n",
        "        K = self.k\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "        n_samples, d_features_data = x.shape # n_samples, d in original code\n",
        "\n",
        "        # Initialize arrays for storing per-iteration metrics\n",
        "        # sse_history stores the traditional SSE\n",
        "        # balance_loss_history stores the balance penalty\n",
        "        # objective_value_history stores the sum of D(i,j) which is being minimized directly\n",
        "        sse_history = np.zeros(self.max_iter)\n",
        "        balance_loss_history = np.zeros(self.max_iter)\n",
        "        objective_value_history = np.zeros(self.max_iter) # Corresponds to pre_dis\n",
        "\n",
        "        # Initialize centroids\n",
        "        # If init_centroids_arg is provided, use it, otherwise use random initialization\n",
        "        if init_centroids_arg is not None:\n",
        "            if init_centroids_arg.shape != (K, d_features_data):\n",
        "                raise ValueError(f\"Provided init_centroids shape {init_centroids_arg.shape} \"\n",
        "                                 f\"is not ({K}, {d_features_data})\")\n",
        "            current_centroids = np.copy(init_centroids_arg)\n",
        "        else:\n",
        "            current_centroids = initial_centroid(x, K, n_samples) # Uses np.random internally\n",
        "\n",
        "        # size_cluster is 1*K vector, stores size of each cluster for the get_distance objective\n",
        "        # Initialized to ones to avoid issues if lambda_ > 0 and a cluster is initially empty,\n",
        "        # though it gets updated immediately in the first iteration.\n",
        "        # A more common initialization might be n_samples/K or based on initial assignment.\n",
        "        # Let's base it on an initial quick assignment or n_samples/K to be more robust.\n",
        "        # For simplicity of matching the provided code, it starts with ones and is quickly updated.\n",
        "        current_size_cluster = np.ones(K) # Will be updated after first assignment\n",
        "\n",
        "        current_labels = np.zeros(n_samples, dtype=int) # To store labels for each point\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Assignment step: Calculate D matrix and assign points to clusters\n",
        "            # D(point, cluster_j) = distance(point, centroid_j)^2 + lambda * size_cluster_j\n",
        "            D_matrix = get_distance(x, current_centroids, K, n_samples, d_features_data,\n",
        "                                    current_size_cluster, self.lambda_)\n",
        "\n",
        "            min_dist_to_centroid_plus_balance = np.min(D_matrix, axis=1) # (n_samples,)\n",
        "            assigned_labels = np.argmin(D_matrix, axis=1)           # (n_samples,)\n",
        "            sum_objective_values = np.sum(min_dist_to_centroid_plus_balance)\n",
        "\n",
        "            current_labels = assigned_labels\n",
        "            objective_value_history[i] = sum_objective_values\n",
        "\n",
        "            # Update step: Recalculate centroids and cluster sizes\n",
        "            # current_size_cluster is based on the new assignments\n",
        "            current_size_cluster = np.bincount(current_labels, minlength=K)\n",
        "            current_centroids = get_centroid(x, current_labels, K, n_samples, d_features_data)\n",
        "\n",
        "            # Calculate SSE and Balance Loss for this iteration (for monitoring)\n",
        "            iter_sse = 0\n",
        "            iter_balance_penalty_terms = np.zeros(K)\n",
        "\n",
        "            for j in range(K):\n",
        "                cluster_points = x[current_labels == j, :]\n",
        "                if cluster_points.shape[0] > 0: # If cluster is not empty\n",
        "                    # SSE part: sum of squared distances to its actual centroid\n",
        "                    iter_sse += np.sum(np.sum((cluster_points - current_centroids[j, :])**2, axis=1))\n",
        "                # Balance loss part (using current_size_cluster which is already updated)\n",
        "                iter_balance_penalty_terms[j] = (current_size_cluster[j] - n_samples / K)**2\n",
        "\n",
        "            sse_history[i] = iter_sse\n",
        "            balance_loss_history[i] = np.sum(iter_balance_penalty_terms)\n",
        "\n",
        "            if self.verbose and (i % 5 == 0 or i == self.max_iter -1) :\n",
        "                print(f\"Iter {i+1}/{self.max_iter}: Objective={objective_value_history[i]:.4f}, \"\n",
        "                      f\"SSE={sse_history[i]:.4f}, BalanceLoss={balance_loss_history[i]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        # Store results\n",
        "        self.centroids = current_centroids\n",
        "        self.labels_ = current_labels\n",
        "        self.final_cluster_sizes_ = current_size_cluster\n",
        "\n",
        "        self.objective_history_ = objective_value_history\n",
        "        self.sse_history_ = sse_history\n",
        "        self.balance_loss_history_ = balance_loss_history\n",
        "\n",
        "        self.final_objective_ = objective_value_history[-1]\n",
        "        self.final_sse_ = sse_history[-1]\n",
        "        self.final_balance_loss_ = balance_loss_history[-1]\n",
        "\n",
        "        self.obj = self.objective_history_ # Storing the history of the optimized objective\n",
        "\n",
        "        self.sse_ = self.final_sse_\n",
        "        self.balance_loss_ = self.final_balance_loss_\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"FCFC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final Objective (sum of D(i,j)): {self.final_objective_:.4f}\")\n",
        "            print(f\"Final SSE: {self.final_sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.final_balance_loss_:.4f}\")\n",
        "            print(f\"Final cluster sizes: {self.final_cluster_sizes_}\")\n",
        "\n",
        "class Lloyd:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.centroids = None\n",
        "        self.obj_history_ = None\n",
        "        self.labels_ = None\n",
        "\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "        self.obj = None\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning random point.\")\n",
        "                centroids[j] = data_points[np.random.randint(len(data_points))]\n",
        "\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        start_time = time.time()\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input dimension {x_orig_data.shape[1]} != {self.d}\")\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        x = np.ascontiguousarray(x_orig_data, dtype='float32')\n",
        "\n",
        "        kmeans = faiss.Kmeans(\n",
        "            d=self.d,\n",
        "            k=self.k,\n",
        "            niter=self.niter,\n",
        "            nredo=self.nredo,\n",
        "            verbose=self.verbose,\n",
        "            min_points_per_centroid=self.min_points_per_centroid,\n",
        "            max_points_per_centroid=self.max_points_per_centroid,\n",
        "            seed=self.seed,\n",
        "            gpu=self.gpu,\n",
        "            spherical=self.spherical,\n",
        "            update_index=self.update_index,\n",
        "            frozen_centroids=self.frozen_centroids\n",
        "        )\n",
        "\n",
        "        kmeans.train(x, init_centroids=init_centroids)\n",
        "\n",
        "        _, self.labels_ = kmeans.index.search(x, 1)\n",
        "        self.labels_ = self.labels_.flatten()\n",
        "\n",
        "        self.centroids = kmeans.centroids\n",
        "        self.obj_history_ = kmeans.obj if kmeans.obj is not None and len(kmeans.obj) > 0 else np.zeros(self.niter)\n",
        "        self.obj = kmeans.obj[-1] if kmeans.obj is not None and len(kmeans.obj) > 0 else None\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "\n",
        "        # Print every 5th iteration's objective value\n",
        "        if self.verbose and self.obj_history_ is not None and len(self.obj_history_) > 0:\n",
        "            print(\"\\n--- Objective Value (every 5 iterations) ---\")\n",
        "            for i, val in enumerate(self.obj_history_):\n",
        "                if (i + 1) % 5 == 0 or i == len(self.obj_history_) - 1:\n",
        "                    print(f\"  Iter {i+1:2d}: {val:.6f}\")\n",
        "\n",
        "        final_sse = 0\n",
        "        for i in range(n):\n",
        "            cluster_idx = self.labels_[i]\n",
        "            final_sse += np.sum((x_orig_data[i] - self.centroids[cluster_idx]) ** 2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal = n / self.k\n",
        "            final_balance_loss = np.sum((sizes - ideal) ** 2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Lloyd training finished in {self.runtime_:.4f}s\")\n",
        "            print(f\"Final obj: {self.obj}\")\n",
        "            print(f\"Cluster sizes: {dict(zip(*np.unique(self.labels_, return_counts=True)))}\")\n",
        "            print(f\"SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class BCLS:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False,\n",
        "                 lambda_=1.0): # lambda_ from FCFC, but BCLS uses 'lam' internally\n",
        "        # d: dimensionality of data\n",
        "        # k: number of clusters\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo # Not used by BCLS algorithm itself\n",
        "        self.verbose = verbose\n",
        "        # The following Faiss-like parameters are not directly used by BCLS's core logic:\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.seed = seed\n",
        "        self.lambda_bcls = lambda_ # BCLS specific lambda for sum_Y term in objective\n",
        "                                  # If the lambda_ parameter was meant for this, it's used as 'lam' below.\n",
        "                                  # If it was for something else, then 'lam' needs its own source.\n",
        "                                  # Assuming lambda_ is the 'lam' for BCLS objective.\n",
        "\n",
        "        # Results storage\n",
        "        self.centroids = None       # Will store centroids in original data space\n",
        "        self.obj_history_ = None    # Stores Obj2 from the loop\n",
        "        self.labels_ = None         # Final cluster assignments (0-indexed)\n",
        "        self.Y_final_ = None        # Final Y matrix (one-hot indicators)\n",
        "\n",
        "        # Final metrics\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "\n",
        "        # For compatibility with previous structure if any part expects 'obj'\n",
        "        self.obj = None\n",
        "\n",
        "\n",
        "    def init1(self, n_samples, num_clusters):\n",
        "        \"\"\"\n",
        "        Initializes the Y matrix (n_samples x num_clusters) with one-hot encoding.\n",
        "        Labels are 1 to num_clusters, then converted to 0-indexed for Python.\n",
        "        \"\"\"\n",
        "        # np.random is affected by self.seed if set before calling train\n",
        "        labels_1_indexed = np.random.randint(1, num_clusters + 1, size=n_samples)\n",
        "        F = np.zeros((n_samples, num_clusters))\n",
        "        F[np.arange(n_samples), labels_1_indexed - 1] = 1\n",
        "        # F = csr_matrix(F) # Can be sparse if n and k are very large\n",
        "        return F\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates centroids from data points and their labels.\n",
        "        data_points: (n_samples, n_features) - original or centered\n",
        "        labels: (n_samples,) - 0-indexed\n",
        "        num_clusters: k\n",
        "        data_dim: d\n",
        "        Returns: (num_clusters, data_dim) centroids\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data. Returning zero centroids.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i, :]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning a random data point as its centroid.\")\n",
        "                if len(data_points) > 0:\n",
        "                    # Seed this random choice for consistency if multiple empty clusters\n",
        "                    rng_empty_fallback = np.random.RandomState(self.seed + j + 1000) # Offset seed\n",
        "                    centroids[j] = data_points[rng_empty_fallback.choice(len(data_points)), :]\n",
        "                # else: centroids[j] remains zeros\n",
        "        return centroids\n",
        "\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None): # x_orig_data is n x dim\n",
        "        np.random.seed(self.seed) # Ensure reproducibility for operations within train\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input data feature dimension {x_orig_data.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d}\")\n",
        "\n",
        "        ITER = self.niter\n",
        "        # BCLS Algorithm Hyperparameters (taken from the provided snippet)\n",
        "        gamma = 0.00001  # Regularization for W\n",
        "        lam = self.lambda_bcls # Controls balance term in objective (sum_Y**2)\n",
        "        mu = 0.01        # ALM parameter\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        c = self.k  # number of clusters\n",
        "\n",
        "        # Initialize Y\n",
        "        Y = self.init1(n, c) # Y is n x c\n",
        "\n",
        "        # Center the data (BCLS works with centered data)\n",
        "        meanX = np.mean(x_orig_data, axis=0, keepdims=True) # 1 x dim\n",
        "        x_centered = x_orig_data - meanX # n x dim\n",
        "\n",
        "        # ALM variables\n",
        "        Lambda_alm = np.zeros((n, c)) # Lagrange multipliers for Y - Z = 0\n",
        "        rho = 1.005                # Update factor for mu\n",
        "\n",
        "        # Precompute part of W update\n",
        "        # P_inv = x_centered.T @ x_centered + gamma * np.eye(dim)\n",
        "        # P = np.linalg.inv(P_inv)\n",
        "        # Using pseudo-inverse for potentially better stability if P_inv is singular/ill-conditioned\n",
        "        try:\n",
        "            P = np.linalg.inv(x_centered.T @ x_centered + gamma * np.eye(dim))\n",
        "        except np.linalg.LinAlgError:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Standard inverse failed for P. Using pseudo-inverse.\")\n",
        "            P = np.linalg.pinv(x_centered.T @ x_centered + gamma * np.eye(dim))\n",
        "\n",
        "\n",
        "        obj_history = np.zeros(ITER)\n",
        "        # Optional: if you want to track SSE/BalanceLoss per iteration (on centered data)\n",
        "        # sse_iter_history = np.zeros(ITER)\n",
        "        # balance_loss_iter_history = np.zeros(ITER)\n",
        "\n",
        "\n",
        "        for iter_idx in range(ITER):\n",
        "            # --- Solve W and b ---\n",
        "            # W: dim x c, b: 1 x c\n",
        "            W = P @ (x_centered.T @ Y)\n",
        "            b = np.mean(Y, axis=0, keepdims=True) # Or (1/n) * (np.ones((1,n)) @ Y)\n",
        "\n",
        "            # E = XW + 1b' - Y (Error term for reconstruction using centered X)\n",
        "            # E_recon: n x c\n",
        "            E_recon = x_centered @ W + np.ones((n, 1)) @ b - Y\n",
        "\n",
        "            # --- Solve Z (auxiliary variable for Y) ---\n",
        "            # Z: n x c\n",
        "            # Denominator matrix for Z update:\n",
        "            # Factor = mu**2 + 2 * n * lam * mu  (scalar)\n",
        "            # Coeff_matrix_inv = (-2 * lam * np.ones((n,n)) + (mu + 2 * n * lam) * np.eye(n)) / Factor\n",
        "            # Z = Coeff_matrix_inv @ (mu * Y + Lambda_alm)\n",
        "            # Simpler if Z is updated element-wise or if structure allows.\n",
        "            # The provided formula for Z seems like a direct solution from a specific formulation.\n",
        "            # Let's assume the formula is correct as given:\n",
        "            # Note: (mu**2 + 2 * n * lam * mu) is a scalar.\n",
        "            # The matrix to invert for Z is effectively ( (mu + 2*n*lam)*I - 2*lam*J ), where J is all-ones matrix.\n",
        "            # This matrix has a specific inverse (Sherman-Woodbury).\n",
        "            # For now, using the provided direct calculation:\n",
        "            mat_for_Z_inv_num = -2 * lam * np.ones((n, n)) + (mu + 2 * n * lam) * np.eye(n)\n",
        "            mat_for_Z_inv_den = (mu**2 + 2 * n * lam * mu)\n",
        "            if np.abs(mat_for_Z_inv_den) < 1e-9: # Avoid division by zero\n",
        "                 if self.verbose: print(f\"Warning: Denominator for Z is near zero at iter {iter_idx}\")\n",
        "                 Z = Y # Fallback or handle error\n",
        "            else:\n",
        "                 Z = (mat_for_Z_inv_num / mat_for_Z_inv_den) @ (mu * Y + Lambda_alm)\n",
        "\n",
        "\n",
        "            # --- Solve Y (indicator matrix) ---\n",
        "            # V: n x c\n",
        "            V_update = (1 / (2 + mu)) * (2 * x_centered @ W + 2 * np.ones((n, 1)) @ b + mu * Z - Lambda_alm)\n",
        "\n",
        "            # Update Y by selecting the max element in each row of V_update\n",
        "            current_labels = np.argmax(V_update, axis=1) # n-element array of 0-indexed labels\n",
        "            Y = np.zeros((n, c))\n",
        "            Y[np.arange(n), current_labels] = 1\n",
        "\n",
        "            # --- Update Lambda (Lagrange multipliers) and mu (penalty parameter) for ALM ---\n",
        "            Lambda_alm = Lambda_alm + mu * (Y - Z)\n",
        "            mu = min(mu * rho, 1e5) # Cap mu to avoid very large values\n",
        "\n",
        "            # --- Calculate Objective Value (for centered data) ---\n",
        "            sum_Y_elements = np.sum(Y) # Sum of all elements in Y (should be n if Y is strictly one-hot)\n",
        "            obj_history[iter_idx] = np.trace(E_recon.T @ E_recon) + \\\n",
        "                                    gamma * np.trace(W.T @ W) + \\\n",
        "                                    lam * (sum_Y_elements**2) # Or lam * np.sum( (np.sum(Y, axis=0) - n/c)**2 ) if balance is per cluster size\n",
        "\n",
        "\n",
        "            # --- In-loop SSE and Balance Loss (on centered data, for monitoring if needed) ---\n",
        "            # These are calculated based on current Y and centered data.\n",
        "            # Centroids for centered data: c x dim\n",
        "            temp_centroids_centered = self.compute_centroids_from_data(x_centered, current_labels, c, dim)\n",
        "\n",
        "            sse_iter = 0\n",
        "            for i in range(n):\n",
        "                cluster_idx = current_labels[i]\n",
        "                # Using np.sum for squared norm for clarity with dimensions\n",
        "                sse_iter += np.sum((x_centered[i, :] - temp_centroids_centered[cluster_idx, :])**2)\n",
        "            # sse_iter_history[iter_idx] = sse_iter\n",
        "\n",
        "            cluster_sizes_iter = np.sum(Y, axis=0) # n_elements per cluster (1 x c)\n",
        "            ideal_size_iter = n / c\n",
        "            balance_loss_iter = np.sum((cluster_sizes_iter - ideal_size_iter)**2)\n",
        "            # balance_loss_iter_history[iter_idx] = balance_loss_iter\n",
        "\n",
        "            if self.verbose and (iter_idx % 10 == 0 or iter_idx == ITER -1):\n",
        "                print(f\"Iter {iter_idx+1}/{ITER}, BCLS Obj: {obj_history[iter_idx]:.4f}, \"\n",
        "                      f\"Iter SSE (centered): {sse_iter:.2f}, Iter Bal (centered): {balance_loss_iter:.2f}\")\n",
        "\n",
        "\n",
        "        # --- End of iterations ---\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        # Store final results\n",
        "        self.labels_ = np.argmax(Y, axis=1) # Final 0-indexed labels\n",
        "        self.Y_final_ = Y                   # Final one-hot indicator matrix\n",
        "        self.obj_history_ = obj_history\n",
        "        self.obj = obj_history # Compatibility\n",
        "\n",
        "        # Calculate final centroids in ORIGINAL data space\n",
        "        # Use x_orig_data and self.labels_\n",
        "        final_centroids_orig_space = self.compute_centroids_from_data(x_orig_data, self.labels_, c, dim)\n",
        "        self.centroids = final_centroids_orig_space # Store k x dim centroids\n",
        "\n",
        "        # Calculate final SSE using ORIGINAL data and ORIGINAL space centroids\n",
        "        final_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point_orig = x_orig_data[i, :]\n",
        "                centroid_orig = self.centroids[cluster_idx, :]\n",
        "                final_sse += np.sum((point_orig - centroid_orig)**2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        # Calculate final Balance Loss\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            final_cluster_sizes = np.bincount(self.labels_, minlength=c)\n",
        "            ideal_size = n / c\n",
        "            final_balance_loss = np.sum((final_cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BCLS training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BCLS objective value: {self.obj_history_[-1]:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of final centroids (original space): {self.centroids.shape}\")\n",
        "            print(f\"Final SSE (original space): {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "\n",
        "    def compute_centroids(self, x_transposed, F_indicator):\n",
        "        \"\"\"\n",
        "        Computes centroids.\n",
        "        x_transposed: (dim, n_samples) data matrix (e.g., centered data transposed)\n",
        "        F_indicator: (n_samples, k) one-hot cluster indicator matrix\n",
        "        Returns: (k, dim) centroids\n",
        "        DEPRECATED in favor of compute_centroids_from_data for clarity, but kept if used elsewhere.\n",
        "        This version is slightly different from compute_centroids_from_data input format.\n",
        "        \"\"\"\n",
        "        num_clusters = F_indicator.shape[1]\n",
        "        data_dim = x_transposed.shape[0]\n",
        "        n_samples_check = x_transposed.shape[1]\n",
        "\n",
        "        if F_indicator.shape[0] != n_samples_check:\n",
        "            raise ValueError(\"Mismatch in number of samples between x_transposed and F_indicator.\")\n",
        "\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        # Determine labels from F_indicator\n",
        "        labels = np.argmax(F_indicator, axis=1) # (n_samples,)\n",
        "\n",
        "        for i in range(n_samples_check):\n",
        "            cluster_label = labels[i]\n",
        "            centroids[cluster_label] += x_transposed[:, i] # x_transposed[:, i] is a data point (dim,)\n",
        "            counts[cluster_label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning (compute_centroids): Cluster {j} is empty. Assigning random point.\")\n",
        "                if n_samples_check > 0:\n",
        "                    rng_empty_fallback = np.random.RandomState(self.seed + j + 2000)\n",
        "                    centroids[j] = x_transposed[:, rng_empty_fallback.choice(n_samples_check)]\n",
        "        return centroids\n",
        "\n",
        "class CDKM_PurePy:\n",
        "    def __init__(self, X: np.ndarray, c_true: int, debug: int = 0):\n",
        "        self.X = X.astype(np.float64)  # shape (N, dim)\n",
        "        self.N, self.dim = self.X.shape\n",
        "        self.c_true = c_true\n",
        "        self.debug = debug\n",
        "\n",
        "        self.Y = []             # replicate list of label vectors\n",
        "        self.n_iter_ = []       # number of iterations per replicate\n",
        "\n",
        "        if debug:\n",
        "            print(f\"N = {self.N}, dim = {self.dim}, k = {self.c_true}\")\n",
        "\n",
        "    def opt(self, init_Y: np.ndarray, ITER: int):\n",
        "        \"\"\"\n",
        "        init_Y: (rep, N) array of integer labels\n",
        "        \"\"\"\n",
        "        rep = init_Y.shape[0]\n",
        "        for rep_i in range(rep):\n",
        "            y = init_Y[rep_i].copy()\n",
        "            n_iter = self.opt_once(y, ITER)\n",
        "            self.Y.append(y)\n",
        "            self.n_iter_.append(n_iter)\n",
        "\n",
        "    def opt_once(self, y: np.ndarray, ITER: int) -> int:\n",
        "        \"\"\"\n",
        "        y: shape (N,), initial cluster assignment\n",
        "        \"\"\"\n",
        "        X = self.X\n",
        "        N, dim, c_true = self.N, self.dim, self.c_true\n",
        "\n",
        "        xnorm = np.sum(X**2, axis=1)  # shape (N,)\n",
        "        Sx = np.zeros((dim, c_true))\n",
        "        n = np.zeros(c_true)\n",
        "\n",
        "        for i in range(N):\n",
        "            Sx[:, y[i]] += X[i]\n",
        "            n[y[i]] += 1\n",
        "\n",
        "        s = np.sum(Sx**2, axis=0)  # squared norm of each cluster sum vector\n",
        "\n",
        "        for iter in range(ITER):\n",
        "            converge = True\n",
        "            for i in range(N):\n",
        "                c_old = y[i]\n",
        "                if n[c_old] == 1:\n",
        "                    continue\n",
        "\n",
        "                xi = X[i]\n",
        "                xiSx = xi @ Sx  # (c,)\n",
        "                tmp1 = s + 2 * xiSx + xnorm[i]\n",
        "                tmp1 = tmp1 / (n + 1)\n",
        "                tmp2 = s / n\n",
        "\n",
        "                delta = tmp1 - tmp2\n",
        "                delta[c_old] = s[c_old] / n[c_old] - \\\n",
        "                    (s[c_old] - 2 * xiSx[c_old] + xnorm[i]) / (n[c_old] - 1)\n",
        "\n",
        "                c_new = np.argmax(delta)\n",
        "\n",
        "                if c_new != c_old:\n",
        "                    converge = False\n",
        "                    y[i] = c_new\n",
        "\n",
        "                    Sx[:, c_old] -= xi\n",
        "                    Sx[:, c_new] += xi\n",
        "\n",
        "                    s[c_old] = np.sum(Sx[:, c_old]**2)\n",
        "                    s[c_new] = np.sum(Sx[:, c_new]**2)\n",
        "\n",
        "                    n[c_old] -= 1\n",
        "                    n[c_new] += 1\n",
        "\n",
        "                if self.debug and i % 10000 == 0:\n",
        "                    print(f\"i = {i}\")\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"iter = {iter}\")\n",
        "\n",
        "            if converge:\n",
        "                break\n",
        "\n",
        "        # if iter + 1 == ITER:\n",
        "            # print(\"not converge\")\n",
        "\n",
        "        return iter + 1\n",
        "\n",
        "    @property\n",
        "    def y_pre(self):\n",
        "        return self.Y\n",
        "\n",
        "\n",
        "class CDKM:\n",
        "    def __init__(self, d, k, niter=200, nredo=10, verbose=False, seed=1234, debug=0):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.seed = seed\n",
        "        self.debug = debug\n",
        "        self.centroids = None\n",
        "        self.labels_ = None\n",
        "        self.Y_final_ = None\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "        self.n_iter_ = None\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        np.random.seed(self.seed)\n",
        "        start_time = time.time()\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        if dim != self.d:\n",
        "            raise ValueError(f\"Data dimension {dim} does not match expected {self.d}.\")\n",
        "\n",
        "        if init_centroids is None:\n",
        "            init_Y = initial_Y(x_orig_data, self.k, self.nredo, \"random\")\n",
        "        else:\n",
        "            init_Y = init_centroids\n",
        "\n",
        "        model = CDKM_PurePy(x_orig_data, self.k, debug=self.debug)\n",
        "        model.opt(init_Y, ITER=self.niter)\n",
        "        Y = model.y_pre\n",
        "        self.n_iter_ = model.n_iter_\n",
        "\n",
        "        centroids = compute_cluster_centers_cdkm(x_orig_data, Y)\n",
        "        labels = np.argmax(one_hot(Y[0], self.k), axis=1)\n",
        "\n",
        "        # Compute SSE\n",
        "        sse = np.sum((x_orig_data - centroids[labels])**2)\n",
        "\n",
        "        # Compute balance loss\n",
        "        counts = np.bincount(labels, minlength=self.k)\n",
        "        ideal_size = n / self.k\n",
        "        balance_loss = np.sum((counts - ideal_size)**2)\n",
        "\n",
        "        self.Y_final_ = one_hot(Y[0], self.k)\n",
        "        self.centroids = centroids\n",
        "        self.labels_ = labels\n",
        "        self.sse_ = sse\n",
        "        self.balance_loss_ = balance_loss\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"CDKM finished in {self.runtime_:.4f}s; \"\n",
        "                  f\"SSE = {self.sse_:.4f}; \"\n",
        "                  f\"Balance Loss = {self.balance_loss_:.4f}; \"\n",
        "                  f\"Iterations = {self.n_iter_}\")\n",
        "\n",
        "class BKNC:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=False,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data (n_features)\n",
        "        # k: number of clusters (c in BKNC)\n",
        "        self.d_features = d\n",
        "        self.k = k  # c in BKNC\n",
        "        self.niter = niter # Niter in BKNC\n",
        "        self.lambda_ = lambda_ # lambda in BKNC\n",
        "        self.seed = seed\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Other Faiss Kmeans parameters - not directly used by BKNC logic\n",
        "        self.nredo = nredo\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu # BKNC as implemented here is CPU-only\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        # BKNC specific results\n",
        "        self.F_ = None          # The F matrix from BKNC (n_samples x k)\n",
        "        self.R_ = None          # The R matrix (k x k)\n",
        "        self.Y_ = None          # The Y matrix (one-hot labels, n_samples x k)\n",
        "        self.labels_ = None     # Final cluster assignments (idx, shape: n_samples)\n",
        "        self.obj_history_ = []  # History of the objective function trace(F'X_m'X_mF)\n",
        "        self.final_obj_ = None\n",
        "        self.runtime_ = 0\n",
        "\n",
        "        # For compatibility with original FCFC structure\n",
        "        self.centroids = None # Will be populated with cluster means\n",
        "        self.obj = None # Can store obj_history_ here\n",
        "\n",
        "        # Final metrics as requested\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "\n",
        "\n",
        "    def _initialize_Y_bknc(self, n_samples, c):\n",
        "        \"\"\"\n",
        "        Equivalent to MATLAB's init function for Y.\n",
        "        Creates an n_samples x c one-hot encoded matrix from random labels.\n",
        "        \"\"\"\n",
        "        # labels are 0 to c-1\n",
        "        # This internal seeding should be fine as long as the main train method sets the overall seed.\n",
        "        # If this method were called multiple times independently *within* one train call,\n",
        "        # and expected different Ys, then it would need a different seeding strategy.\n",
        "        # For now, it's called once per train.\n",
        "        labels = np.random.randint(0, c, size=n_samples)\n",
        "        Y = np.zeros((n_samples, c), dtype=int)\n",
        "        Y[np.arange(n_samples), labels] = 1\n",
        "        return Y\n",
        "\n",
        "    def _calculate_cluster_centroids(self, data, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates the mean of points in each cluster.\n",
        "        data: (n_samples, n_features)\n",
        "        labels: (n_samples,)\n",
        "        num_clusters: k\n",
        "        data_dim: d_features\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim))\n",
        "        if labels is None: # Should not happen if called after labels are set\n",
        "             if self.verbose:\n",
        "                print(\"Warning: Labels are None in _calculate_cluster_centroids. Returning zero centroids.\")\n",
        "             return centroids\n",
        "\n",
        "        for i in range(num_clusters):\n",
        "            cluster_points = data[labels == i]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[i] = np.mean(cluster_points, axis=0)\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {i} is empty during centroid calculation. Assigning a random data point as its centroid.\")\n",
        "                if len(data) > 0:\n",
        "                    # Use a random number generator seeded by self.seed for consistent fallback\n",
        "                    rng_fallback = np.random.RandomState(self.seed + i) # Add i for variety if multiple fallbacks\n",
        "                    centroids[i] = data[rng_fallback.choice(len(data))]\n",
        "                else: # No data points at all (edge case)\n",
        "                    centroids[i] = np.zeros(data_dim)\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Implements the BKNC algorithm.\n",
        "        x: data matrix (n_samples, n_features)\n",
        "        weights: Not used by BKNC.\n",
        "        init_centroids: Not used by BKNC.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility for the entire train method\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "\n",
        "        X_m = x.T  # (n_features, n_samples) - X_m is the MATLAB-like X\n",
        "        n_features_internal, n_samples = X_m.shape # n_features_internal is self.d_features\n",
        "        c = self.k # Number of clusters\n",
        "\n",
        "        # Initialize Y (n_samples, c)\n",
        "        # _initialize_Y_bknc uses np.random, which is now seeded by self.seed\n",
        "        Y = self._initialize_Y_bknc(n_samples, c)\n",
        "\n",
        "        # Initialize R (c, c) as a random orthogonal matrix\n",
        "        # np.random.rand is also affected by the global seed set above\n",
        "        R = orth(np.random.rand(c, c))\n",
        "\n",
        "        obj_log = np.zeros(self.niter)\n",
        "\n",
        "        # F_loop initialization is also seeded\n",
        "        for iter_num in range(self.niter):\n",
        "            F_loop = orth(np.random.rand(n_samples, c))\n",
        "            G = Y @ R.T\n",
        "\n",
        "            for _ in range(10):\n",
        "                TempM_F = X_m @ F_loop\n",
        "                M_calc_F = 2 * X_m.T @ TempM_F + self.lambda_ * G\n",
        "                U_f, _, Vh_f = np.linalg.svd(M_calc_F, full_matrices=False)\n",
        "                F_loop = U_f @ Vh_f\n",
        "            F_current = F_loop\n",
        "\n",
        "            N_calc_R = F_current.T @ Y\n",
        "            U_r, _, Vh_r = np.linalg.svd(N_calc_R, full_matrices=False)\n",
        "            R = U_r @ Vh_r\n",
        "\n",
        "            P_calc_Y = R.T @ F_current.T\n",
        "            idx = np.argmax(P_calc_Y, axis=0)\n",
        "            Y = np.zeros((n_samples, c), dtype=int)\n",
        "            Y[np.arange(n_samples), idx] = 1\n",
        "\n",
        "            TempF_obj = X_m @ F_current\n",
        "            obj_log[iter_num] = np.trace(TempF_obj.T @ TempF_obj)\n",
        "\n",
        "            if self.verbose and (iter_num % 5 == 0 or iter_num == self.niter -1):\n",
        "                print(f\"Iter {iter_num+1}/{self.niter}, BKNC Obj: {obj_log[iter_num]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time()\n",
        "\n",
        "        # Store BKNC results\n",
        "        self.F_ = F_current\n",
        "        self.R_ = R\n",
        "        self.Y_ = Y # This is the one-hot version of labels from the last iteration\n",
        "        self.labels_ = idx # finalInd in MATLAB (0-indexed labels)\n",
        "        self.obj_history_ = obj_log\n",
        "        self.final_obj_ = obj_log[-1]\n",
        "        self.obj = self.obj_history_ # Compatibility\n",
        "\n",
        "        # --- Calculate final centroids, SSE, and Balance Loss ---\n",
        "        # self.centroids are calculated based on original data `x` and final `self.labels_`\n",
        "        self.centroids = self._calculate_cluster_centroids(x, self.labels_, self.k, self.d_features)\n",
        "\n",
        "        # Calculate SSE\n",
        "        current_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n_samples):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point = x[i, :]\n",
        "                centroid_val = self.centroids[cluster_idx, :]\n",
        "                current_sse += np.sum((point - centroid_val)**2) # Squared Euclidean distance\n",
        "        self.sse_ = current_sse\n",
        "\n",
        "        # Calculate Balance Loss\n",
        "        current_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            cluster_sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal_size = n_samples / self.k\n",
        "            current_balance_loss = np.sum((cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = current_balance_loss\n",
        "\n",
        "        # Final runtime calculation\n",
        "        self.runtime_ = time.time() - start_time # Corrected runtime calculation\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BKNC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BKNC objective (trace): {self.final_obj_:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of calculated centroids: {self.centroids.shape}\")\n",
        "            print(f\"Final SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class MyKMeans:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=False,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data (n_features)\n",
        "        # k: number of clusters (c in BKNC)\n",
        "        self.d_features = d\n",
        "        self.k = k  # c in BKNC\n",
        "        self.niter = niter # Niter in BKNC\n",
        "        self.lambda_ = lambda_ # lambda in BKNC\n",
        "        self.lambda_reformed = (1-lambda_)/lambda_\n",
        "        self.seed = seed\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Other Faiss Kmeans parameters - not directly used by BKNC logic\n",
        "        self.nredo = nredo\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu # BKNC as implemented here is CPU-only\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        # BKNC specific results\n",
        "        self.F_ = None          # The F matrix from BKNC (n_samples x k)\n",
        "        self.R_ = None          # The R matrix (k x k)\n",
        "        self.Y_ = None          # The Y matrix (one-hot labels, n_samples x k)\n",
        "        self.labels_ = None     # Final cluster assignments (idx, shape: n_samples)\n",
        "        self.obj_history_ = []  # History of the objective function trace(F'X_m'X_mF)\n",
        "        self.final_obj_ = None\n",
        "        self.runtime_ = 0\n",
        "\n",
        "        # For compatibility with original FCFC structure\n",
        "        self.centroids = None # Will be populated with cluster means\n",
        "        self.obj = None # Can store obj_history_ here\n",
        "\n",
        "        # Final metrics as requested\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "\n",
        "\n",
        "    def _initialize_Y_bknc(self, n_samples, c):\n",
        "        \"\"\"\n",
        "        Equivalent to MATLAB's init function for Y.\n",
        "        Creates an n_samples x c one-hot encoded matrix from random labels.\n",
        "        \"\"\"\n",
        "        # labels are 0 to c-1\n",
        "        # This internal seeding should be fine as long as the main train method sets the overall seed.\n",
        "        # If this method were called multiple times independently *within* one train call,\n",
        "        # and expected different Ys, then it would need a different seeding strategy.\n",
        "        # For now, it's called once per train.\n",
        "        labels = np.random.randint(0, c, size=n_samples)\n",
        "        Y = np.zeros((n_samples, c), dtype=int)\n",
        "        Y[np.arange(n_samples), labels] = 1\n",
        "        return Y\n",
        "\n",
        "    def _calculate_cluster_centroids(self, data, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates the mean of points in each cluster.\n",
        "        data: (n_samples, n_features)\n",
        "        labels: (n_samples,)\n",
        "        num_clusters: k\n",
        "        data_dim: d_features\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim))\n",
        "        if labels is None: # Should not happen if called after labels are set\n",
        "             if self.verbose:\n",
        "                print(\"Warning: Labels are None in _calculate_cluster_centroids. Returning zero centroids.\")\n",
        "             return centroids\n",
        "\n",
        "        for i in range(num_clusters):\n",
        "            cluster_points = data[labels == i]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[i] = np.mean(cluster_points, axis=0)\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {i} is empty during centroid calculation. Assigning a random data point as its centroid.\")\n",
        "                if len(data) > 0:\n",
        "                    # Use a random number generator seeded by self.seed for consistent fallback\n",
        "                    rng_fallback = np.random.RandomState(self.seed + i) # Add i for variety if multiple fallbacks\n",
        "                    centroids[i] = data[rng_fallback.choice(len(data))]\n",
        "                else: # No data points at all (edge case)\n",
        "                    centroids[i] = np.zeros(data_dim)\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Implements the BKNC algorithm.\n",
        "        x: data matrix (n_samples, n_features)\n",
        "        weights: Not used by BKNC.\n",
        "        init_centroids: Not used by BKNC.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility for the entire train method\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "\n",
        "        X_m = x.T  # (n_features, n_samples) - X_m is the MATLAB-like X\n",
        "        n_features_internal, n_samples = X_m.shape # n_features_internal is self.d_features\n",
        "        c = self.k # Number of clusters\n",
        "\n",
        "        # Initialize Y (n_samples, c)\n",
        "        # _initialize_Y_bknc uses np.random, which is now seeded by self.seed\n",
        "        Y = self._initialize_Y_bknc(n_samples, c)\n",
        "\n",
        "        # Initialize R (c, c) as a random orthogonal matrix\n",
        "        # np.random.rand is also affected by the global seed set above\n",
        "        R = orth(np.random.rand(c, c))\n",
        "\n",
        "        obj_log = np.zeros(self.niter)\n",
        "\n",
        "        # F_loop initialization is also seeded\n",
        "        for iter_num in range(self.niter):\n",
        "            F_loop = orth(np.random.rand(n_samples, c))\n",
        "            G = Y @ R.T\n",
        "\n",
        "            for _ in range(10):\n",
        "                TempM_F = X_m @ F_loop\n",
        "                M_calc_F = 2 * X_m.T @ TempM_F + self.lambda_reformed * G\n",
        "                U_f, _, Vh_f = np.linalg.svd(M_calc_F, full_matrices=False)\n",
        "                F_loop = U_f @ Vh_f\n",
        "            F_current = F_loop\n",
        "\n",
        "            N_calc_R = F_current.T @ Y\n",
        "            U_r, _, Vh_r = np.linalg.svd(N_calc_R, full_matrices=False)\n",
        "            R = U_r @ Vh_r\n",
        "\n",
        "            P_calc_Y = R.T @ F_current.T\n",
        "            idx = np.argmax(P_calc_Y, axis=0)\n",
        "            Y = np.zeros((n_samples, c), dtype=int)\n",
        "            Y[np.arange(n_samples), idx] = 1\n",
        "\n",
        "            TempF_obj = X_m @ F_current\n",
        "            obj_log[iter_num] = np.trace(TempF_obj.T @ TempF_obj)\n",
        "\n",
        "            if self.verbose and (iter_num % 5 == 0 or iter_num == self.niter -1):\n",
        "                print(f\"Iter {iter_num+1}/{self.niter}, BKNC Obj: {obj_log[iter_num]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time()\n",
        "\n",
        "        # Store BKNC results\n",
        "        self.F_ = F_current\n",
        "        self.R_ = R\n",
        "        self.Y_ = Y # This is the one-hot version of labels from the last iteration\n",
        "        self.labels_ = idx # finalInd in MATLAB (0-indexed labels)\n",
        "        self.obj_history_ = obj_log\n",
        "        self.final_obj_ = obj_log[-1]\n",
        "        self.obj = self.obj_history_ # Compatibility\n",
        "\n",
        "        # --- Calculate final centroids, SSE, and Balance Loss ---\n",
        "        # self.centroids are calculated based on original data `x` and final `self.labels_`\n",
        "        self.centroids = self._calculate_cluster_centroids(x, self.labels_, self.k, self.d_features)\n",
        "\n",
        "        # Calculate SSE\n",
        "        current_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n_samples):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point = x[i, :]\n",
        "                centroid_val = self.centroids[cluster_idx, :]\n",
        "                current_sse += np.sum((point - centroid_val)**2) # Squared Euclidean distance\n",
        "        self.sse_ = current_sse\n",
        "\n",
        "        # Calculate Balance Loss\n",
        "        current_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            cluster_sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal_size = n_samples / self.k\n",
        "            current_balance_loss = np.sum((cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = current_balance_loss\n",
        "\n",
        "        # Final runtime calculation\n",
        "        self.runtime_ = time.time() - start_time # Corrected runtime calculation\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BKNC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BKNC objective (trace): {self.final_obj_:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of calculated centroids: {self.centroids.shape}\")\n",
        "            print(f\"Final SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "# Helper functions (moved outside the class, or could be static methods)\n",
        "def initial_Y(X, c, rep, way=\"random\"):\n",
        "        N = X.shape[0]\n",
        "        Y = np.zeros((rep, N), dtype=np.int32)\n",
        "\n",
        "        if way == \"random\":\n",
        "            for rep_i in range(rep):\n",
        "                Y[rep_i] = np.random.randint(0, c, N)\n",
        "\n",
        "        elif way == \"k-means++\":\n",
        "            for rep_i in range(rep):\n",
        "                Y[rep_i] = KMeans(n_clusters=c, init=\"k-means++\", n_init=1, max_iter=1).fit(X).labels_\n",
        "\n",
        "        else:\n",
        "            assert 2 == 1\n",
        "\n",
        "        return Y\n",
        "def one_hot(y: np.ndarray, k: int):\n",
        "    n = len(y)\n",
        "    Y = np.zeros((n, k), dtype=np.float32)\n",
        "    Y[np.arange(n), y] = 1.0\n",
        "    return Y\n",
        "def compute_cluster_centers_cdkm(X, Y):\n",
        "    \"\"\"\n",
        "    X: (n, d)\n",
        "    Y: list of cluster label arrays, each of shape (n,)\n",
        "    \"\"\"\n",
        "    y = Y[0]  # shape (n,)\n",
        "    n, k = X.shape[0], np.max(y) + 1\n",
        "    Y0 = np.zeros((n, k), dtype=np.float64)\n",
        "    Y0[np.arange(n), y] = 1.0  # one-hot\n",
        "\n",
        "    weights = np.sum(Y0, axis=0)  # (k,)\n",
        "    weights[weights == 0] = 1e-10\n",
        "\n",
        "    centers = (Y0.T @ X) / weights[:, None]\n",
        "    return centers\n",
        "\n",
        "# kNN ground-truth\n",
        "def get_ground_truth_knn(x, query_indices, topk):\n",
        "    knn = NearestNeighbors(n_neighbors=topk+1).fit(x)\n",
        "    _, indices = knn.kneighbors(x[query_indices])\n",
        "    return indices[:, 1:]\n",
        "\n",
        "# def get_cluster_approx_neighbors(x, labels, query_indices, topk, timeout_per_query=None):\n",
        "#     cluster_dict = defaultdict(list)\n",
        "#     for i, lbl in enumerate(labels):\n",
        "#         cluster_dict[lbl].append(i)\n",
        "\n",
        "#     neighbors = np.zeros((len(query_indices), topk), dtype=int)\n",
        "\n",
        "#     def compute_distances(xi, cluster_points):\n",
        "#         return np.linalg.norm(cluster_points - xi, axis=1)\n",
        "\n",
        "#     for idx, i in enumerate(query_indices):\n",
        "#         label = labels[i]\n",
        "#         same_cluster = [j for j in cluster_dict[label] if j != i]\n",
        "\n",
        "#         if not same_cluster:\n",
        "#             # fallback: \n",
        "#             neighbors[idx] = np.random.choice(len(labels), size=topk, replace=False)\n",
        "#             continue\n",
        "\n",
        "#         cluster_points = x[same_cluster]\n",
        "#         xi = x[i]\n",
        "\n",
        "#         with ThreadPoolExecutor(max_workers=1) as executor:\n",
        "#             future = executor.submit(compute_distances, xi, cluster_points)\n",
        "#             try:\n",
        "#                 distances = future.result(timeout=timeout_per_query)\n",
        "#                 sorted_idx = np.argsort(distances)[:topk]\n",
        "#                 neighbors[idx] = np.array(same_cluster)[sorted_idx]\n",
        "#             except FuturesTimeout:\n",
        "#                 # fallback: \n",
        "#                 neighbors[idx] = np.random.choice(same_cluster, size=topk, replace=True)\n",
        "#                 print(\"Timeout!\")\n",
        "\n",
        "#     return neighbors\n",
        "\n",
        "def get_cluster_approx_neighbors(x, labels, query_indices, topk, max_candidates_per_query=None):\n",
        "    cluster_dict = defaultdict(list)\n",
        "    for i, lbl in enumerate(labels):\n",
        "        cluster_dict[lbl].append(i)\n",
        "\n",
        "    neighbors = np.zeros((len(query_indices), topk), dtype=int)\n",
        "\n",
        "    for idx, i in enumerate(query_indices):\n",
        "        label = labels[i]\n",
        "        same_cluster = [j for j in cluster_dict[label] if j != i]\n",
        "\n",
        "        # \n",
        "        if max_candidates_per_query is not None and len(same_cluster) > max_candidates_per_query:\n",
        "            same_cluster = np.random.choice(same_cluster, size=max_candidates_per_query, replace=False)\n",
        "\n",
        "        if len(same_cluster) == 0:\n",
        "            neighbors[idx] = np.random.choice(len(labels), size=topk, replace=False)\n",
        "            continue\n",
        "\n",
        "        cluster_points = x[same_cluster]\n",
        "        xi = x[i]\n",
        "\n",
        "        distances = np.linalg.norm(cluster_points - xi, axis=1)\n",
        "        sorted_idx = np.argsort(distances)[:topk]\n",
        "        neighbors[idx] = np.array(same_cluster)[sorted_idx]\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "# Recall computation\n",
        "def compute_recall(gt_knn, approx_knn):\n",
        "    hits = 0\n",
        "    for i in range(gt_knn.shape[0]):\n",
        "        hits += len(set(gt_knn[i]) & set(approx_knn[i]))\n",
        "    return hits / (gt_knn.shape[0] * gt_knn.shape[1])\n",
        "\n",
        "def get_centroid(data, label, K, n, d_features):\n",
        "    \"\"\"\n",
        "    Update centroids after the assignment phase.\n",
        "    data: (n, d_features)\n",
        "    label: (n,)\n",
        "    K: number of clusters\n",
        "    n: number of samples\n",
        "    d_features: number of features\n",
        "    \"\"\"\n",
        "    centroids = np.zeros((K, d_features))\n",
        "    for k_idx in range(K):\n",
        "        members = (label == k_idx)\n",
        "        if np.any(members):\n",
        "            # Np.sum on boolean array members gives count of True values\n",
        "            centroids[k_idx, :] = np.sum(data[members, :], axis=0) / np.sum(members)\n",
        "        else:\n",
        "            # Handle empty cluster: assign a random point from data\n",
        "            # This random choice is now affected by the seed set in train()\n",
        "            if n > 0 : # Ensure data is not empty\n",
        "                 centroids[k_idx, :] = data[np.random.choice(n), :]\n",
        "            # else: centroid remains zeros if data is empty (edge case)\n",
        "    return centroids\n",
        "\n",
        "\n",
        "def get_distance(data, centroids, K, n, d_features, size_cluster, lambda_param):\n",
        "    \"\"\"\n",
        "    Objective function term for assignment:\n",
        "    D(i,j) = distance(i-th data point, j-th centroid)^2 + lambda_param * size_of_jth_cluster\n",
        "    data: (n, d_features)\n",
        "    centroids: (K, d_features)\n",
        "    size_cluster: (K,) - current size of each cluster\n",
        "    lambda_param: balance weight\n",
        "    Returns: D_matrix (n, K)\n",
        "    \"\"\"\n",
        "    D_matrix = np.zeros((n, K))\n",
        "    for k_idx in range(K):\n",
        "        # Squared Euclidean distance\n",
        "        dist_sq = np.sum((data - centroids[k_idx, :])**2, axis=1)\n",
        "        D_matrix[:, k_idx] = dist_sq + lambda_param * size_cluster[k_idx]\n",
        "    return D_matrix\n",
        "\n",
        "\n",
        "def initial_centroid(x_data, K, n_samples):\n",
        "    \"\"\"\n",
        "    Initialize centroids randomly by choosing K unique points from the data.\n",
        "    x_data: (n_samples, d_features)\n",
        "    K: number of clusters\n",
        "    n_samples: number of samples\n",
        "    \"\"\"\n",
        "    if K > n_samples:\n",
        "        raise ValueError(\"K (number of clusters) cannot be greater than n_samples.\")\n",
        "    # This random choice is now affected by the seed set in train()\n",
        "    indices = np.random.choice(n_samples, K, replace=False)\n",
        "    return x_data[indices, :]\n",
        "\n",
        "# \n",
        "def load_data_chunked(path, dtype='float32', chunksize=1000):\n",
        "    \"\"\"\"\"\"\n",
        "    chunks = []\n",
        "    for chunk in pd.read_csv(path, header=None, chunksize=chunksize):\n",
        "        chunks.append(chunk.astype(dtype))\n",
        "    return np.concatenate(chunks, axis=0)\n",
        "\n",
        "# Main experiment for recall@k repeated n_runs times\n",
        "def run_recall_experiment(model_class, model_name, x, k, query_count, topk=10, n_runs=10, max_candidates_per_query=800):\n",
        "    n_samples = x.shape[0]\n",
        "    recalls = []\n",
        "    # query_indices = np.random.choice(n_samples, size=query_count, replace=False)\n",
        "    # gt_knn = get_ground_truth_knn(x, query_indices, topk)\n",
        "    if model_name in [\"FCFC\", \"BCLS\", \"BKNC\", \"MyKMeans\"]:\n",
        "        model = model_class(d=x.shape[1], k=k, niter=10, lambda_=0.1, seed=1234, verbose=False)\n",
        "    else:\n",
        "        model = model_class(d=x.shape[1], k=k, niter=10, seed=1234, verbose=False)\n",
        "    model.train(x)\n",
        "    for run in range(n_runs):\n",
        "        #  run  query_indices\n",
        "        np.random.seed(1234 + run)\n",
        "        query_indices = np.random.choice(n_samples, size=query_count, replace=False)\n",
        "        gt_knn = get_ground_truth_knn(x, query_indices, topk)\n",
        "        approx_knn = get_cluster_approx_neighbors(x, model.labels_, query_indices, topk, max_candidates_per_query)\n",
        "        recall = compute_recall(gt_knn, approx_knn)\n",
        "        recalls.append(recall)\n",
        "    print(f\"recall: {np.mean(recalls)}\")\n",
        "    return np.mean(recalls)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # \n",
        "    models = [\n",
        "        (Lloyd, \"Lloyd\"),\n",
        "        (CDKM, \"CDKM\"),\n",
        "        (BCLS, \"BCLS\"),\n",
        "        (FCFC, \"FCFC\"),\n",
        "        (BKNC, \"BKNC\"),\n",
        "        (MyKMeans, \"MyKMeans\")\n",
        "    ]\n",
        "\n",
        "    # Configuration\n",
        "    data_scale = 10000\n",
        "    query_count = 1 # \n",
        "    k_clusters = 10\n",
        "    topks = [5, 10, 20] # \n",
        "    # topks = [5, 10]\n",
        "    n_runs = 100\n",
        "    # n_runs = 20\n",
        "    # timeout_per_query = 0.0001\n",
        "    max_candidates_per_query = data_scale / k_clusters * 0.8\n",
        "\n",
        "    # \n",
        "    datasets = [\n",
        "        (\"/content/sample_data/Huatuo_1024d_10k.csv\", 1024, 0.002),\n",
        "        (\"/content/sample_data/LiveChat_1024d_10k.csv\", 1024, 0.002),\n",
        "        (\"/content/sample_data/deep_96d_10k.csv\", 96, 0.00015),\n",
        "        (\"/content/sample_data/glove_300d_10k.csv\", 3000, 0.0005),\n",
        "        (\"/content/sample_data/sift_128d_10k.csv\", 128, 0.0002)\n",
        "    ]\n",
        "\n",
        "    # Result DataFrame\n",
        "    columns = [\"QueryCount\", \"Method\", \"Huatuo\", \"LiveChat\", \"Deep\", \"Glove\", \"SIFT\"]\n",
        "    results = []\n",
        "\n",
        "    # Run full experiment\n",
        "    for topk in topks:\n",
        "        for model_class, model_name in models:\n",
        "            row = [topk, model_name]\n",
        "            for dataset_path, dim, timeout in datasets:\n",
        "                print(f\"Running {model_name} on {Path(dataset_path).stem} (d={dim}, topk={topk})...\")\n",
        "                X_data = load_data_chunked(dataset_path)\n",
        "                avg_recall = run_recall_experiment(model_class, model_name, X_data, k_clusters, query_count=query_count, topk=topk, n_runs=n_runs, max_candidates_per_query=800)\n",
        "                row.append(round(avg_recall, 4))\n",
        "                del X_data\n",
        "                gc.collect()\n",
        "            results.append(row)\n",
        "\n",
        "    # Save results\n",
        "    recall_df = pd.DataFrame(results, columns=columns)\n",
        "    recall_df.index += 1\n",
        "    recall_df.index.name = \"Row\"\n",
        "    recall_df.to_csv(\"recall_results.csv\", index=False)\n",
        "    recall_df.head(12)"
      ]
    }
  ]
}