{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 更新的runtime\n",
        "!pip install faiss-cpu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from scipy.linalg import orth # For creating orthogonal matrices\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "import csv\n",
        "import os\n",
        "\n",
        "class Lloyd:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.centroids = None\n",
        "        self.obj_history_ = None\n",
        "        self.labels_ = None\n",
        "\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "        self.obj = None\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning random point.\")\n",
        "                centroids[j] = data_points[np.random.randint(len(data_points))]\n",
        "\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        start_time = time.time()\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input dimension {x_orig_data.shape[1]} != {self.d}\")\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        x = np.ascontiguousarray(x_orig_data, dtype='float32')\n",
        "\n",
        "        kmeans = faiss.Kmeans(\n",
        "            d=self.d,\n",
        "            k=self.k,\n",
        "            niter=self.niter,\n",
        "            nredo=self.nredo,\n",
        "            verbose=self.verbose,\n",
        "            min_points_per_centroid=self.min_points_per_centroid,\n",
        "            max_points_per_centroid=self.max_points_per_centroid,\n",
        "            seed=self.seed,\n",
        "            gpu=self.gpu,\n",
        "            spherical=self.spherical,\n",
        "            update_index=self.update_index,\n",
        "            frozen_centroids=self.frozen_centroids\n",
        "        )\n",
        "\n",
        "        kmeans.train(x, init_centroids=init_centroids)\n",
        "\n",
        "        _, self.labels_ = kmeans.index.search(x, 1)\n",
        "        self.labels_ = self.labels_.flatten()\n",
        "\n",
        "        self.centroids = kmeans.centroids\n",
        "        self.obj_history_ = kmeans.obj if kmeans.obj is not None and len(kmeans.obj) > 0 else np.zeros(self.niter)\n",
        "        self.obj = kmeans.obj[-1] if kmeans.obj is not None and len(kmeans.obj) > 0 else None\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "\n",
        "        # Print every 5th iteration's objective value\n",
        "        if self.verbose and self.obj_history_ is not None and len(self.obj_history_) > 0:\n",
        "            print(\"\\n--- Objective Value (every 5 iterations) ---\")\n",
        "            for i, val in enumerate(self.obj_history_):\n",
        "                if (i + 1) % 5 == 0 or i == len(self.obj_history_) - 1:\n",
        "                    print(f\"  Iter {i+1:2d}: {val:.6f}\")\n",
        "\n",
        "        final_sse = 0\n",
        "        for i in range(n):\n",
        "            cluster_idx = self.labels_[i]\n",
        "            final_sse += np.sum((x_orig_data[i] - self.centroids[cluster_idx]) ** 2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal = n / self.k\n",
        "            final_balance_loss = np.sum((sizes - ideal) ** 2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Lloyd training finished in {self.runtime_:.4f}s\")\n",
        "            print(f\"Final obj: {self.obj}\")\n",
        "            print(f\"Cluster sizes: {dict(zip(*np.unique(self.labels_, return_counts=True)))}\")\n",
        "            print(f\"SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class DynamicKMeansCoreset:\n",
        "    def __init__(self, d, k, threshold=100, **lloyd_kwargs):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.threshold = threshold\n",
        "        self.lloyd_kwargs = lloyd_kwargs\n",
        "\n",
        "        self.buffer = [] # 临时存储插入/删除的点ID\n",
        "        self.coreset_tree = defaultdict(list) # 分层存储核心集的数据结构\n",
        "        self.active_points: Dict[str, Tuple[np.ndarray, float]] = {} # 当前活跃的点(未删除)\n",
        "        self.deleted_points = set() # 已删除的点ID集合\n",
        "        self.model = Lloyd(d=d, k=k, **lloyd_kwargs)\n",
        "\n",
        "    def insert(self, point_id: str, vector: np.ndarray, weight: float = 1.0):\n",
        "        self.active_points[point_id] = (np.array(vector, dtype=np.float32), weight)\n",
        "        self.buffer.append(point_id)\n",
        "        if len(self.buffer) >= self.threshold:\n",
        "            self._add_coreset(self.buffer)\n",
        "            self.buffer.clear()\n",
        "\n",
        "    def delete(self, point_id: str):\n",
        "        if point_id in self.active_points:\n",
        "            self.deleted_points.add(point_id)\n",
        "            self.buffer.append(point_id)\n",
        "            if len(self.buffer) >= self.threshold:\n",
        "                self._add_coreset(self.buffer)\n",
        "                self.buffer.clear()\n",
        "\n",
        "    def _add_coreset(self, point_ids: List[str]):\n",
        "        X, weights = [], []\n",
        "        for pid in point_ids:\n",
        "            if pid in self.active_points and pid not in self.deleted_points:\n",
        "                vec, w = self.active_points[pid]\n",
        "                X.append(vec)\n",
        "                weights.append(w)\n",
        "\n",
        "        if not X:\n",
        "            return\n",
        "\n",
        "        X = np.vstack(X).astype(np.float32)  # 将向量堆叠成矩阵\n",
        "        weights = np.array(weights, dtype=np.float32)  # 将权重转为numpy数组\n",
        "        self._merge_into_tree(0, (X, weights))  # 将新核心集合并到树中\n",
        "\n",
        "    def _merge_into_tree(self, level: int, new_coreset: Tuple[np.ndarray, np.ndarray]):\n",
        "        self.coreset_tree[level].append(new_coreset)\n",
        "        if len(self.coreset_tree[level]) > 1:\n",
        "            X1, w1 = self.coreset_tree[level].pop()\n",
        "            X2, w2 = self.coreset_tree[level].pop()\n",
        "            X = np.vstack([X1, X2])\n",
        "            W = np.concatenate([w1, w2])\n",
        "            reducer = Lloyd(d=self.d, k=self.k, **self.lloyd_kwargs)\n",
        "            reducer.train(X, weights=W)\n",
        "            reduced_X = reducer.centroids\n",
        "            reduced_W = np.bincount(reducer.labels_, minlength=self.k).astype(np.float32)\n",
        "            self._merge_into_tree(level + 1, (reduced_X, reduced_W))\n",
        "\n",
        "    def retrain(self):\n",
        "        if self.buffer:\n",
        "            self._add_coreset(self.buffer)\n",
        "            self.buffer.clear()\n",
        "        X_all, W_all = [], []\n",
        "        for level in self.coreset_tree:\n",
        "            for X, W in self.coreset_tree[level]:\n",
        "                X_all.append(X)\n",
        "                W_all.append(W)\n",
        "        if not X_all:\n",
        "            return\n",
        "        X = np.vstack(X_all)\n",
        "        W = np.concatenate(W_all)\n",
        "        mask = W > 0\n",
        "        X = X[mask]\n",
        "        W = W[mask]\n",
        "        self.model.train(X, weights=W)\n",
        "\n",
        "    def get_centroids(self) -> Optional[np.ndarray]:\n",
        "        return self.model.centroids\n",
        "\n",
        "    def get_labels(self) -> Optional[np.ndarray]:\n",
        "        return self.model.labels_\n",
        "\n",
        "    def get_metrics(self) -> Dict[str, float]:\n",
        "        return {\n",
        "            \"sse\": self.model.sse_,\n",
        "            \"balance_loss\": self.model.balance_loss_,\n",
        "            \"runtime\": self.model.runtime_,\n",
        "            \"objective\": self.model.obj\n",
        "        }\n",
        "\n",
        "class OnlineKMeans:\n",
        "    def __init__(self, d, k, init_method='split', verbose=False, seed=0):\n",
        "        self.d, self.k = d, k\n",
        "        self.verbose = verbose\n",
        "        np.random.seed(seed)\n",
        "        self.centroids = None\n",
        "        self.counts = None  # 每簇访问计数\n",
        "\n",
        "    def initialize(self, first_point):\n",
        "        self.centroids = first_point.reshape(1, -1)\n",
        "        self.counts = np.array([1], dtype=int)\n",
        "\n",
        "    def maybe_split(self):\n",
        "        # 找方差最大簇，二分裂\n",
        "        if self.centroids.shape[0] < self.k:\n",
        "            variances = []\n",
        "            for i in range(len(self.counts)):\n",
        "                variances.append(np.random.rand())  # 实际应估方差，这里 placeholder\n",
        "            j = np.argmax(variances)\n",
        "            c = self.centroids[j]\n",
        "            delta = np.random.randn(self.d) * 1e-3\n",
        "            new_c1, new_c2 = c + delta, c - delta\n",
        "            self.centroids[j] = new_c1\n",
        "            self.centroids = np.vstack([self.centroids, new_c2])\n",
        "            self.counts[j] = self.counts[j] // 2\n",
        "            self.counts = np.append(self.counts, self.counts[j])\n",
        "            if self.verbose:\n",
        "                print(f\"Split centroid {j} into 2; now {self.centroids.shape[0]} total.\")\n",
        "\n",
        "    def partial_fit(self, x_new):\n",
        "        if self.centroids is None:\n",
        "            self.initialize(x_new)\n",
        "            return\n",
        "        # 找最近质心\n",
        "        dists = np.linalg.norm(self.centroids - x_new, axis=1)\n",
        "        j = np.argmin(dists)\n",
        "        self.counts[j] += 1\n",
        "        eta = 1.0 / self.counts[j]\n",
        "        self.centroids[j] += eta * (x_new - self.centroids[j])\n",
        "        # 尝试分裂\n",
        "        if self.centroids.shape[0] < self.k:\n",
        "            self.maybe_split()\n",
        "\n",
        "    def get_balance_loss(self):\n",
        "        if self.counts is None or len(self.counts) == 0:\n",
        "            return 0.0\n",
        "        total = np.sum(self.counts)\n",
        "        ideal = total / self.k\n",
        "        return np.sum((self.counts - ideal) ** 2)\n",
        "\n",
        "class MiniBatchKMeans(Lloyd):\n",
        "    def __init__(self, d, k, batch_size=32, iters=100, **kwargs):\n",
        "        super().__init__(d=d, k=k, **kwargs)\n",
        "        self.batch_size = batch_size\n",
        "        self.iters = iters\n",
        "\n",
        "    def partial_fit(self, X_stream):\n",
        "        # X_stream: shape (n_stream, d)\n",
        "        # 初始化质心（如果为空则从 stream 抽 batch 初始化）\n",
        "        if self.centroids is None:\n",
        "            init = X_stream[np.random.choice(len(X_stream), self.k, replace=False)]\n",
        "            self.centroids = init\n",
        "            self.counts = np.zeros(self.k, dtype=int)\n",
        "\n",
        "        for i in range(self.iters):\n",
        "            idx = np.random.choice(len(X_stream), self.batch_size, replace=False)\n",
        "            batch = X_stream[idx]\n",
        "            index = faiss.IndexFlatL2(self.d)\n",
        "            index.add(self.centroids.astype(np.float32))\n",
        "            _, labels = index.search(batch.astype(np.float32), 1)\n",
        "\n",
        "            labels = labels.flatten()\n",
        "            for j in range(self.k):\n",
        "                mask = (labels == j)\n",
        "                m = np.sum(mask)\n",
        "                if m > 0:\n",
        "                    centroid = self.centroids[j]\n",
        "                    mean_batch = batch[mask].mean(axis=0)\n",
        "                    eta = 1.0 / (self.counts[j] + m)\n",
        "                    self.centroids[j] = (1 - eta) * centroid + eta * mean_batch\n",
        "                    self.counts[j] += m\n",
        "        # 设置 labels 未定义，balance/SSE 可后续重算\n",
        "\n",
        "    def get_balance_loss(self):\n",
        "        if getattr(self, 'counts', None) is None:\n",
        "            return None\n",
        "        ideal = self.counts.sum() / self.k\n",
        "        return np.sum((self.counts - ideal)**2)\n",
        "\n",
        "def evaluate_test_sse(test_data, centroids):\n",
        "    dists = np.linalg.norm(test_data[:, None, :] - centroids[None, :, :], axis=2)\n",
        "    nearest = np.argmin(dists, axis=1)\n",
        "    return np.sum((test_data - centroids[nearest])**2)\n",
        "\n",
        "def run_test_update_only_comparison(datasets, k=10, threshold=200):\n",
        "    for ds in datasets:\n",
        "        name = Path(ds[\"train\"]).stem.split(\"_\")[0]\n",
        "        dim = ds[\"dim\"]\n",
        "        train_data = pd.read_csv(ds[\"train\"], header=None).values.astype(np.float32)\n",
        "        test_data = pd.read_csv(ds[\"test\"], header=None).values.astype(np.float32)\n",
        "\n",
        "        print(f\"\\n====== Dataset: {name} (dim={dim}) ======\")\n",
        "\n",
        "        # 🧠 Step 1️⃣: 使用 Lloyd 方法训练 10K 数据，作为所有方法的初始化\n",
        "        lloyd = Lloyd(d=dim, k=k, verbose=False)\n",
        "        lloyd.train(train_data)\n",
        "        print(f\"[Lloyd Init] SSE: {lloyd.sse_:.2f} | BalLoss: {lloyd.balance_loss_:.2f} | Time: {lloyd.runtime_:.2f}s\")\n",
        "\n",
        "        base_centroids = lloyd.centroids.copy()\n",
        "        base_labels = lloyd.labels_.copy()\n",
        "\n",
        "        ### 🎯 方法1：DynamicCoreset（更新 test_data）\n",
        "        dyn = DynamicKMeansCoreset(d=dim, k=k, threshold=threshold, verbose=False)\n",
        "        for i, vec in enumerate(train_data):\n",
        "            dyn.insert(f\"{name}_train_{i}\", vec)\n",
        "        dyn.model.centroids = base_centroids.copy()\n",
        "        dyn.model.labels_ = base_labels.copy()\n",
        "\n",
        "        test_sse_dyn_before = evaluate_test_sse(test_data, base_centroids)\n",
        "        for i, vec in enumerate(test_data):\n",
        "            dyn.insert(f\"{name}_test_{i}\", vec)\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        dyn.retrain()\n",
        "        t1 = time.perf_counter()\n",
        "        test_sse_dyn_after = evaluate_test_sse(test_data, dyn.get_centroids())\n",
        "\n",
        "        print(f\"[Coreset Update] SSE: {dyn.model.sse_:.2f} | BalLoss: {dyn.model.balance_loss_:.2f} | Time: {t1 - t0:.2f}s\")\n",
        "        print(f\"[Coreset Test SSE] Before: {test_sse_dyn_before:.2f} → After: {test_sse_dyn_after:.2f}\")\n",
        "\n",
        "        ### 🟦 方法2：OnlineKMeans（更新 test_data）\n",
        "        online = OnlineKMeans(d=dim, k=k)\n",
        "        online.centroids = base_centroids.copy()\n",
        "        online.counts = np.bincount(base_labels, minlength=k)\n",
        "\n",
        "        test_sse_online_before = evaluate_test_sse(test_data, base_centroids)\n",
        "        t2 = time.perf_counter()\n",
        "        for vec in test_data:\n",
        "            online.partial_fit(vec)\n",
        "        t3 = time.perf_counter()\n",
        "        test_sse_online_after = evaluate_test_sse(test_data, online.centroids)\n",
        "\n",
        "        # 获取更新后的指标\n",
        "        cent = online.centroids\n",
        "        dists = np.linalg.norm(test_data[:,None,:] - cent[None,:,:], axis=2)\n",
        "        online_sse = np.sum((test_data - cent[np.argmin(dists,axis=1)])**2)\n",
        "        online_balance = model.get_balance_loss()\n",
        "\n",
        "        print(f\"[OnlineKMeans Update] SSE: {online_sse:.2f} | BalLoss: {online_balance:.2f} | Time: {t3 - t2:.2f}s\")\n",
        "        print(f\"[OnlineKMeans Test SSE] Before: {test_sse_online_before:.2f} → After: {test_sse_online_after:.2f}\")\n",
        "\n",
        "        ### 🟨 方法3：MiniBatchKMeans（更新 test_data）\n",
        "        mini = MiniBatchKMeans(d=dim, k=k, batch_size=128, iters=10, verbose=False)\n",
        "        mini.centroids = base_centroids.copy()\n",
        "        mini.counts = np.bincount(base_labels, minlength=k)\n",
        "\n",
        "        test_sse_mini_before = evaluate_test_sse(test_data, mini.centroids)\n",
        "        t4 = time.perf_counter()\n",
        "        mini.partial_fit(test_data)\n",
        "        t5 = time.perf_counter()\n",
        "        test_sse_mini_after = evaluate_test_sse(test_data, mini.centroids)\n",
        "\n",
        "        print(f\"[MiniBatchKMeans Update] BalLoss: {mini.get_balance_loss():.2f} | Time: {t5 - t4:.2f}s\")\n",
        "        print(f\"[MiniBatch Test SSE] Before: {test_sse_mini_before:.2f} → After: {test_sse_mini_after:.2f}\")\n",
        "\n",
        "def run_test_update_comparison(datasets, k=10, threshold=200):\n",
        "    # 准备CSV文件存储结果\n",
        "    csv_file = 'runtime_comparison.csv'\n",
        "    headers = ['Dataset', 'Scale', 'Coreset', 'OnlineKMeans', 'MiniBatchKMeans']\n",
        "\n",
        "    # 定义数据集和数据量的固定顺序\n",
        "    DATASET_ORDER = ['Huatuo', 'LiveChat', 'deep', 'glove', 'sift']\n",
        "    SCALE_ORDER = ['100', '500', '1k', '2k', '5k']\n",
        "\n",
        "    # 如果文件不存在，写入表头\n",
        "    if not os.path.exists(csv_file):\n",
        "        with open(csv_file, mode='w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(headers)\n",
        "\n",
        "    # 收集所有结果，稍后按顺序写入\n",
        "    all_results = []\n",
        "\n",
        "    # 按固定顺序处理数据集\n",
        "    for dataset_name in DATASET_ORDER:\n",
        "        # 找到对应的数据集配置\n",
        "        ds = next((d for d in datasets if Path(d[\"train\"]).stem.split(\"_\")[0] == dataset_name), None)\n",
        "        if not ds:\n",
        "            continue\n",
        "\n",
        "        name = dataset_name\n",
        "        dim = ds[\"dim\"]\n",
        "        train_data = pd.read_csv(ds[\"train\"], header=None).values.astype(np.float32)\n",
        "\n",
        "        # 准备不同大小的测试数据集\n",
        "        test_files = {\n",
        "            '100': ds[\"test1\"],\n",
        "            '500': ds[\"test2\"],\n",
        "            '1k': ds[\"test3\"],\n",
        "            '2k': ds[\"test4\"],\n",
        "            '5k': ds[\"test5\"]\n",
        "        }\n",
        "\n",
        "        # 使用Lloyd方法初始化\n",
        "        lloyd = Lloyd(d=dim, k=k, verbose=False)\n",
        "        lloyd.train(train_data)\n",
        "        base_centroids = lloyd.centroids.copy()\n",
        "        base_labels = lloyd.labels_.copy()\n",
        "\n",
        "        # 按数据量顺序处理\n",
        "        for size in SCALE_ORDER:\n",
        "            test_path = test_files.get(size)\n",
        "            if not test_path or not os.path.exists(test_path):\n",
        "                print(f\"警告：测试文件 {test_path} 不存在，跳过\")\n",
        "                continue\n",
        "\n",
        "            test_data = pd.read_csv(test_path, header=None).values.astype(np.float32)\n",
        "            print(f\"\\n====== 数据集: {name} (dim={dim}) | 测试数据量: {size} ======\")\n",
        "\n",
        "            # 存储运行时间的字典\n",
        "            runtime_results = {\n",
        "                'Dataset': name,\n",
        "                'Scale': size,\n",
        "                'Coreset': 0,\n",
        "                'OnlineKMeans': 0,\n",
        "                'MiniBatchKMeans': 0\n",
        "            }\n",
        "\n",
        "            # 方法1: DynamicCoreset\n",
        "            dyn = DynamicKMeansCoreset(d=dim, k=k, threshold=threshold, verbose=False)\n",
        "            for i, vec in enumerate(train_data):\n",
        "                dyn.insert(f\"{name}_train_{i}\", vec)\n",
        "            dyn.model.centroids = base_centroids.copy()\n",
        "            dyn.model.labels_ = base_labels.copy()\n",
        "\n",
        "            t0 = time.perf_counter()\n",
        "            for i, vec in enumerate(test_data):\n",
        "                dyn.insert(f\"{name}_test_{i}\", vec)\n",
        "            dyn.retrain()\n",
        "            t1 = time.perf_counter()\n",
        "            runtime_results['Coreset'] = t1 - t0\n",
        "\n",
        "            # 方法2: OnlineKMeans\n",
        "            online = OnlineKMeans(d=dim, k=k)\n",
        "            online.centroids = base_centroids.copy()\n",
        "            online.counts = np.bincount(base_labels, minlength=k)\n",
        "\n",
        "            t2 = time.perf_counter()\n",
        "            for vec in test_data:\n",
        "                online.partial_fit(vec)\n",
        "            t3 = time.perf_counter()\n",
        "            runtime_results['OnlineKMeans'] = t3 - t2\n",
        "\n",
        "            # 方法3: MiniBatchKMeans\n",
        "            mini = MiniBatchKMeans(d=dim, k=k, batch_size=32, iters=10, verbose=False)\n",
        "            mini.centroids = base_centroids.copy()\n",
        "            mini.counts = np.bincount(base_labels, minlength=k)\n",
        "\n",
        "            t4 = time.perf_counter()\n",
        "            mini.partial_fit(test_data)\n",
        "            t5 = time.perf_counter()\n",
        "            runtime_results['MiniBatchKMeans'] = t5 - t4\n",
        "\n",
        "            # 打印结果\n",
        "            print(f\"[Coreset] 时间: {runtime_results['Coreset']:.4f}s\")\n",
        "            print(f\"[OnlineKMeans] 时间: {runtime_results['OnlineKMeans']:.4f}s\")\n",
        "            print(f\"[MiniBatchKMeans] 时间: {runtime_results['MiniBatchKMeans']:.4f}s\")\n",
        "\n",
        "            # 添加到结果列表\n",
        "            all_results.append([\n",
        "                runtime_results['Dataset'],\n",
        "                runtime_results['Scale'],\n",
        "                runtime_results['Coreset'],\n",
        "                runtime_results['OnlineKMeans'],\n",
        "                runtime_results['MiniBatchKMeans']\n",
        "            ])\n",
        "\n",
        "    # 按顺序写入所有结果\n",
        "    with open(csv_file, mode='w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(headers)\n",
        "        writer.writerows(all_results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    datasets = [\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/Huatuo_1024d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/Huatuo_1024d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/Huatuo_1024d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/Huatuo_1024d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/Huatuo_1024d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/Huatuo_1024d_5k.csv\",\n",
        "            \"dim\": 1024\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/LiveChat_1024d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/LiveChat_1024d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/LiveChat_1024d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/LiveChat_1024d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/LiveChat_1024d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/LiveChat_1024d_5k.csv\",\n",
        "            \"dim\": 1024\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/deep_96d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/deep_96d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/deep_96d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/deep_96d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/deep_96d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/deep_96d_5k.csv\",\n",
        "            \"dim\": 96\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/glove_300d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/glove_300d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/glove_300d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/glove_300d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/glove_300d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/glove_300d_5k.csv\",\n",
        "            \"dim\": 300\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/sift_128d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/sift_128d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/sift_128d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/sift_128d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/sift_128d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/sift_128d_5k.csv\",\n",
        "            \"dim\": 128\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # run_test_update_only_comparison(datasets, k=10, threshold=200)\n",
        "    run_test_update_comparison(datasets, k=10, threshold=200)\n",
        "    print(f\"\\n结果已保存\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWjhAS3fEzKF",
        "outputId": "32f7b194-511b-4f78-f948-c9deb8d4469b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "\n",
            "====== 数据集: Huatuo (dim=1024) | 测试数据量: 100 ======\n",
            "[Coreset] 时间: 0.0082s\n",
            "[OnlineKMeans] 时间: 0.0058s\n",
            "[MiniBatchKMeans] 时间: 0.0087s\n",
            "\n",
            "====== 数据集: Huatuo (dim=1024) | 测试数据量: 500 ======\n",
            "[Coreset] 时间: 0.0344s\n",
            "[OnlineKMeans] 时间: 0.0276s\n",
            "[MiniBatchKMeans] 时间: 0.0098s\n",
            "\n",
            "====== 数据集: Huatuo (dim=1024) | 测试数据量: 1k ======\n",
            "[Coreset] 时间: 0.0710s\n",
            "[OnlineKMeans] 时间: 0.0580s\n",
            "[MiniBatchKMeans] 时间: 0.0121s\n",
            "\n",
            "====== 数据集: Huatuo (dim=1024) | 测试数据量: 2k ======\n",
            "[Coreset] 时间: 0.1445s\n",
            "[OnlineKMeans] 时间: 0.2645s\n",
            "[MiniBatchKMeans] 时间: 0.0133s\n",
            "\n",
            "====== 数据集: Huatuo (dim=1024) | 测试数据量: 5k ======\n",
            "[Coreset] 时间: 0.3669s\n",
            "[OnlineKMeans] 时间: 0.3070s\n",
            "[MiniBatchKMeans] 时间: 0.0139s\n",
            "\n",
            "====== 数据集: LiveChat (dim=1024) | 测试数据量: 100 ======\n",
            "[Coreset] 时间: 0.0211s\n",
            "[OnlineKMeans] 时间: 0.0148s\n",
            "[MiniBatchKMeans] 时间: 0.0143s\n",
            "\n",
            "====== 数据集: LiveChat (dim=1024) | 测试数据量: 500 ======\n",
            "[Coreset] 时间: 0.0344s\n",
            "[OnlineKMeans] 时间: 0.0265s\n",
            "[MiniBatchKMeans] 时间: 0.0095s\n",
            "\n",
            "====== 数据集: LiveChat (dim=1024) | 测试数据量: 1k ======\n",
            "[Coreset] 时间: 0.0697s\n",
            "[OnlineKMeans] 时间: 0.0567s\n",
            "[MiniBatchKMeans] 时间: 0.0111s\n",
            "\n",
            "====== 数据集: LiveChat (dim=1024) | 测试数据量: 2k ======\n",
            "[Coreset] 时间: 0.1605s\n",
            "[OnlineKMeans] 时间: 0.0999s\n",
            "[MiniBatchKMeans] 时间: 0.0123s\n",
            "\n",
            "====== 数据集: LiveChat (dim=1024) | 测试数据量: 5k ======\n",
            "[Coreset] 时间: 0.3420s\n",
            "[OnlineKMeans] 时间: 0.3136s\n",
            "[MiniBatchKMeans] 时间: 0.0150s\n",
            "\n",
            "====== 数据集: deep (dim=96) | 测试数据量: 100 ======\n",
            "[Coreset] 时间: 0.0029s\n",
            "[OnlineKMeans] 时间: 0.0031s\n",
            "[MiniBatchKMeans] 时间: 0.0054s\n",
            "\n",
            "====== 数据集: deep (dim=96) | 测试数据量: 500 ======\n",
            "[Coreset] 时间: 0.0113s\n",
            "[OnlineKMeans] 时间: 0.0153s\n",
            "[MiniBatchKMeans] 时间: 0.0057s\n",
            "\n",
            "====== 数据集: deep (dim=96) | 测试数据量: 1k ======\n",
            "[Coreset] 时间: 0.0234s\n",
            "[OnlineKMeans] 时间: 0.0274s\n",
            "[MiniBatchKMeans] 时间: 0.0065s\n",
            "\n",
            "====== 数据集: deep (dim=96) | 测试数据量: 2k ======\n",
            "[Coreset] 时间: 0.0434s\n",
            "[OnlineKMeans] 时间: 0.0523s\n",
            "[MiniBatchKMeans] 时间: 0.0065s\n",
            "\n",
            "====== 数据集: deep (dim=96) | 测试数据量: 5k ======\n",
            "[Coreset] 时间: 0.1077s\n",
            "[OnlineKMeans] 时间: 0.1244s\n",
            "[MiniBatchKMeans] 时间: 0.0076s\n",
            "\n",
            "====== 数据集: glove (dim=300) | 测试数据量: 100 ======\n",
            "[Coreset] 时间: 0.0042s\n",
            "[OnlineKMeans] 时间: 0.0042s\n",
            "[MiniBatchKMeans] 时间: 0.0061s\n",
            "\n",
            "====== 数据集: glove (dim=300) | 测试数据量: 500 ======\n",
            "[Coreset] 时间: 0.0181s\n",
            "[OnlineKMeans] 时间: 0.0203s\n",
            "[MiniBatchKMeans] 时间: 0.0071s\n",
            "\n",
            "====== 数据集: glove (dim=300) | 测试数据量: 1k ======\n",
            "[Coreset] 时间: 0.0332s\n",
            "[OnlineKMeans] 时间: 0.0352s\n",
            "[MiniBatchKMeans] 时间: 0.0079s\n",
            "\n",
            "====== 数据集: glove (dim=300) | 测试数据量: 2k ======\n",
            "[Coreset] 时间: 0.1647s\n",
            "[OnlineKMeans] 时间: 0.1164s\n",
            "[MiniBatchKMeans] 时间: 0.0120s\n",
            "\n",
            "====== 数据集: glove (dim=300) | 测试数据量: 5k ======\n",
            "[Coreset] 时间: 0.3736s\n",
            "[OnlineKMeans] 时间: 0.3019s\n",
            "[MiniBatchKMeans] 时间: 0.0113s\n",
            "\n",
            "====== 数据集: sift (dim=128) | 测试数据量: 100 ======\n",
            "[Coreset] 时间: 0.0030s\n",
            "[OnlineKMeans] 时间: 0.0031s\n",
            "[MiniBatchKMeans] 时间: 0.0056s\n",
            "\n",
            "====== 数据集: sift (dim=128) | 测试数据量: 500 ======\n",
            "[Coreset] 时间: 0.0119s\n",
            "[OnlineKMeans] 时间: 0.0150s\n",
            "[MiniBatchKMeans] 时间: 0.0063s\n",
            "\n",
            "====== 数据集: sift (dim=128) | 测试数据量: 1k ======\n",
            "[Coreset] 时间: 0.0248s\n",
            "[OnlineKMeans] 时间: 0.0278s\n",
            "[MiniBatchKMeans] 时间: 0.0065s\n",
            "\n",
            "====== 数据集: sift (dim=128) | 测试数据量: 2k ======\n",
            "[Coreset] 时间: 0.0733s\n",
            "[OnlineKMeans] 时间: 0.0535s\n",
            "[MiniBatchKMeans] 时间: 0.0072s\n",
            "\n",
            "====== 数据集: sift (dim=128) | 测试数据量: 5k ======\n",
            "[Coreset] 时间: 0.1135s\n",
            "[OnlineKMeans] 时间: 0.1282s\n",
            "[MiniBatchKMeans] 时间: 0.0080s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 更新的balance loss\n",
        "!pip install faiss-cpu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from scipy.linalg import orth # For creating orthogonal matrices\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "import csv\n",
        "import os\n",
        "\n",
        "class Lloyd:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.centroids = None\n",
        "        self.obj_history_ = None\n",
        "        self.labels_ = None\n",
        "\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "        self.obj = None\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning random point.\")\n",
        "                centroids[j] = data_points[np.random.randint(len(data_points))]\n",
        "\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        start_time = time.time()\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input dimension {x_orig_data.shape[1]} != {self.d}\")\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        x = np.ascontiguousarray(x_orig_data, dtype='float32')\n",
        "\n",
        "        kmeans = faiss.Kmeans(\n",
        "            d=self.d,\n",
        "            k=self.k,\n",
        "            niter=self.niter,\n",
        "            nredo=self.nredo,\n",
        "            verbose=self.verbose,\n",
        "            min_points_per_centroid=self.min_points_per_centroid,\n",
        "            max_points_per_centroid=self.max_points_per_centroid,\n",
        "            seed=self.seed,\n",
        "            gpu=self.gpu,\n",
        "            spherical=self.spherical,\n",
        "            update_index=self.update_index,\n",
        "            frozen_centroids=self.frozen_centroids\n",
        "        )\n",
        "\n",
        "        kmeans.train(x, init_centroids=init_centroids)\n",
        "\n",
        "        _, self.labels_ = kmeans.index.search(x, 1)\n",
        "        self.labels_ = self.labels_.flatten()\n",
        "\n",
        "        self.centroids = kmeans.centroids\n",
        "        self.obj_history_ = kmeans.obj if kmeans.obj is not None and len(kmeans.obj) > 0 else np.zeros(self.niter)\n",
        "        self.obj = kmeans.obj[-1] if kmeans.obj is not None and len(kmeans.obj) > 0 else None\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "\n",
        "        # Print every 5th iteration's objective value\n",
        "        if self.verbose and self.obj_history_ is not None and len(self.obj_history_) > 0:\n",
        "            print(\"\\n--- Objective Value (every 5 iterations) ---\")\n",
        "            for i, val in enumerate(self.obj_history_):\n",
        "                if (i + 1) % 5 == 0 or i == len(self.obj_history_) - 1:\n",
        "                    print(f\"  Iter {i+1:2d}: {val:.6f}\")\n",
        "\n",
        "        final_sse = 0\n",
        "        for i in range(n):\n",
        "            cluster_idx = self.labels_[i]\n",
        "            final_sse += np.sum((x_orig_data[i] - self.centroids[cluster_idx]) ** 2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal = n / self.k\n",
        "            final_balance_loss = np.sum((sizes - ideal) ** 2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Lloyd training finished in {self.runtime_:.4f}s\")\n",
        "            print(f\"Final obj: {self.obj}\")\n",
        "            print(f\"Cluster sizes: {dict(zip(*np.unique(self.labels_, return_counts=True)))}\")\n",
        "            print(f\"SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class DynamicKMeansCoreset:\n",
        "    def __init__(self, d, k, threshold=100, **lloyd_kwargs):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.threshold = threshold\n",
        "        self.lloyd_kwargs = lloyd_kwargs\n",
        "\n",
        "        self.buffer = [] # 临时存储插入/删除的点ID\n",
        "        self.coreset_tree = defaultdict(list) # 分层存储核心集的数据结构\n",
        "        self.active_points: Dict[str, Tuple[np.ndarray, float]] = {} # 当前活跃的点(未删除)\n",
        "        self.deleted_points = set() # 已删除的点ID集合\n",
        "        self.model = Lloyd(d=d, k=k, **lloyd_kwargs)\n",
        "\n",
        "    def insert(self, point_id: str, vector: np.ndarray, weight: float = 1.0):\n",
        "        self.active_points[point_id] = (np.array(vector, dtype=np.float32), weight)\n",
        "        self.buffer.append(point_id)\n",
        "        if len(self.buffer) >= self.threshold:\n",
        "            self._add_coreset(self.buffer)\n",
        "            self.buffer.clear()\n",
        "\n",
        "    def delete(self, point_id: str):\n",
        "        if point_id in self.active_points:\n",
        "            self.deleted_points.add(point_id)\n",
        "            self.buffer.append(point_id)\n",
        "            if len(self.buffer) >= self.threshold:\n",
        "                self._add_coreset(self.buffer)\n",
        "                self.buffer.clear()\n",
        "\n",
        "    def _add_coreset(self, point_ids: List[str]):\n",
        "        X, weights = [], []\n",
        "        for pid in point_ids:\n",
        "            if pid in self.active_points and pid not in self.deleted_points:\n",
        "                vec, w = self.active_points[pid]\n",
        "                X.append(vec)\n",
        "                weights.append(w)\n",
        "\n",
        "        if not X:\n",
        "            return\n",
        "\n",
        "        X = np.vstack(X).astype(np.float32)  # 将向量堆叠成矩阵\n",
        "        weights = np.array(weights, dtype=np.float32)  # 将权重转为numpy数组\n",
        "        self._merge_into_tree(0, (X, weights))  # 将新核心集合并到树中\n",
        "\n",
        "    def _merge_into_tree(self, level: int, new_coreset: Tuple[np.ndarray, np.ndarray]):\n",
        "        self.coreset_tree[level].append(new_coreset)\n",
        "        if len(self.coreset_tree[level]) > 1:\n",
        "            X1, w1 = self.coreset_tree[level].pop()\n",
        "            X2, w2 = self.coreset_tree[level].pop()\n",
        "            X = np.vstack([X1, X2])\n",
        "            W = np.concatenate([w1, w2])\n",
        "            reducer = Lloyd(d=self.d, k=self.k, **self.lloyd_kwargs)\n",
        "            reducer.train(X, weights=W)\n",
        "            reduced_X = reducer.centroids\n",
        "            reduced_W = np.bincount(reducer.labels_, minlength=self.k).astype(np.float32)\n",
        "            self._merge_into_tree(level + 1, (reduced_X, reduced_W))\n",
        "\n",
        "    def retrain(self):\n",
        "        if self.buffer:\n",
        "            self._add_coreset(self.buffer)\n",
        "            self.buffer.clear()\n",
        "        X_all, W_all = [], []\n",
        "        for level in self.coreset_tree:\n",
        "            for X, W in self.coreset_tree[level]:\n",
        "                X_all.append(X)\n",
        "                W_all.append(W)\n",
        "        if not X_all:\n",
        "            return\n",
        "        X = np.vstack(X_all)\n",
        "        W = np.concatenate(W_all)\n",
        "        mask = W > 0\n",
        "        X = X[mask]\n",
        "        W = W[mask]\n",
        "        self.model.train(X, weights=W)\n",
        "\n",
        "    def get_centroids(self) -> Optional[np.ndarray]:\n",
        "        return self.model.centroids\n",
        "\n",
        "    def get_labels(self) -> Optional[np.ndarray]:\n",
        "        return self.model.labels_\n",
        "\n",
        "    def get_metrics(self) -> Dict[str, float]:\n",
        "        return {\n",
        "            \"sse\": self.model.sse_,\n",
        "            \"balance_loss\": self.model.balance_loss_,\n",
        "            \"runtime\": self.model.runtime_,\n",
        "            \"objective\": self.model.obj\n",
        "        }\n",
        "\n",
        "class OnlineKMeans:\n",
        "    def __init__(self, d, k, init_method='split', verbose=False, seed=0):\n",
        "        self.d, self.k = d, k\n",
        "        self.verbose = verbose\n",
        "        np.random.seed(seed)\n",
        "        self.centroids = None\n",
        "        self.counts = None  # 每簇访问计数\n",
        "\n",
        "    def initialize(self, first_point):\n",
        "        self.centroids = first_point.reshape(1, -1)\n",
        "        self.counts = np.array([1], dtype=int)\n",
        "\n",
        "    def maybe_split(self):\n",
        "        # 找方差最大簇，二分裂\n",
        "        if self.centroids.shape[0] < self.k:\n",
        "            variances = []\n",
        "            for i in range(len(self.counts)):\n",
        "                variances.append(np.random.rand())  # 实际应估方差，这里 placeholder\n",
        "            j = np.argmax(variances)\n",
        "            c = self.centroids[j]\n",
        "            delta = np.random.randn(self.d) * 1e-3\n",
        "            new_c1, new_c2 = c + delta, c - delta\n",
        "            self.centroids[j] = new_c1\n",
        "            self.centroids = np.vstack([self.centroids, new_c2])\n",
        "            self.counts[j] = self.counts[j] // 2\n",
        "            self.counts = np.append(self.counts, self.counts[j])\n",
        "            if self.verbose:\n",
        "                print(f\"Split centroid {j} into 2; now {self.centroids.shape[0]} total.\")\n",
        "\n",
        "    def partial_fit(self, x_new):\n",
        "        if self.centroids is None:\n",
        "            self.initialize(x_new)\n",
        "            return\n",
        "        # 找最近质心\n",
        "        dists = np.linalg.norm(self.centroids - x_new, axis=1)\n",
        "        j = np.argmin(dists)\n",
        "        self.counts[j] += 1\n",
        "        eta = 1.0 / self.counts[j]\n",
        "        self.centroids[j] += eta * (x_new - self.centroids[j])\n",
        "        # 尝试分裂\n",
        "        if self.centroids.shape[0] < self.k:\n",
        "            self.maybe_split()\n",
        "\n",
        "    def get_balance_loss(self):\n",
        "        if self.counts is None or len(self.counts) == 0:\n",
        "            return 0.0\n",
        "        total = np.sum(self.counts)\n",
        "        ideal = total / self.k\n",
        "        return np.sum((self.counts - ideal) ** 2)\n",
        "\n",
        "class MiniBatchKMeans(Lloyd):\n",
        "    def __init__(self, d, k, batch_size=256, iters=100, **kwargs):\n",
        "        super().__init__(d=d, k=k, **kwargs)\n",
        "        self.batch_size = batch_size\n",
        "        self.iters = iters\n",
        "\n",
        "    def partial_fit(self, X_stream):\n",
        "        # X_stream: shape (n_stream, d)\n",
        "        # 初始化质心（如果为空则从 stream 抽 batch 初始化）\n",
        "        if self.centroids is None:\n",
        "            init = X_stream[np.random.choice(len(X_stream), self.k, replace=False)]\n",
        "            self.centroids = init\n",
        "            self.counts = np.zeros(self.k, dtype=int)\n",
        "\n",
        "        for i in range(self.iters):\n",
        "            idx = np.random.choice(len(X_stream), self.batch_size, replace=False)\n",
        "            batch = X_stream[idx]\n",
        "            index = faiss.IndexFlatL2(self.d)\n",
        "            index.add(self.centroids.astype(np.float32))\n",
        "            _, labels = index.search(batch.astype(np.float32), 1)\n",
        "\n",
        "            labels = labels.flatten()\n",
        "            for j in range(self.k):\n",
        "                mask = (labels == j)\n",
        "                m = np.sum(mask)\n",
        "                if m > 0:\n",
        "                    centroid = self.centroids[j]\n",
        "                    mean_batch = batch[mask].mean(axis=0)\n",
        "                    eta = 1.0 / (self.counts[j] + m)\n",
        "                    self.centroids[j] = (1 - eta) * centroid + eta * mean_batch\n",
        "                    self.counts[j] += m\n",
        "        # 设置 labels 未定义，balance/SSE 可后续重算\n",
        "\n",
        "    def get_balance_loss(self):\n",
        "        if getattr(self, 'counts', None) is None:\n",
        "            return None\n",
        "        ideal = self.counts.sum() / self.k\n",
        "        return np.sum((self.counts - ideal)**2)\n",
        "\n",
        "def evaluate_test_sse(test_data, centroids):\n",
        "    dists = np.linalg.norm(test_data[:, None, :] - centroids[None, :, :], axis=2)\n",
        "    nearest = np.argmin(dists, axis=1)\n",
        "    return np.sum((test_data - centroids[nearest])**2)\n",
        "\n",
        "\n",
        "def run_test_update_comparison(datasets, k=10, threshold=200):\n",
        "    # 准备CSV文件存储结果\n",
        "    csv_file = 'balance_loss_comparison.csv'\n",
        "    headers = ['Dataset', 'Scale', 'Coreset', 'OnlineKMeans', 'MiniBatch']\n",
        "\n",
        "    # 定义数据集和数据量的固定顺序\n",
        "    DATASET_ORDER = ['Huatuo', 'LiveChat', 'deep', 'glove', 'sift']\n",
        "    SCALE_ORDER = ['100', '500', '1k', '2k', '5k']\n",
        "\n",
        "    # 如果文件不存在，写入表头\n",
        "    if not os.path.exists(csv_file):\n",
        "        with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(headers)\n",
        "\n",
        "    # 收集所有结果\n",
        "    all_results = []\n",
        "\n",
        "    # 按固定顺序处理数据集\n",
        "    for dataset_name in DATASET_ORDER:\n",
        "        # 找到对应的数据集配置\n",
        "        ds = next((d for d in datasets if Path(d[\"train\"]).stem.split(\"_\")[0] == dataset_name), None)\n",
        "        if not ds:\n",
        "            continue\n",
        "\n",
        "        name = dataset_name\n",
        "        dim = ds[\"dim\"]\n",
        "        train_data = pd.read_csv(ds[\"train\"], header=None).values.astype(np.float32)\n",
        "\n",
        "        # 准备不同大小的测试数据集\n",
        "        test_files = {\n",
        "            '100': ds[\"test1\"],\n",
        "            '500': ds[\"test2\"],\n",
        "            '1k': ds[\"test3\"],\n",
        "            '2k': ds[\"test4\"],\n",
        "            '5k': ds[\"test5\"]\n",
        "        }\n",
        "\n",
        "        # 使用Lloyd方法初始化获取基础平衡损失\n",
        "        lloyd = Lloyd(d=dim, k=k, verbose=False)\n",
        "        lloyd.train(train_data)\n",
        "        base_balance_loss = lloyd.balance_loss_\n",
        "        base_centroids = lloyd.centroids.copy()\n",
        "        base_labels = lloyd.labels_.copy()\n",
        "\n",
        "        # 按数据量顺序处理\n",
        "        for size in SCALE_ORDER:\n",
        "            test_path = test_files.get(size)\n",
        "            if not test_path or not os.path.exists(test_path):\n",
        "                print(f\"警告：测试文件 {test_path} 不存在，跳过\")\n",
        "                continue\n",
        "\n",
        "            test_data = pd.read_csv(test_path, header=None).values.astype(np.float32)\n",
        "            print(f\"\\n====== 数据集: {name} (维度={dim}) | 测试数据量: {size} ======\")\n",
        "\n",
        "            # 存储结果的字典\n",
        "            results = {\n",
        "                'Dataset': name,\n",
        "                'Scale': size,\n",
        "                'Coreset': 0,\n",
        "                'OnlineKMeans': 0,\n",
        "                'MiniBatch': 0\n",
        "            }\n",
        "\n",
        "            # 方法1: DynamicCoreset\n",
        "            dyn = DynamicKMeansCoreset(d=dim, k=k, threshold=threshold, verbose=False)\n",
        "            # 插入训练数据\n",
        "            for i, vec in enumerate(train_data):\n",
        "                dyn.insert(f\"{name}_train_{i}\", vec)\n",
        "            # 插入测试数据\n",
        "            for i, vec in enumerate(test_data):\n",
        "                dyn.insert(f\"{name}_test_{i}\", vec)\n",
        "            dyn.retrain()\n",
        "\n",
        "            # 计算整体平衡损失（训练+测试）\n",
        "            combined_data = np.vstack([train_data, test_data])\n",
        "            dists = np.linalg.norm(combined_data[:, None, :] - dyn.model.centroids[None, :, :], axis=2)\n",
        "            labels = np.argmin(dists, axis=1)\n",
        "            sizes = np.bincount(labels, minlength=k)\n",
        "            ideal = len(combined_data) / k\n",
        "            results['Coreset'] = np.sum((sizes - ideal) ** 2)\n",
        "\n",
        "            # 方法2: OnlineKMeans\n",
        "            online = OnlineKMeans(d=dim, k=k)\n",
        "            online.centroids = base_centroids.copy()\n",
        "            online.counts = np.bincount(base_labels, minlength=k)\n",
        "\n",
        "            # 先用训练数据更新\n",
        "            for vec in train_data:\n",
        "                online.partial_fit(vec)\n",
        "            # 再用测试数据更新\n",
        "            for vec in test_data:\n",
        "                online.partial_fit(vec)\n",
        "\n",
        "            # 计算整体平衡损失\n",
        "            dists = np.linalg.norm(combined_data[:, None, :] - online.centroids[None, :, :], axis=2)\n",
        "            labels = np.argmin(dists, axis=1)\n",
        "            sizes = np.bincount(labels, minlength=k)\n",
        "            results['OnlineKMeans'] = np.sum((sizes - ideal) ** 2)\n",
        "\n",
        "            # 方法3: MiniBatchKMeans\n",
        "            mini = MiniBatchKMeans(d=dim, k=k, batch_size=32, iters=10, verbose=False)\n",
        "            mini.centroids = base_centroids.copy()\n",
        "            mini.counts = np.bincount(base_labels, minlength=k)\n",
        "\n",
        "            # 先用训练数据更新\n",
        "            mini.partial_fit(train_data)\n",
        "            # 再用测试数据更新\n",
        "            mini.partial_fit(test_data)\n",
        "\n",
        "            # 计算整体平衡损失\n",
        "            dists = np.linalg.norm(combined_data[:, None, :] - mini.centroids[None, :, :], axis=2)\n",
        "            labels = np.argmin(dists, axis=1)\n",
        "            sizes = np.bincount(labels, minlength=k)\n",
        "            results['MiniBatch'] = np.sum((sizes - ideal) ** 2)\n",
        "\n",
        "            # 打印结果\n",
        "            print(f\"初始平衡损失: {base_balance_loss:.2f}\")\n",
        "            print(f\"[Coreset] 平衡损失: {results['Coreset']:.2f}\")\n",
        "            print(f\"[OnlineKMeans] 平衡损失: {results['OnlineKMeans']:.2f}\")\n",
        "            print(f\"[MiniBatchKMeans] 平衡损失: {results['MiniBatch']:.2f}\")\n",
        "\n",
        "            # 添加到结果列表\n",
        "            all_results.append([\n",
        "                results['Dataset'],\n",
        "                results['Scale'],\n",
        "                results['Coreset'],\n",
        "                results['OnlineKMeans'],\n",
        "                results['MiniBatch']\n",
        "            ])\n",
        "\n",
        "    # 按顺序写入所有结果\n",
        "    with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(headers)\n",
        "        writer.writerows(all_results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    datasets = [\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/Huatuo_1024d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/Huatuo_1024d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/Huatuo_1024d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/Huatuo_1024d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/Huatuo_1024d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/Huatuo_1024d_5k.csv\",\n",
        "            \"dim\": 1024\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/LiveChat_1024d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/LiveChat_1024d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/LiveChat_1024d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/LiveChat_1024d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/LiveChat_1024d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/LiveChat_1024d_5k.csv\",\n",
        "            \"dim\": 1024\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/deep_96d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/deep_96d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/deep_96d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/deep_96d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/deep_96d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/deep_96d_5k.csv\",\n",
        "            \"dim\": 96\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/glove_300d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/glove_300d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/glove_300d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/glove_300d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/glove_300d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/glove_300d_5k.csv\",\n",
        "            \"dim\": 300\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/sift_128d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/sift_128d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/sift_128d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/sift_128d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/sift_128d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/sift_128d_5k.csv\",\n",
        "            \"dim\": 128\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    run_test_update_comparison(datasets, k=10, threshold=200)\n",
        "    print(f\"\\n结果已保存\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NppSfh7uZgtC",
        "outputId": "4f619d8e-6773-4cd0-c6d8-2393c63b69e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 100 ======\n",
            "初始平衡损失: 837824.90\n",
            "[Coreset] 平衡损失: 18520505.60\n",
            "[OnlineKMeans] 平衡损失: 861825.60\n",
            "[MiniBatchKMeans] 平衡损失: 860715.60\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 500 ======\n",
            "初始平衡损失: 837824.90\n",
            "[Coreset] 平衡损失: 21121399.60\n",
            "[OnlineKMeans] 平衡损失: 937327.60\n",
            "[MiniBatchKMeans] 平衡损失: 945901.60\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 1k ======\n",
            "初始平衡损失: 837824.90\n",
            "[Coreset] 平衡损失: 4364881.60\n",
            "[OnlineKMeans] 平衡损失: 1027969.60\n",
            "[MiniBatchKMeans] 平衡损失: 1044507.60\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 2k ======\n",
            "初始平衡损失: 837824.90\n",
            "[Coreset] 平衡损失: 5409673.60\n",
            "[OnlineKMeans] 平衡损失: 1242217.60\n",
            "[MiniBatchKMeans] 平衡损失: 1268505.60\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 5k ======\n",
            "初始平衡损失: 837824.90\n",
            "[Coreset] 平衡损失: 10848703.60\n",
            "[OnlineKMeans] 平衡损失: 1935787.60\n",
            "[MiniBatchKMeans] 平衡损失: 1955811.60\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 100 ======\n",
            "初始平衡损失: 1284214.90\n",
            "[Coreset] 平衡损失: 88950887.60\n",
            "[OnlineKMeans] 平衡损失: 1336655.60\n",
            "[MiniBatchKMeans] 平衡损失: 1324987.60\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 500 ======\n",
            "初始平衡损失: 1284214.90\n",
            "[Coreset] 平衡损失: 46579175.60\n",
            "[OnlineKMeans] 平衡损失: 1467303.60\n",
            "[MiniBatchKMeans] 平衡损失: 1437917.60\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 1k ======\n",
            "初始平衡损失: 1284214.90\n",
            "[Coreset] 平衡损失: 60329683.60\n",
            "[OnlineKMeans] 平衡损失: 1613771.60\n",
            "[MiniBatchKMeans] 平衡损失: 1588573.60\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 2k ======\n",
            "初始平衡损失: 1284214.90\n",
            "[Coreset] 平衡损失: 51309661.60\n",
            "[OnlineKMeans] 平衡损失: 1938925.60\n",
            "[MiniBatchKMeans] 平衡损失: 1896659.60\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 5k ======\n",
            "初始平衡损失: 1284214.90\n",
            "[Coreset] 平衡损失: 60204891.60\n",
            "[OnlineKMeans] 平衡损失: 3088367.60\n",
            "[MiniBatchKMeans] 平衡损失: 3038255.60\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 100 ======\n",
            "初始平衡损失: 1789002.90\n",
            "[Coreset] 平衡损失: 23395703.60\n",
            "[OnlineKMeans] 平衡损失: 1820977.60\n",
            "[MiniBatchKMeans] 平衡损失: 1842727.60\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 500 ======\n",
            "初始平衡损失: 1789002.90\n",
            "[Coreset] 平衡损失: 40951413.60\n",
            "[OnlineKMeans] 平衡损失: 1994531.60\n",
            "[MiniBatchKMeans] 平衡损失: 2023319.60\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 1k ======\n",
            "初始平衡损失: 1789002.90\n",
            "[Coreset] 平衡损失: 3995299.60\n",
            "[OnlineKMeans] 平衡损失: 2177983.60\n",
            "[MiniBatchKMeans] 平衡损失: 2224963.60\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 2k ======\n",
            "初始平衡损失: 1789002.90\n",
            "[Coreset] 平衡损失: 2635919.60\n",
            "[OnlineKMeans] 平衡损失: 2596763.60\n",
            "[MiniBatchKMeans] 平衡损失: 2635389.60\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 5k ======\n",
            "初始平衡损失: 1789002.90\n",
            "[Coreset] 平衡损失: 3622433.60\n",
            "[OnlineKMeans] 平衡损失: 4012285.60\n",
            "[MiniBatchKMeans] 平衡损失: 4136295.60\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 100 ======\n",
            "初始平衡损失: 1079546.90\n",
            "[Coreset] 平衡损失: 84802701.60\n",
            "[OnlineKMeans] 平衡损失: 1075785.60\n",
            "[MiniBatchKMeans] 平衡损失: 1095913.60\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 500 ======\n",
            "初始平衡损失: 1079546.90\n",
            "[Coreset] 平衡损失: 43568541.60\n",
            "[OnlineKMeans] 平衡损失: 1146143.60\n",
            "[MiniBatchKMeans] 平衡损失: 1170281.60\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 1k ======\n",
            "初始平衡损失: 1079546.90\n",
            "[Coreset] 平衡损失: 75294901.60\n",
            "[OnlineKMeans] 平衡损失: 1219037.60\n",
            "[MiniBatchKMeans] 平衡损失: 1242579.60\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 2k ======\n",
            "初始平衡损失: 1079546.90\n",
            "[Coreset] 平衡损失: 67905239.60\n",
            "[OnlineKMeans] 平衡损失: 1415769.60\n",
            "[MiniBatchKMeans] 平衡损失: 1440525.60\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 5k ======\n",
            "初始平衡损失: 1079546.90\n",
            "[Coreset] 平衡损失: 94439543.60\n",
            "[OnlineKMeans] 平衡损失: 2197829.60\n",
            "[MiniBatchKMeans] 平衡损失: 2184747.60\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 100 ======\n",
            "初始平衡损失: 280066.90\n",
            "[Coreset] 平衡损失: 13275200.90\n",
            "[OnlineKMeans] 平衡损失: 287654.90\n",
            "[MiniBatchKMeans] 平衡损失: 290116.90\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 500 ======\n",
            "初始平衡损失: 280066.90\n",
            "[Coreset] 平衡损失: 7027488.90\n",
            "[OnlineKMeans] 平衡损失: 304058.90\n",
            "[MiniBatchKMeans] 平衡损失: 306968.90\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 1k ======\n",
            "初始平衡损失: 280066.90\n",
            "[Coreset] 平衡损失: 4862588.90\n",
            "[OnlineKMeans] 平衡损失: 304612.90\n",
            "[MiniBatchKMeans] 平衡损失: 310990.90\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 2k ======\n",
            "初始平衡损失: 280066.90\n",
            "[Coreset] 平衡损失: 6107406.90\n",
            "[OnlineKMeans] 平衡损失: 358240.90\n",
            "[MiniBatchKMeans] 平衡损失: 366250.90\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 5k ======\n",
            "初始平衡损失: 280066.90\n",
            "[Coreset] 平衡损失: 11976628.90\n",
            "[OnlineKMeans] 平衡损失: 1023908.90\n",
            "[MiniBatchKMeans] 平衡损失: 1119940.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from scipy.linalg import orth # For creating orthogonal matrices\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "import csv\n",
        "import os\n",
        "\n",
        "class Lloyd:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.centroids = None\n",
        "        self.obj_history_ = None\n",
        "        self.labels_ = None\n",
        "\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "        self.obj = None\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning random point.\")\n",
        "                centroids[j] = data_points[np.random.randint(len(data_points))]\n",
        "\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        start_time = time.time()\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input dimension {x_orig_data.shape[1]} != {self.d}\")\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        x = np.ascontiguousarray(x_orig_data, dtype='float32')\n",
        "\n",
        "        kmeans = faiss.Kmeans(\n",
        "            d=self.d,\n",
        "            k=self.k,\n",
        "            niter=self.niter,\n",
        "            nredo=self.nredo,\n",
        "            verbose=self.verbose,\n",
        "            min_points_per_centroid=self.min_points_per_centroid,\n",
        "            max_points_per_centroid=self.max_points_per_centroid,\n",
        "            seed=self.seed,\n",
        "            gpu=self.gpu,\n",
        "            spherical=self.spherical,\n",
        "            update_index=self.update_index,\n",
        "            frozen_centroids=self.frozen_centroids\n",
        "        )\n",
        "\n",
        "        kmeans.train(x, init_centroids=init_centroids)\n",
        "\n",
        "        _, self.labels_ = kmeans.index.search(x, 1)\n",
        "        self.labels_ = self.labels_.flatten()\n",
        "\n",
        "        self.centroids = kmeans.centroids\n",
        "        self.obj_history_ = kmeans.obj if kmeans.obj is not None and len(kmeans.obj) > 0 else np.zeros(self.niter)\n",
        "        self.obj = kmeans.obj[-1] if kmeans.obj is not None and len(kmeans.obj) > 0 else None\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "\n",
        "        # Print every 5th iteration's objective value\n",
        "        if self.verbose and self.obj_history_ is not None and len(self.obj_history_) > 0:\n",
        "            print(\"\\n--- Objective Value (every 5 iterations) ---\")\n",
        "            for i, val in enumerate(self.obj_history_):\n",
        "                if (i + 1) % 5 == 0 or i == len(self.obj_history_) - 1:\n",
        "                    print(f\"  Iter {i+1:2d}: {val:.6f}\")\n",
        "\n",
        "        final_sse = 0\n",
        "        for i in range(n):\n",
        "            cluster_idx = self.labels_[i]\n",
        "            final_sse += np.sum((x_orig_data[i] - self.centroids[cluster_idx]) ** 2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal = n / self.k\n",
        "            final_balance_loss = np.sum((sizes - ideal) ** 2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Lloyd training finished in {self.runtime_:.4f}s\")\n",
        "            print(f\"Final obj: {self.obj}\")\n",
        "            print(f\"Cluster sizes: {dict(zip(*np.unique(self.labels_, return_counts=True)))}\")\n",
        "            print(f\"SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class DynamicKMeansCoreset:\n",
        "    def __init__(self, d, k, threshold=100, **lloyd_kwargs):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.threshold = threshold\n",
        "        self.lloyd_kwargs = lloyd_kwargs\n",
        "\n",
        "        self.buffer = [] # 临时存储插入/删除的点ID\n",
        "        self.coreset_tree = defaultdict(list) # 分层存储核心集的数据结构\n",
        "        self.active_points: Dict[str, Tuple[np.ndarray, float]] = {} # 当前活跃的点(未删除)\n",
        "        self.deleted_points = set() # 已删除的点ID集合\n",
        "        self.model = Lloyd(d=d, k=k, **lloyd_kwargs)\n",
        "\n",
        "    def insert(self, point_id: str, vector: np.ndarray, weight: float = 1.0):\n",
        "        self.active_points[point_id] = (np.array(vector, dtype=np.float32), weight)\n",
        "        self.buffer.append(point_id)\n",
        "        if len(self.buffer) >= self.threshold:\n",
        "            self._add_coreset(self.buffer)\n",
        "            self.buffer.clear()\n",
        "\n",
        "    def delete(self, point_id: str):\n",
        "        if point_id in self.active_points:\n",
        "            self.deleted_points.add(point_id)\n",
        "            self.buffer.append(point_id)\n",
        "            if len(self.buffer) >= self.threshold:\n",
        "                self._add_coreset(self.buffer)\n",
        "                self.buffer.clear()\n",
        "\n",
        "    def _add_coreset(self, point_ids: List[str]):\n",
        "        X, weights = [], []\n",
        "        for pid in point_ids:\n",
        "            if pid in self.active_points and pid not in self.deleted_points:\n",
        "                vec, w = self.active_points[pid]\n",
        "                X.append(vec)\n",
        "                weights.append(w)\n",
        "\n",
        "        if not X:\n",
        "            return\n",
        "\n",
        "        X = np.vstack(X).astype(np.float32)  # 将向量堆叠成矩阵\n",
        "        weights = np.array(weights, dtype=np.float32)  # 将权重转为numpy数组\n",
        "        self._merge_into_tree(0, (X, weights))  # 将新核心集合并到树中\n",
        "\n",
        "    def _merge_into_tree(self, level: int, new_coreset: Tuple[np.ndarray, np.ndarray]):\n",
        "        self.coreset_tree[level].append(new_coreset)\n",
        "        if len(self.coreset_tree[level]) > 1:\n",
        "            X1, w1 = self.coreset_tree[level].pop()\n",
        "            X2, w2 = self.coreset_tree[level].pop()\n",
        "            X = np.vstack([X1, X2])\n",
        "            W = np.concatenate([w1, w2])\n",
        "            reducer = Lloyd(d=self.d, k=self.k, **self.lloyd_kwargs)\n",
        "            reducer.train(X, weights=W)\n",
        "            reduced_X = reducer.centroids\n",
        "            reduced_W = np.bincount(reducer.labels_, minlength=self.k).astype(np.float32)\n",
        "            self._merge_into_tree(level + 1, (reduced_X, reduced_W))\n",
        "\n",
        "    def retrain(self):\n",
        "        if self.buffer:\n",
        "            self._add_coreset(self.buffer)\n",
        "            self.buffer.clear()\n",
        "        X_all, W_all = [], []\n",
        "        for level in self.coreset_tree:\n",
        "            for X, W in self.coreset_tree[level]:\n",
        "                X_all.append(X)\n",
        "                W_all.append(W)\n",
        "        if not X_all:\n",
        "            return\n",
        "        X = np.vstack(X_all)\n",
        "        W = np.concatenate(W_all)\n",
        "        mask = W > 0\n",
        "        X = X[mask]\n",
        "        W = W[mask]\n",
        "        self.model.train(X, weights=W)\n",
        "\n",
        "    def get_centroids(self) -> Optional[np.ndarray]:\n",
        "        return self.model.centroids\n",
        "\n",
        "    def get_labels(self) -> Optional[np.ndarray]:\n",
        "        return self.model.labels_\n",
        "\n",
        "    def get_metrics(self) -> Dict[str, float]:\n",
        "        return {\n",
        "            \"sse\": self.model.sse_,\n",
        "            \"balance_loss\": self.model.balance_loss_,\n",
        "            \"runtime\": self.model.runtime_,\n",
        "            \"objective\": self.model.obj\n",
        "        }\n",
        "\n",
        "class OnlineKMeans:\n",
        "    def __init__(self, d, k, init_method='split', verbose=False, seed=0):\n",
        "        self.d, self.k = d, k\n",
        "        self.verbose = verbose\n",
        "        np.random.seed(seed)\n",
        "        self.centroids = None\n",
        "        self.counts = None  # 每簇访问计数\n",
        "\n",
        "    def initialize(self, first_point):\n",
        "        self.centroids = first_point.reshape(1, -1)\n",
        "        self.counts = np.array([1], dtype=int)\n",
        "\n",
        "    def maybe_split(self):\n",
        "        # 找方差最大簇，二分裂\n",
        "        if self.centroids.shape[0] < self.k:\n",
        "            variances = []\n",
        "            for i in range(len(self.counts)):\n",
        "                variances.append(np.random.rand())  # 实际应估方差，这里 placeholder\n",
        "            j = np.argmax(variances)\n",
        "            c = self.centroids[j]\n",
        "            delta = np.random.randn(self.d) * 1e-3\n",
        "            new_c1, new_c2 = c + delta, c - delta\n",
        "            self.centroids[j] = new_c1\n",
        "            self.centroids = np.vstack([self.centroids, new_c2])\n",
        "            self.counts[j] = self.counts[j] // 2\n",
        "            self.counts = np.append(self.counts, self.counts[j])\n",
        "            if self.verbose:\n",
        "                print(f\"Split centroid {j} into 2; now {self.centroids.shape[0]} total.\")\n",
        "\n",
        "    def partial_fit(self, x_new):\n",
        "        if self.centroids is None:\n",
        "            self.initialize(x_new)\n",
        "            return\n",
        "        # 找最近质心\n",
        "        dists = np.linalg.norm(self.centroids - x_new, axis=1)\n",
        "        j = np.argmin(dists)\n",
        "        self.counts[j] += 1\n",
        "        eta = 1.0 / self.counts[j]\n",
        "        self.centroids[j] += eta * (x_new - self.centroids[j])\n",
        "        # 尝试分裂\n",
        "        if self.centroids.shape[0] < self.k:\n",
        "            self.maybe_split()\n",
        "\n",
        "    def get_balance_loss(self):\n",
        "        if self.counts is None or len(self.counts) == 0:\n",
        "            return 0.0\n",
        "        total = np.sum(self.counts)\n",
        "        ideal = total / self.k\n",
        "        return np.sum((self.counts - ideal) ** 2)\n",
        "\n",
        "class MiniBatchKMeans(Lloyd):\n",
        "    def __init__(self, d, k, batch_size=256, iters=100, **kwargs):\n",
        "        super().__init__(d=d, k=k, **kwargs)\n",
        "        self.batch_size = batch_size\n",
        "        self.iters = iters\n",
        "\n",
        "    def partial_fit(self, X_stream):\n",
        "        # X_stream: shape (n_stream, d)\n",
        "        # 初始化质心（如果为空则从 stream 抽 batch 初始化）\n",
        "        if self.centroids is None:\n",
        "            init = X_stream[np.random.choice(len(X_stream), self.k, replace=False)]\n",
        "            self.centroids = init\n",
        "            self.counts = np.zeros(self.k, dtype=int)\n",
        "\n",
        "        for i in range(self.iters):\n",
        "            idx = np.random.choice(len(X_stream), self.batch_size, replace=False)\n",
        "            batch = X_stream[idx]\n",
        "            index = faiss.IndexFlatL2(self.d)\n",
        "            index.add(self.centroids.astype(np.float32))\n",
        "            _, labels = index.search(batch.astype(np.float32), 1)\n",
        "\n",
        "            labels = labels.flatten()\n",
        "            for j in range(self.k):\n",
        "                mask = (labels == j)\n",
        "                m = np.sum(mask)\n",
        "                if m > 0:\n",
        "                    centroid = self.centroids[j]\n",
        "                    mean_batch = batch[mask].mean(axis=0)\n",
        "                    eta = 1.0 / (self.counts[j] + m)\n",
        "                    self.centroids[j] = (1 - eta) * centroid + eta * mean_batch\n",
        "                    self.counts[j] += m\n",
        "        # 设置 labels 未定义，balance/SSE 可后续重算\n",
        "\n",
        "    def get_balance_loss(self):\n",
        "        if getattr(self, 'counts', None) is None:\n",
        "            return None\n",
        "        ideal = self.counts.sum() / self.k\n",
        "        return np.sum((self.counts - ideal)**2)\n",
        "\n",
        "def evaluate_test_sse(test_data, centroids):\n",
        "    dists = np.linalg.norm(test_data[:, None, :] - centroids[None, :, :], axis=2)\n",
        "    nearest = np.argmin(dists, axis=1)\n",
        "    return np.sum((test_data - centroids[nearest])**2)\n",
        "\n",
        "\n",
        "def run_test_update_comparison(datasets, k=10, threshold=200):\n",
        "    # 准备CSV文件存储结果\n",
        "    csv_file = 'sse_comparison.csv'\n",
        "    headers = ['Dataset', 'Scale', 'Coreset', 'OnlineKMeans', 'MiniBatch']\n",
        "\n",
        "    # 定义数据集和数据量的固定顺序\n",
        "    DATASET_ORDER = ['Huatuo', 'LiveChat', 'deep', 'glove', 'sift']\n",
        "    SCALE_ORDER = ['100', '500', '1k', '2k', '5k']\n",
        "\n",
        "    # 如果文件不存在，写入表头\n",
        "    if not os.path.exists(csv_file):\n",
        "        with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(headers)\n",
        "\n",
        "    # 收集所有结果\n",
        "    all_results = []\n",
        "\n",
        "    # 按固定顺序处理数据集\n",
        "    for dataset_name in DATASET_ORDER:\n",
        "        # 找到对应的数据集配置\n",
        "        ds = next((d for d in datasets if Path(d[\"train\"]).stem.split(\"_\")[0] == dataset_name), None)\n",
        "        if not ds:\n",
        "            continue\n",
        "\n",
        "        name = dataset_name\n",
        "        dim = ds[\"dim\"]\n",
        "        train_data = pd.read_csv(ds[\"train\"], header=None).values.astype(np.float32)\n",
        "\n",
        "        # 准备不同大小的测试数据集\n",
        "        test_files = {\n",
        "            '100': ds[\"test1\"],\n",
        "            '500': ds[\"test2\"],\n",
        "            '1k': ds[\"test3\"],\n",
        "            '2k': ds[\"test4\"],\n",
        "            '5k': ds[\"test5\"]\n",
        "        }\n",
        "\n",
        "        # 使用Lloyd方法初始化获取基础平衡损失\n",
        "        lloyd = Lloyd(d=dim, k=k, verbose=False)\n",
        "        lloyd.train(train_data)\n",
        "        base_balance_loss = lloyd.balance_loss_\n",
        "        base_centroids = lloyd.centroids.copy()\n",
        "        base_labels = lloyd.labels_.copy()\n",
        "\n",
        "        # 按数据量顺序处理\n",
        "        for size in SCALE_ORDER:\n",
        "            test_path = test_files.get(size)\n",
        "            if not test_path or not os.path.exists(test_path):\n",
        "                print(f\"警告：测试文件 {test_path} 不存在，跳过\")\n",
        "                continue\n",
        "\n",
        "            test_data = pd.read_csv(test_path, header=None).values.astype(np.float32)\n",
        "            print(f\"\\n====== 数据集: {name} (维度={dim}) | 测试数据量: {size} ======\")\n",
        "\n",
        "            # 存储结果的字典\n",
        "            results = {\n",
        "                'Dataset': name,\n",
        "                'Scale': size,\n",
        "                'Coreset': 0,\n",
        "                'OnlineKMeans': 0,\n",
        "                'MiniBatch': 0\n",
        "            }\n",
        "\n",
        "            # 方法1: DynamicCoreset\n",
        "            dyn = DynamicKMeansCoreset(d=dim, k=k, threshold=threshold, verbose=False)\n",
        "            # 插入训练数据\n",
        "            for i, vec in enumerate(train_data):\n",
        "                dyn.insert(f\"{name}_train_{i}\", vec)\n",
        "            # 插入测试数据\n",
        "            for i, vec in enumerate(test_data):\n",
        "                dyn.insert(f\"{name}_test_{i}\", vec)\n",
        "            dyn.retrain()\n",
        "\n",
        "            # 计算整体SSE（训练+测试）\n",
        "            combined_data = np.vstack([train_data, test_data])\n",
        "            dists = np.linalg.norm(combined_data[:, None, :] - dyn.model.centroids[None, :, :], axis=2)\n",
        "            nearest_centroids = np.argmin(dists, axis=1)\n",
        "            sse = np.sum((combined_data - dyn.model.centroids[nearest_centroids]) ** 2)\n",
        "            results['Coreset'] = sse\n",
        "\n",
        "            # 方法2: OnlineKMeans\n",
        "            online = OnlineKMeans(d=dim, k=k)\n",
        "            online.centroids = base_centroids.copy()\n",
        "            online.counts = np.bincount(base_labels, minlength=k)\n",
        "\n",
        "            # 先用训练数据更新\n",
        "            for vec in train_data:\n",
        "                online.partial_fit(vec)\n",
        "            # 再用测试数据更新\n",
        "            for vec in test_data:\n",
        "                online.partial_fit(vec)\n",
        "\n",
        "            # 计算整体SSE\n",
        "            dists = np.linalg.norm(combined_data[:, None, :] - online.centroids[None, :, :], axis=2)\n",
        "            nearest_centroids = np.argmin(dists, axis=1)\n",
        "            sse = np.sum((combined_data - online.centroids[nearest_centroids]) ** 2)\n",
        "            results['OnlineKMeans'] = sse\n",
        "\n",
        "            # 方法3: MiniBatchKMeans\n",
        "            mini = MiniBatchKMeans(d=dim, k=k, batch_size=32, iters=10, verbose=False)\n",
        "            mini.centroids = base_centroids.copy()\n",
        "            mini.counts = np.bincount(base_labels, minlength=k)\n",
        "\n",
        "            # 先用训练数据更新\n",
        "            mini.partial_fit(train_data)\n",
        "            # 再用测试数据更新\n",
        "            mini.partial_fit(test_data)\n",
        "\n",
        "            # 计算整体SSE\n",
        "            dists = np.linalg.norm(combined_data[:, None, :] - mini.centroids[None, :, :], axis=2)\n",
        "            nearest_centroids = np.argmin(dists, axis=1)\n",
        "            sse = np.sum((combined_data - mini.centroids[nearest_centroids]) ** 2)\n",
        "            results['MiniBatch'] = sse\n",
        "\n",
        "            # 打印结果\n",
        "            print(f\"初始SSE: {base_balance_loss:.2f}\")\n",
        "            print(f\"[Coreset] SSE: {results['Coreset']:.2f}\")\n",
        "            print(f\"[OnlineKMeans] SSE: {results['OnlineKMeans']:.2f}\")\n",
        "            print(f\"[MiniBatchKMeans] SSE: {results['MiniBatch']:.2f}\")\n",
        "\n",
        "            # 添加到结果列表\n",
        "            all_results.append([\n",
        "                results['Dataset'],\n",
        "                results['Scale'],\n",
        "                results['Coreset'],\n",
        "                results['OnlineKMeans'],\n",
        "                results['MiniBatch']\n",
        "            ])\n",
        "\n",
        "    # 按顺序写入所有结果\n",
        "    with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(headers)\n",
        "        writer.writerows(all_results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    datasets = [\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/Huatuo_1024d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/Huatuo_1024d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/Huatuo_1024d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/Huatuo_1024d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/Huatuo_1024d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/Huatuo_1024d_5k.csv\",\n",
        "            \"dim\": 1024\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/LiveChat_1024d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/LiveChat_1024d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/LiveChat_1024d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/LiveChat_1024d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/LiveChat_1024d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/LiveChat_1024d_5k.csv\",\n",
        "            \"dim\": 1024\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/deep_96d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/deep_96d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/deep_96d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/deep_96d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/deep_96d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/deep_96d_5k.csv\",\n",
        "            \"dim\": 96\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/glove_300d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/glove_300d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/glove_300d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/glove_300d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/glove_300d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/glove_300d_5k.csv\",\n",
        "            \"dim\": 300\n",
        "        },\n",
        "        {\n",
        "            \"train\": \"/content/sample_data/sift_128d_10k.csv\",\n",
        "            \"test1\": \"/content/sample_data/sift_128d_100.csv\",\n",
        "            \"test2\": \"/content/sample_data/sift_128d_500.csv\",\n",
        "            \"test3\": \"/content/sample_data/sift_128d_1k.csv\",\n",
        "            \"test4\": \"/content/sample_data/sift_128d_2k.csv\",\n",
        "            \"test5\": \"/content/sample_data/sift_128d_5k.csv\",\n",
        "            \"dim\": 128\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    run_test_update_comparison(datasets, k=10, threshold=200)\n",
        "    print(f\"\\n结果已保存\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J0bDISVhnqy",
        "outputId": "45720a8f-160f-4bce-edcc-45d635cd1f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 100 ======\n",
            "初始SSE: 837824.90\n",
            "[Coreset] SSE: 2764.94\n",
            "[OnlineKMeans] SSE: 2502.68\n",
            "[MiniBatchKMeans] SSE: 2502.71\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 500 ======\n",
            "初始SSE: 837824.90\n",
            "[Coreset] SSE: 2897.14\n",
            "[OnlineKMeans] SSE: 2601.31\n",
            "[MiniBatchKMeans] SSE: 2601.40\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 1k ======\n",
            "初始SSE: 837824.90\n",
            "[Coreset] SSE: 2868.90\n",
            "[OnlineKMeans] SSE: 2726.15\n",
            "[MiniBatchKMeans] SSE: 2726.32\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 2k ======\n",
            "初始SSE: 837824.90\n",
            "[Coreset] SSE: 3146.81\n",
            "[OnlineKMeans] SSE: 2973.99\n",
            "[MiniBatchKMeans] SSE: 2974.35\n",
            "\n",
            "====== 数据集: Huatuo (维度=1024) | 测试数据量: 5k ======\n",
            "初始SSE: 837824.90\n",
            "[Coreset] SSE: 3882.81\n",
            "[OnlineKMeans] SSE: 3716.28\n",
            "[MiniBatchKMeans] SSE: 3717.31\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 100 ======\n",
            "初始SSE: 1284214.90\n",
            "[Coreset] SSE: 2074.40\n",
            "[OnlineKMeans] SSE: 1879.87\n",
            "[MiniBatchKMeans] SSE: 1879.90\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 500 ======\n",
            "初始SSE: 1284214.90\n",
            "[Coreset] SSE: 2134.75\n",
            "[OnlineKMeans] SSE: 1953.95\n",
            "[MiniBatchKMeans] SSE: 1954.02\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 1k ======\n",
            "初始SSE: 1284214.90\n",
            "[Coreset] SSE: 2222.50\n",
            "[OnlineKMeans] SSE: 2045.94\n",
            "[MiniBatchKMeans] SSE: 2046.09\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 2k ======\n",
            "初始SSE: 1284214.90\n",
            "[Coreset] SSE: 2418.15\n",
            "[OnlineKMeans] SSE: 2232.59\n",
            "[MiniBatchKMeans] SSE: 2232.92\n",
            "\n",
            "====== 数据集: LiveChat (维度=1024) | 测试数据量: 5k ======\n",
            "初始SSE: 1284214.90\n",
            "[Coreset] SSE: 3024.90\n",
            "[OnlineKMeans] SSE: 2792.67\n",
            "[MiniBatchKMeans] SSE: 2793.50\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 100 ======\n",
            "初始SSE: 1789002.90\n",
            "[Coreset] SSE: 8737.98\n",
            "[OnlineKMeans] SSE: 7609.07\n",
            "[MiniBatchKMeans] SSE: 7610.43\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 500 ======\n",
            "初始SSE: 1789002.90\n",
            "[Coreset] SSE: 9040.04\n",
            "[OnlineKMeans] SSE: 7912.37\n",
            "[MiniBatchKMeans] SSE: 7913.72\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 1k ======\n",
            "初始SSE: 1789002.90\n",
            "[Coreset] SSE: 8579.63\n",
            "[OnlineKMeans] SSE: 8287.54\n",
            "[MiniBatchKMeans] SSE: 8289.30\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 2k ======\n",
            "初始SSE: 1789002.90\n",
            "[Coreset] SSE: 9262.58\n",
            "[OnlineKMeans] SSE: 9034.84\n",
            "[MiniBatchKMeans] SSE: 9037.09\n",
            "\n",
            "====== 数据集: deep (维度=96) | 测试数据量: 5k ======\n",
            "初始SSE: 1789002.90\n",
            "[Coreset] SSE: 11575.59\n",
            "[OnlineKMeans] SSE: 11297.43\n",
            "[MiniBatchKMeans] SSE: 11302.78\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 100 ======\n",
            "初始SSE: 1079546.90\n",
            "[Coreset] SSE: 402250.22\n",
            "[OnlineKMeans] SSE: 354278.09\n",
            "[MiniBatchKMeans] SSE: 354300.38\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 500 ======\n",
            "初始SSE: 1079546.90\n",
            "[Coreset] SSE: 413605.97\n",
            "[OnlineKMeans] SSE: 370035.47\n",
            "[MiniBatchKMeans] SSE: 370077.41\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 1k ======\n",
            "初始SSE: 1079546.90\n",
            "[Coreset] SSE: 436316.12\n",
            "[OnlineKMeans] SSE: 389778.94\n",
            "[MiniBatchKMeans] SSE: 389886.84\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 2k ======\n",
            "初始SSE: 1079546.90\n",
            "[Coreset] SSE: 471775.19\n",
            "[OnlineKMeans] SSE: 429126.91\n",
            "[MiniBatchKMeans] SSE: 429487.09\n",
            "\n",
            "====== 数据集: glove (维度=300) | 测试数据量: 5k ======\n",
            "初始SSE: 1079546.90\n",
            "[Coreset] SSE: 617843.31\n",
            "[OnlineKMeans] SSE: 552147.69\n",
            "[MiniBatchKMeans] SSE: 554482.50\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 100 ======\n",
            "初始SSE: 280066.90\n",
            "[Coreset] SSE: 699021824.00\n",
            "[OnlineKMeans] SSE: 586606272.00\n",
            "[MiniBatchKMeans] SSE: 586677632.00\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 500 ======\n",
            "初始SSE: 280066.90\n",
            "[Coreset] SSE: 719656768.00\n",
            "[OnlineKMeans] SSE: 612793984.00\n",
            "[MiniBatchKMeans] SSE: 612915200.00\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 1k ======\n",
            "初始SSE: 280066.90\n",
            "[Coreset] SSE: 705449600.00\n",
            "[OnlineKMeans] SSE: 640815808.00\n",
            "[MiniBatchKMeans] SSE: 641051072.00\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 2k ======\n",
            "初始SSE: 280066.90\n",
            "[Coreset] SSE: 773137088.00\n",
            "[OnlineKMeans] SSE: 700423872.00\n",
            "[MiniBatchKMeans] SSE: 700846080.00\n",
            "\n",
            "====== 数据集: sift (维度=128) | 测试数据量: 5k ======\n",
            "初始SSE: 280066.90\n",
            "[Coreset] SSE: 958579264.00\n",
            "[OnlineKMeans] SSE: 854416576.00\n",
            "[MiniBatchKMeans] SSE: 856355520.00\n"
          ]
        }
      ]
    }
  ]
}