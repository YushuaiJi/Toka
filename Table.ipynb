{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOg0IStMch5B",
        "outputId": "9adf23d2-d2bb-4757-e462-232553797d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Running Lloyd on Huatuo-1024d_10k...\n",
            "sse_list: [np.float32(2667.1853), np.float32(2658.881), np.float32(2664.398), np.float32(2654.715), np.float32(2666.353), np.float32(2654.886), np.float32(2663.17), np.float32(2687.1843), np.float32(2654.109), np.float32(2671.2185)]\n",
            "balance_loss_list: [np.float64(1718538.7999999998), np.float64(3395060.8), np.float64(1823978.8), np.float64(2988678.8), np.float64(2776648.8), np.float64(1680162.8), np.float64(1018210.8), np.float64(8414936.8), np.float64(2122044.8), np.float64(9054240.8)]\n",
            "SSE_Mean: 2664.210205078125\n",
            "SSE_Var: 89.90210723876953\n",
            "BalanceLoss_Mean: 3499250.2\n",
            "BalanceLoss_Var: 7315438422508.842\n",
            "Running CDKM on Huatuo-1024d_10k...\n",
            "sse_list: [np.float64(2649.1871792725756), np.float64(2649.187062892103), np.float64(2655.154617549748), np.float64(2659.991362006629), np.float64(2657.138125999354), np.float64(2663.4083936770485), np.float64(2652.1866709480714), np.float64(2664.1451044247283), np.float64(2653.003185827281), np.float64(2652.4251577016066)]\n",
            "balance_loss_list: [np.float64(676234.8), np.float64(676410.8), np.float64(3072200.8), np.float64(3484076.8), np.float64(2490580.8000000003), np.float64(4721134.8), np.float64(3458278.8000000003), np.float64(3822462.8), np.float64(1248974.8), np.float64(1048068.8)]\n",
            "SSE_Mean: 2655.5826860299144\n",
            "SSE_Var: 26.655899581579764\n",
            "BalanceLoss_Mean: 2469842.4\n",
            "BalanceLoss_Var: 1921222606811.0398\n",
            "Running BCLS on Huatuo-1024d_10k...\n",
            "sse_list: [np.float32(3124.0237), np.float32(3128.3472), np.float32(3128.8655), np.float32(3125.9612), np.float32(3124.8784), np.float32(3126.5713), np.float32(3127.608), np.float32(3127.6265), np.float32(3128.2378), np.float32(3126.039)]\n",
            "balance_loss_list: [np.float64(503508.80000000005), np.float64(965630.7999999999), np.float64(63242.799999999996), np.float64(143394.80000000002), np.float64(194134.8), np.float64(633880.8), np.float64(232186.80000000002), np.float64(373098.8), np.float64(331190.8), np.float64(417038.80000000005)]\n",
            "SSE_Mean: 3126.81591796875\n",
            "SSE_Var: 2.2795803546905518\n",
            "BalanceLoss_Mean: 385730.8\n",
            "BalanceLoss_Var: 63885849386.4\n",
            "Running FCFC on Huatuo-1024d_10k...\n",
            "sse_list: [np.float64(3137.9481646053655), np.float64(3137.9481646053655), np.float64(3137.9481646053655), np.float64(3137.9481646053655), np.float64(3137.9481646053655), np.float64(3137.9481646053655), np.float64(3137.9481646053655), np.float64(3137.9481646053655), np.float64(3137.9481646053655), np.float64(3137.9481646053655)]\n",
            "balance_loss_list: [np.float64(80016000.80000003), np.float64(80016000.80000003), np.float64(80016000.80000003), np.float64(80016000.80000001), np.float64(80016000.80000003), np.float64(80016000.80000003), np.float64(80016000.80000001), np.float64(80016000.80000003), np.float64(80016000.80000001), np.float64(80016000.80000001)]\n",
            "SSE_Mean: 3137.9481646053655\n",
            "SSE_Var: 0.0\n",
            "BalanceLoss_Mean: 80016000.8\n",
            "BalanceLoss_Var: 6.217248937900876e-16\n",
            "Running BKNC on Huatuo-1024d_10k...\n",
            "sse_list: [np.float64(2672.5650841584643), np.float64(2662.751698180302), np.float64(2673.374392436675), np.float64(2691.1102991884372), np.float64(2671.6666993024874), np.float64(2673.231698720877), np.float64(2666.7207883428277), np.float64(2679.016903657025), np.float64(2670.015050059571), np.float64(2673.9070884565735)]\n",
            "balance_loss_list: [np.float64(657182.8), np.float64(355698.8), np.float64(440344.8), np.float64(582678.8), np.float64(583094.7999999999), np.float64(589586.8), np.float64(344252.8), np.float64(654522.7999999999), np.float64(497682.80000000005), np.float64(669018.7999999998)]\n",
            "SSE_Mean: 2673.435970250324\n",
            "SSE_Var: 51.863498709216984\n",
            "BalanceLoss_Mean: 537406.4\n",
            "BalanceLoss_Var: 13356915890.239994\n",
            "Running MyKMeans on Huatuo-1024d_10k...\n",
            "sse_list: [np.float64(3090.0992572954724), np.float64(3110.2623109030505), np.float64(3113.998660535177), np.float64(3118.0103964457976), np.float64(3117.9840036610785), np.float64(3116.3649990699205), np.float64(3070.996768162427), np.float64(3121.9506830096316), np.float64(3114.6582201521956), np.float64(3089.2508773297864)]\n",
            "balance_loss_list: [np.float64(999482.7999999999), np.float64(418742.8), np.float64(460946.8), np.float64(489550.8), np.float64(287612.8), np.float64(390786.8), np.float64(348038.8), np.float64(288822.8), np.float64(459044.80000000005), np.float64(478470.8)]\n",
            "SSE_Mean: 3106.357617656454\n",
            "SSE_Var: 256.41481654925735\n",
            "BalanceLoss_Mean: 462149.9999999999\n",
            "BalanceLoss_Var: 37025861000.15999\n",
            "Running Lloyd on LiveChat-1024d_10k...\n",
            "sse_list: [np.float32(1928.6753), np.float32(1930.57), np.float32(1930.0527), np.float32(1930.1453), np.float32(1930.1892), np.float32(1942.9879), np.float32(1934.9718), np.float32(1940.8993), np.float32(1929.0696), np.float32(1935.7405)]\n",
            "balance_loss_list: [np.float64(4153738.8), np.float64(2214678.8), np.float64(5099562.8), np.float64(9271024.8), np.float64(3258834.8), np.float64(6381990.800000001), np.float64(6498336.8), np.float64(1483686.8), np.float64(3679466.8000000003), np.float64(2732210.8)]\n",
            "SSE_Mean: 1933.330322265625\n",
            "SSE_Var: 23.72583770751953\n",
            "BalanceLoss_Mean: 4477353.2\n",
            "BalanceLoss_Var: 5043184332329.441\n",
            "Running CDKM on LiveChat-1024d_10k...\n",
            "sse_list: [np.float64(1929.4858036063054), np.float64(1929.114535623512), np.float64(1927.960719418604), np.float64(1927.9522543462417), np.float64(1928.9379658573298), np.float64(1928.388688112981), np.float64(1927.8589704499677), np.float64(1929.2907542614582), np.float64(1927.2252334933426), np.float64(1928.0012097302665)]\n",
            "balance_loss_list: [np.float64(4983056.8), np.float64(2208280.8), np.float64(3760594.8), np.float64(3820366.8000000003), np.float64(3742020.8), np.float64(3776266.8000000003), np.float64(4114820.8), np.float64(2178238.8), np.float64(4638102.800000001), np.float64(4113810.8000000003)]\n",
            "SSE_Mean: 1928.421613490001\n",
            "SSE_Var: 0.4993104174037142\n",
            "BalanceLoss_Mean: 3733556.0\n",
            "BalanceLoss_Var: 742505282856.1603\n",
            "Running BCLS on LiveChat-1024d_10k...\n",
            "sse_list: [np.float32(2033.1434), np.float32(2032.4966), np.float32(2033.4332), np.float32(2032.3406), np.float32(2033.007), np.float32(2033.2024), np.float32(2032.4559), np.float32(2033.1936), np.float32(2033.5123), np.float32(2032.7317)]\n",
            "balance_loss_list: [np.float64(362338.80000000005), np.float64(633234.8), np.float64(572282.8), np.float64(226024.8), np.float64(363164.8), np.float64(510754.80000000005), np.float64(853262.7999999999), np.float64(31058.800000000003), np.float64(419112.79999999993), np.float64(746884.7999999999)]\n",
            "SSE_Mean: 2032.9517822265625\n",
            "SSE_Var: 0.15821473300457\n",
            "BalanceLoss_Mean: 471812.0\n",
            "BalanceLoss_Var: 54007853406.56\n",
            "Running FCFC on LiveChat-1024d_10k...\n",
            "sse_list: [np.float64(2038.36565102446), np.float64(2038.36565102446), np.float64(2038.36565102446), np.float64(2038.36565102446), np.float64(2038.36565102446), np.float64(2038.36565102446), np.float64(2038.36565102446), np.float64(2038.36565102446), np.float64(2038.36565102446), np.float64(2038.36565102446)]\n",
            "balance_loss_list: [np.float64(80016000.80000003), np.float64(80016000.80000001), np.float64(80016000.80000001), np.float64(80016000.80000003), np.float64(80016000.80000001), np.float64(80016000.80000001), np.float64(80016000.80000001), np.float64(80016000.80000001), np.float64(80016000.80000001), np.float64(80016000.8)]\n",
            "SSE_Mean: 2038.3656510244605\n",
            "SSE_Var: 2.0679515313825692e-25\n",
            "BalanceLoss_Mean: 80016000.8\n",
            "BalanceLoss_Var: 3.3306690738754696e-16\n",
            "Running BKNC on LiveChat-1024d_10k...\n",
            "sse_list: [np.float64(1936.9072732430511), np.float64(1936.604222120216), np.float64(1938.9409404669098), np.float64(1932.576613079894), np.float64(1934.792954722853), np.float64(1937.2118325312294), np.float64(1932.135609626297), np.float64(1932.6493572413633), np.float64(1948.944351733186), np.float64(1935.5896965635952)]\n",
            "balance_loss_list: [np.float64(1147538.8), np.float64(767826.8), np.float64(856880.8), np.float64(1150662.8), np.float64(1190150.8), np.float64(1047828.7999999999), np.float64(1116632.8), np.float64(1119248.8), np.float64(1838382.8), np.float64(1055048.8)]\n",
            "SSE_Mean: 1936.6352851328597\n",
            "SSE_Var: 21.43314728860153\n",
            "BalanceLoss_Mean: 1129020.2000000002\n",
            "BalanceLoss_Var: 72457685384.04001\n",
            "Running MyKMeans on LiveChat-1024d_10k...\n",
            "sse_list: [np.float64(2030.6487137332197), np.float64(2037.5563335170905), np.float64(2037.6348702930868), np.float64(2037.588392423205), np.float64(2037.566136276177), np.float64(2037.586068522751), np.float64(2036.9577928334554), np.float64(2037.5300603123094), np.float64(2037.581703182518), np.float64(2037.5616898991213)]\n",
            "balance_loss_list: [np.float64(98300.79999999999), np.float64(8968.8), np.float64(5248.799999999999), np.float64(5092.799999999999), np.float64(5660.800000000001), np.float64(11506.8), np.float64(14070.8), np.float64(3854.7999999999997), np.float64(12212.800000000001), np.float64(7536.8)]\n",
            "SSE_Mean: 2036.8211760992933\n",
            "SSE_Var: 4.2678436394403665\n",
            "BalanceLoss_Mean: 17245.399999999994\n",
            "BalanceLoss_Var: 740618879.2399998\n",
            "Running Lloyd on deep_96d_10k...\n",
            "sse_list: [np.float32(8132.8804), np.float32(8126.2266), np.float32(8109.142), np.float32(8129.9883), np.float32(8103.2285), np.float32(8086.499), np.float32(8140.0737), np.float32(8111.456), np.float32(8134.4937), np.float32(8118.8926)]\n",
            "balance_loss_list: [np.float64(1014396.8), np.float64(1474244.8), np.float64(1659050.8), np.float64(425326.80000000005), np.float64(2203886.8000000003), np.float64(924836.8), np.float64(1645388.8), np.float64(1687552.7999999998), np.float64(574662.8), np.float64(1296910.7999999998)]\n",
            "SSE_Mean: 8119.2880859375\n",
            "SSE_Var: 250.8109893798828\n",
            "BalanceLoss_Mean: 1290625.8\n",
            "BalanceLoss_Var: 275839533236.2\n",
            "Running CDKM on deep_96d_10k...\n",
            "sse_list: [np.float64(8110.630506438794), np.float64(8118.525339839797), np.float64(8085.5506360640775), np.float64(8088.873850893544), np.float64(8102.523843341756), np.float64(8129.913599314151), np.float64(8086.019728335709), np.float64(8160.315415240991), np.float64(8110.18208673263), np.float64(8088.850739987591)]\n",
            "balance_loss_list: [np.float64(643442.8), np.float64(621286.8), np.float64(999306.8), np.float64(640090.7999999999), np.float64(689050.7999999999), np.float64(2482246.8), np.float64(960038.8), np.float64(5182422.8), np.float64(1683292.8), np.float64(579066.8)]\n",
            "SSE_Mean: 8108.138574618904\n",
            "SSE_Var: 508.89786368392595\n",
            "BalanceLoss_Mean: 1448024.6\n",
            "BalanceLoss_Var: 1882490758139.5598\n",
            "Running BCLS on deep_96d_10k...\n",
            "sse_list: [np.float32(8839.453), np.float32(8919.565), np.float32(9058.125), np.float32(8846.989), np.float32(8778.75), np.float32(9030.41), np.float32(8941.904), np.float32(8918.038), np.float32(8739.671), np.float32(8879.668)]\n",
            "balance_loss_list: [np.float64(8515384.8), np.float64(24797366.8), np.float64(16268236.8), np.float64(9678074.8), np.float64(16703852.8), np.float64(31447530.8), np.float64(12200678.8), np.float64(13107462.8), np.float64(11015378.800000003), np.float64(17853066.8)]\n",
            "SSE_Mean: 8895.2578125\n",
            "SSE_Var: 9154.599609375\n",
            "BalanceLoss_Mean: 16158703.400000002\n",
            "BalanceLoss_Var: 46340348351728.83\n",
            "Running FCFC on deep_96d_10k...\n",
            "sse_list: [np.float64(9305.086540218987), np.float64(9305.086540218987), np.float64(9305.086540218987), np.float64(9305.086540218987), np.float64(9305.086540218987), np.float64(9305.086540218987), np.float64(9305.086540218987), np.float64(9305.086540218987), np.float64(9305.086540218987), np.float64(9305.086540218987)]\n",
            "balance_loss_list: [np.float64(80016000.80000001), np.float64(80016000.8), np.float64(80016000.80000001), np.float64(80016000.80000001), np.float64(80016000.80000003), np.float64(80016000.8), np.float64(80016000.80000001), np.float64(80016000.80000003), np.float64(80016000.80000003), np.float64(80016000.80000003)]\n",
            "SSE_Mean: 9305.086540218985\n",
            "SSE_Var: 3.308722450212111e-24\n",
            "BalanceLoss_Mean: 80016000.80000003\n",
            "BalanceLoss_Var: 2.6645352591003756e-16\n",
            "Running BKNC on deep_96d_10k...\n",
            "sse_list: [np.float64(8331.44219597572), np.float64(8276.247051740045), np.float64(8185.230266713336), np.float64(8239.881270847702), np.float64(8311.17216655884), np.float64(8217.708407078126), np.float64(8218.215816756461), np.float64(8244.610309305504), np.float64(8248.026691968013), np.float64(8205.462404029144)]\n",
            "balance_loss_list: [np.float64(621100.7999999999), np.float64(180484.8), np.float64(213770.8), np.float64(364818.8), np.float64(394060.80000000005), np.float64(728070.8), np.float64(146182.8), np.float64(287048.80000000005), np.float64(379408.80000000005), np.float64(200894.8)]\n",
            "SSE_Mean: 8247.799658097289\n",
            "SSE_Var: 1938.2386399815023\n",
            "BalanceLoss_Mean: 351584.2\n",
            "BalanceLoss_Var: 33446436835.239998\n",
            "Running MyKMeans on deep_96d_10k...\n",
            "sse_list: [np.float64(8543.958402776056), np.float64(8486.601939750084), np.float64(8632.507907830102), np.float64(8500.062024933526), np.float64(8477.25721878674), np.float64(8612.165377720652), np.float64(8505.41069257506), np.float64(8475.135424021939), np.float64(8478.407564400255), np.float64(8513.368389817542)]\n",
            "balance_loss_list: [np.float64(47562.8), np.float64(37188.8), np.float64(138350.80000000002), np.float64(71182.8), np.float64(238476.8), np.float64(297280.80000000005), np.float64(56686.8), np.float64(166318.80000000002), np.float64(172080.8), np.float64(269944.8)]\n",
            "SSE_Mean: 8522.487494261197\n",
            "SSE_Var: 2900.4100606896673\n",
            "BalanceLoss_Mean: 149507.40000000005\n",
            "BalanceLoss_Var: 8293292660.839999\n",
            "Running Lloyd on glove_300d_10k...\n",
            "sse_list: [np.float32(364035.1), np.float32(364185.47), np.float32(364689.1), np.float32(364732.38), np.float32(364274.6), np.float32(364572.9), np.float32(364541.75), np.float32(365466.0), np.float32(364140.3), np.float32(363943.62)]\n",
            "balance_loss_list: [np.float64(1467902.8), np.float64(581446.7999999999), np.float64(116234.8), np.float64(1549314.8), np.float64(823218.8), np.float64(1935842.8), np.float64(1241626.8), np.float64(807126.8), np.float64(1409922.8), np.float64(1174346.8)]\n",
            "SSE_Mean: 364458.09375\n",
            "SSE_Var: 181723.515625\n",
            "BalanceLoss_Mean: 1110698.4000000001\n",
            "BalanceLoss_Var: 255543656145.44003\n",
            "Running CDKM on glove_300d_10k...\n",
            "sse_list: [np.float64(363822.5742331777), np.float64(364298.10653718706), np.float64(364113.0592954651), np.float64(363791.2767110402), np.float64(363954.0439926675), np.float64(364173.91857497545), np.float64(363760.17627359496), np.float64(364122.6333041142), np.float64(363707.00061608397), np.float64(363792.22082037403)]\n",
            "balance_loss_list: [np.float64(1217462.8), np.float64(1273924.8), np.float64(943578.8), np.float64(1177708.8), np.float64(1358862.8), np.float64(2090426.8), np.float64(1024900.7999999999), np.float64(75802.8), np.float64(916374.7999999999), np.float64(817198.8)]\n",
            "SSE_Mean: 363953.501035868\n",
            "SSE_Var: 38900.84880553703\n",
            "BalanceLoss_Mean: 1089624.2000000002\n",
            "BalanceLoss_Var: 228974674509.64005\n",
            "Running BCLS on glove_300d_10k...\n",
            "sse_list: [np.float32(383532.16), np.float32(384873.94), np.float32(384306.72), np.float32(385011.75), np.float32(384325.62), np.float32(384104.4), np.float32(386093.44), np.float32(383785.44), np.float32(384825.8), np.float32(386089.12)]\n",
            "balance_loss_list: [np.float64(2367650.8), np.float64(21270570.800000004), np.float64(1988034.7999999998), np.float64(6699322.8), np.float64(16762550.8), np.float64(13851620.8), np.float64(24590772.8), np.float64(7622018.799999999), np.float64(14415114.8), np.float64(18041482.8)]\n",
            "SSE_Mean: 384694.84375\n",
            "SSE_Var: 686418.375\n",
            "BalanceLoss_Mean: 12760914.0\n",
            "BalanceLoss_Var: 54740932157742.57\n",
            "Running FCFC on glove_300d_10k...\n",
            "sse_list: [np.float64(383660.70386220317), np.float64(379052.0007383973), np.float64(387400.4349121271), np.float64(385951.734269223), np.float64(387413.68190221826), np.float64(383507.4407216679), np.float64(385758.4504843934), np.float64(385282.93571684667), np.float64(381777.6893597678), np.float64(380306.9150262992)]\n",
            "balance_loss_list: [np.float64(29834288.8), np.float64(25276650.8), np.float64(74715232.80000001), np.float64(75277854.80000001), np.float64(78015612.80000001), np.float64(58894936.800000004), np.float64(63912040.8), np.float64(53199000.800000004), np.float64(29223246.800000004), np.float64(18059592.8)]\n",
            "SSE_Mean: 384011.19869931444\n",
            "SSE_Var: 7518024.0839045225\n",
            "BalanceLoss_Mean: 50640845.800000004\n",
            "BalanceLoss_Var: 478324177833155.5\n",
            "Running BKNC on glove_300d_10k...\n",
            "sse_list: [np.float64(366524.0276266043), np.float64(366924.96897179174), np.float64(365063.9041615189), np.float64(365491.920694712), np.float64(366469.97055145), np.float64(364763.21670930594), np.float64(366493.53896739066), np.float64(366253.4820339117), np.float64(364902.9810433668), np.float64(365337.25188850926)]\n",
            "balance_loss_list: [np.float64(73322.8), np.float64(145290.80000000002), np.float64(385722.8), np.float64(309428.80000000005), np.float64(222114.8), np.float64(242356.8), np.float64(148748.8), np.float64(141070.8), np.float64(280302.8), np.float64(45882.8)]\n",
            "SSE_Mean: 365822.52626485616\n",
            "SSE_Var: 565064.8016562832\n",
            "BalanceLoss_Mean: 199424.2\n",
            "BalanceLoss_Var: 10408766484.039999\n",
            "Running MyKMeans on glove_300d_10k...\n",
            "sse_list: [np.float64(365194.99456469476), np.float64(364837.60555810813), np.float64(365177.8398397861), np.float64(366806.4049275366), np.float64(367032.4620306614), np.float64(366999.63700480084), np.float64(366450.03689659596), np.float64(366082.6567702354), np.float64(366866.55484761926), np.float64(365228.10461679584)]\n",
            "balance_loss_list: [np.float64(77574.8), np.float64(277388.8), np.float64(241898.80000000002), np.float64(106898.79999999999), np.float64(48644.799999999996), np.float64(289290.8), np.float64(141186.8), np.float64(281844.8), np.float64(306530.8), np.float64(120250.8)]\n",
            "SSE_Mean: 366067.62970568344\n",
            "SSE_Var: 690105.0177334545\n",
            "BalanceLoss_Mean: 189151.0\n",
            "BalanceLoss_Var: 8897084439.56\n",
            "Running Lloyd on sift_128d_10k...\n",
            "sse_list: [np.float32(654041600.0), np.float32(653529400.0), np.float32(652913800.0), np.float32(653004500.0), np.float32(655992800.0), np.float32(652021440.0), np.float32(657557440.0), np.float32(663281150.0), np.float32(652788000.0), np.float32(654021440.0)]\n",
            "balance_loss_list: [np.float64(94274.8), np.float64(137918.8), np.float64(50626.799999999996), np.float64(123254.80000000002), np.float64(97748.8), np.float64(1519142.7999999998), np.float64(779636.8), np.float64(434382.8), np.float64(1235146.8), np.float64(125072.79999999999)]\n",
            "SSE_Mean: 654915200.0\n",
            "SSE_Var: 10216973271040.0\n",
            "BalanceLoss_Mean: 459720.6\n",
            "BalanceLoss_Var: 258733630565.9599\n",
            "Running CDKM on sift_128d_10k...\n",
            "sse_list: [np.float64(649673187.425181), np.float64(649887551.3639487), np.float64(649842718.091804), np.float64(652241560.2923993), np.float64(649818313.5755237), np.float64(650136052.1181301), np.float64(652548410.4466001), np.float64(649830339.4900126), np.float64(649825134.5349649), np.float64(652288671.9274392)]\n",
            "balance_loss_list: [np.float64(1011260.8), np.float64(1044264.7999999999), np.float64(1088774.8), np.float64(82238.79999999999), np.float64(1110750.8), np.float64(657678.8), np.float64(174348.80000000002), np.float64(992348.8), np.float64(1094844.8), np.float64(84370.8)]\n",
            "SSE_Mean: 650609193.9266002\n",
            "SSE_Var: 1330090664683.524\n",
            "BalanceLoss_Mean: 734088.2\n",
            "BalanceLoss_Var: 180374226625.63998\n",
            "Running BCLS on sift_128d_10k...\n",
            "sse_list: [np.float32(835623000.0), np.float32(848574460.0), np.float32(845672800.0), np.float32(830425900.0), np.float32(858921200.0), np.float32(844213400.0), np.float32(846333900.0), np.float32(855540300.0), np.float32(842869950.0), np.float32(854938750.0)]\n",
            "balance_loss_list: [np.float64(16709750.800000003), np.float64(39108834.800000004), np.float64(18508268.8), np.float64(19646578.8), np.float64(29910594.800000004), np.float64(24934230.800000004), np.float64(16832106.8), np.float64(22615872.8), np.float64(23520258.8), np.float64(14234010.8)]\n",
            "SSE_Mean: 846311296.0\n",
            "SSE_Var: 70697695051776.0\n",
            "BalanceLoss_Mean: 22602050.800000004\n",
            "BalanceLoss_Var: 49570050625498.42\n",
            "Running FCFC on sift_128d_10k...\n",
            "sse_list: [np.float64(653744647.1835859), np.float64(655689677.5678368), np.float64(659547620.1523557), np.float64(653169595.4415131), np.float64(650059884.2127861), np.float64(652085557.8254654), np.float64(652675882.7956882), np.float64(657111883.1585202), np.float64(654727559.731451), np.float64(657884606.4967985)]\n",
            "balance_loss_list: [np.float64(360210.80000000005), np.float64(91692.79999999999), np.float64(704688.7999999999), np.float64(47902.8), np.float64(1125984.8), np.float64(415332.80000000005), np.float64(245096.8), np.float64(241470.8), np.float64(62846.8), np.float64(1336036.8)]\n",
            "SSE_Mean: 654669691.4566001\n",
            "SSE_Var: 7614722517637.237\n",
            "BalanceLoss_Mean: 463126.4\n",
            "BalanceLoss_Var: 183984749376.64\n",
            "Running BKNC on sift_128d_10k...\n",
            "sse_list: [np.float64(661803502.9707683), np.float64(657300195.2960752), np.float64(654404492.5055566), np.float64(660285589.3961862), np.float64(656822998.859295), np.float64(654829592.4120334), np.float64(657288541.4803202), np.float64(676108699.7704635), np.float64(660703363.1237231), np.float64(654961658.9753885)]\n",
            "balance_loss_list: [np.float64(71262.8), np.float64(182190.80000000002), np.float64(193140.80000000002), np.float64(188260.80000000002), np.float64(290402.80000000005), np.float64(158814.8), np.float64(161108.8), np.float64(452250.80000000005), np.float64(198044.80000000002), np.float64(173454.8)]\n",
            "SSE_Mean: 659450863.478981\n",
            "SSE_Var: 36846559107803.62\n",
            "BalanceLoss_Mean: 206893.20000000004\n",
            "BalanceLoss_Var: 9232047858.240002\n",
            "Running MyKMeans on sift_128d_10k...\n",
            "sse_list: [np.float64(661796659.2616876), np.float64(657300195.2960752), np.float64(654404492.5055566), np.float64(660298169.8956652), np.float64(656822998.859295), np.float64(654832894.4976118), np.float64(657288541.4803202), np.float64(676059917.2512555), np.float64(660740586.5385745), np.float64(654964789.0859095)]\n",
            "balance_loss_list: [np.float64(70434.8), np.float64(182190.80000000002), np.float64(193140.80000000002), np.float64(186598.80000000002), np.float64(290402.80000000005), np.float64(158738.8), np.float64(161108.8), np.float64(452878.79999999993), np.float64(198262.8), np.float64(172926.8)]\n",
            "SSE_Mean: 659450924.4671952\n",
            "SSE_Var: 36686778225421.01\n",
            "BalanceLoss_Mean: 206668.4\n",
            "BalanceLoss_Var: 9295761567.839998\n",
            "\n",
            "Experiment results saved to 'metrics_summary.csv'\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from scipy.linalg import orth # For creating orthogonal matrices\n",
        "\n",
        "class FCFC:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data\n",
        "        # k: number of clusters\n",
        "        self.d_features = d # Renamed for clarity, consistent with other classes\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.max_iter = niter # Keep for consistency with existing loop\n",
        "\n",
        "        # Other parameters (some might not be used by this specific FCFC logic but kept for interface)\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu # This FCFC implementation is CPU-based\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "        self.lambda_ = lambda_  # Balance parameter for the objective function in get_distance\n",
        "\n",
        "        # Results storage\n",
        "        self.centroids = None               # Final cluster centroids (k, d_features)\n",
        "        self.labels_ = None                 # Final cluster assignments for each point (n_samples,)\n",
        "        self.runtime_ = None                # Total training time\n",
        "\n",
        "        self.objective_history_ = None      # History of sum_dis (sum of D(i,j) values)\n",
        "        self.sse_history_ = None            # History of Sum of Squared Errors per iteration\n",
        "        self.balance_loss_history_ = None   # History of Balance Loss per iteration\n",
        "\n",
        "        self.final_objective_ = None        # Final value from objective_history_\n",
        "        self.final_sse_ = None              # Final Sum of Squared Errors\n",
        "        self.final_balance_loss_ = None     # Final Balance Loss\n",
        "        self.final_cluster_sizes_ = None    # Final size of each cluster (k,)\n",
        "\n",
        "        self.sse_ = 0\n",
        "        self.balance_loss_ = 0\n",
        "\n",
        "        # For compatibility, self.obj can point to the primary objective history\n",
        "        self.obj = None\n",
        "\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids_arg=None): # Renamed init_centroids to avoid conflict\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility\n",
        "        start_time = time.time()\n",
        "\n",
        "        K = self.k\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "        n_samples, d_features_data = x.shape # n_samples, d in original code\n",
        "\n",
        "        # Initialize arrays for storing per-iteration metrics\n",
        "        # sse_history stores the traditional SSE\n",
        "        # balance_loss_history stores the balance penalty\n",
        "        # objective_value_history stores the sum of D(i,j) which is being minimized directly\n",
        "        sse_history = np.zeros(self.max_iter)\n",
        "        balance_loss_history = np.zeros(self.max_iter)\n",
        "        objective_value_history = np.zeros(self.max_iter) # Corresponds to pre_dis\n",
        "\n",
        "        # Initialize centroids\n",
        "        # If init_centroids_arg is provided, use it, otherwise use random initialization\n",
        "        if init_centroids_arg is not None:\n",
        "            if init_centroids_arg.shape != (K, d_features_data):\n",
        "                raise ValueError(f\"Provided init_centroids shape {init_centroids_arg.shape} \"\n",
        "                                 f\"is not ({K}, {d_features_data})\")\n",
        "            current_centroids = np.copy(init_centroids_arg)\n",
        "        else:\n",
        "            current_centroids = initial_centroid(x, K, n_samples) # Uses np.random internally\n",
        "\n",
        "        # size_cluster is 1*K vector, stores size of each cluster for the get_distance objective\n",
        "        # Initialized to ones to avoid issues if lambda_ > 0 and a cluster is initially empty,\n",
        "        # though it gets updated immediately in the first iteration.\n",
        "        # A more common initialization might be n_samples/K or based on initial assignment.\n",
        "        # Let's base it on an initial quick assignment or n_samples/K to be more robust.\n",
        "        # For simplicity of matching the provided code, it starts with ones and is quickly updated.\n",
        "        current_size_cluster = np.ones(K) # Will be updated after first assignment\n",
        "\n",
        "        current_labels = np.zeros(n_samples, dtype=int) # To store labels for each point\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Assignment step: Calculate D matrix and assign points to clusters\n",
        "            # D(point, cluster_j) = distance(point, centroid_j)^2 + lambda * size_cluster_j\n",
        "            D_matrix = get_distance(x, current_centroids, K, n_samples, d_features_data,\n",
        "                                    current_size_cluster, self.lambda_)\n",
        "\n",
        "            min_dist_to_centroid_plus_balance = np.min(D_matrix, axis=1) # (n_samples,)\n",
        "            assigned_labels = np.argmin(D_matrix, axis=1)           # (n_samples,)\n",
        "            sum_objective_values = np.sum(min_dist_to_centroid_plus_balance)\n",
        "\n",
        "            current_labels = assigned_labels\n",
        "            objective_value_history[i] = sum_objective_values\n",
        "\n",
        "            # Update step: Recalculate centroids and cluster sizes\n",
        "            # current_size_cluster is based on the new assignments\n",
        "            current_size_cluster = np.bincount(current_labels, minlength=K)\n",
        "            current_centroids = get_centroid(x, current_labels, K, n_samples, d_features_data)\n",
        "\n",
        "            # Calculate SSE and Balance Loss for this iteration (for monitoring)\n",
        "            iter_sse = 0\n",
        "            iter_balance_penalty_terms = np.zeros(K)\n",
        "\n",
        "            for j in range(K):\n",
        "                cluster_points = x[current_labels == j, :]\n",
        "                if cluster_points.shape[0] > 0: # If cluster is not empty\n",
        "                    # SSE part: sum of squared distances to its actual centroid\n",
        "                    iter_sse += np.sum(np.sum((cluster_points - current_centroids[j, :])**2, axis=1))\n",
        "                # Balance loss part (using current_size_cluster which is already updated)\n",
        "                iter_balance_penalty_terms[j] = (current_size_cluster[j] - n_samples / K)**2\n",
        "\n",
        "            sse_history[i] = iter_sse\n",
        "            balance_loss_history[i] = np.sum(iter_balance_penalty_terms)\n",
        "\n",
        "            if self.verbose and (i % 5 == 0 or i == self.max_iter -1) :\n",
        "                print(f\"Iter {i+1}/{self.max_iter}: Objective={objective_value_history[i]:.4f}, \"\n",
        "                      f\"SSE={sse_history[i]:.4f}, BalanceLoss={balance_loss_history[i]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        # Store results\n",
        "        self.centroids = current_centroids\n",
        "        self.labels_ = current_labels\n",
        "        self.final_cluster_sizes_ = current_size_cluster\n",
        "\n",
        "        self.objective_history_ = objective_value_history\n",
        "        self.sse_history_ = sse_history\n",
        "        self.balance_loss_history_ = balance_loss_history\n",
        "\n",
        "        self.final_objective_ = objective_value_history[-1]\n",
        "        self.final_sse_ = sse_history[-1]\n",
        "        self.final_balance_loss_ = balance_loss_history[-1]\n",
        "\n",
        "        self.obj = self.objective_history_ # Storing the history of the optimized objective\n",
        "\n",
        "        self.sse_ = self.final_sse_\n",
        "        self.balance_loss_ = self.final_balance_loss_\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"FCFC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final Objective (sum of D(i,j)): {self.final_objective_:.4f}\")\n",
        "            print(f\"Final SSE: {self.final_sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.final_balance_loss_:.4f}\")\n",
        "            print(f\"Final cluster sizes: {self.final_cluster_sizes_}\")\n",
        "\n",
        "class Lloyd:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.centroids = None\n",
        "        self.obj_history_ = None\n",
        "        self.labels_ = None\n",
        "\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "        self.obj = None\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning random point.\")\n",
        "                centroids[j] = data_points[np.random.randint(len(data_points))]\n",
        "\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        start_time = time.time()\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input dimension {x_orig_data.shape[1]} != {self.d}\")\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        x = np.ascontiguousarray(x_orig_data, dtype='float32')\n",
        "\n",
        "        kmeans = faiss.Kmeans(\n",
        "            d=self.d,\n",
        "            k=self.k,\n",
        "            niter=self.niter,\n",
        "            nredo=self.nredo,\n",
        "            verbose=self.verbose,\n",
        "            min_points_per_centroid=self.min_points_per_centroid,\n",
        "            max_points_per_centroid=self.max_points_per_centroid,\n",
        "            seed=self.seed,\n",
        "            gpu=self.gpu,\n",
        "            spherical=self.spherical,\n",
        "            update_index=self.update_index,\n",
        "            frozen_centroids=self.frozen_centroids\n",
        "        )\n",
        "\n",
        "        kmeans.train(x, init_centroids=init_centroids)\n",
        "\n",
        "        _, self.labels_ = kmeans.index.search(x, 1)\n",
        "        self.labels_ = self.labels_.flatten()\n",
        "\n",
        "        self.centroids = kmeans.centroids\n",
        "        self.obj_history_ = kmeans.obj if kmeans.obj is not None and len(kmeans.obj) > 0 else np.zeros(self.niter)\n",
        "        self.obj = kmeans.obj[-1] if kmeans.obj is not None and len(kmeans.obj) > 0 else None\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "\n",
        "        # Print every 5th iteration's objective value\n",
        "        if self.verbose and self.obj_history_ is not None and len(self.obj_history_) > 0:\n",
        "            print(\"\\n--- Objective Value (every 5 iterations) ---\")\n",
        "            for i, val in enumerate(self.obj_history_):\n",
        "                if (i + 1) % 5 == 0 or i == len(self.obj_history_) - 1:\n",
        "                    print(f\"  Iter {i+1:2d}: {val:.6f}\")\n",
        "\n",
        "        final_sse = 0\n",
        "        for i in range(n):\n",
        "            cluster_idx = self.labels_[i]\n",
        "            final_sse += np.sum((x_orig_data[i] - self.centroids[cluster_idx]) ** 2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal = n / self.k\n",
        "            final_balance_loss = np.sum((sizes - ideal) ** 2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Lloyd training finished in {self.runtime_:.4f}s\")\n",
        "            print(f\"Final obj: {self.obj}\")\n",
        "            print(f\"Cluster sizes: {dict(zip(*np.unique(self.labels_, return_counts=True)))}\")\n",
        "            print(f\"SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class BCLS:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False,\n",
        "                 lambda_=1.0): # lambda_ from FCFC, but BCLS uses 'lam' internally\n",
        "        # d: dimensionality of data\n",
        "        # k: number of clusters\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo # Not used by BCLS algorithm itself\n",
        "        self.verbose = verbose\n",
        "        # The following Faiss-like parameters are not directly used by BCLS's core logic:\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.seed = seed\n",
        "        self.lambda_bcls = lambda_ # BCLS specific lambda for sum_Y term in objective\n",
        "                                  # If the lambda_ parameter was meant for this, it's used as 'lam' below.\n",
        "                                  # If it was for something else, then 'lam' needs its own source.\n",
        "                                  # Assuming lambda_ is the 'lam' for BCLS objective.\n",
        "\n",
        "        # Results storage\n",
        "        self.centroids = None       # Will store centroids in original data space\n",
        "        self.obj_history_ = None    # Stores Obj2 from the loop\n",
        "        self.labels_ = None         # Final cluster assignments (0-indexed)\n",
        "        self.Y_final_ = None        # Final Y matrix (one-hot indicators)\n",
        "\n",
        "        # Final metrics\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "\n",
        "        # For compatibility with previous structure if any part expects 'obj'\n",
        "        self.obj = None\n",
        "\n",
        "\n",
        "    def init1(self, n_samples, num_clusters):\n",
        "        \"\"\"\n",
        "        Initializes the Y matrix (n_samples x num_clusters) with one-hot encoding.\n",
        "        Labels are 1 to num_clusters, then converted to 0-indexed for Python.\n",
        "        \"\"\"\n",
        "        # np.random is affected by self.seed if set before calling train\n",
        "        labels_1_indexed = np.random.randint(1, num_clusters + 1, size=n_samples)\n",
        "        F = np.zeros((n_samples, num_clusters))\n",
        "        F[np.arange(n_samples), labels_1_indexed - 1] = 1\n",
        "        # F = csr_matrix(F) # Can be sparse if n and k are very large\n",
        "        return F\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates centroids from data points and their labels.\n",
        "        data_points: (n_samples, n_features) - original or centered\n",
        "        labels: (n_samples,) - 0-indexed\n",
        "        num_clusters: k\n",
        "        data_dim: d\n",
        "        Returns: (num_clusters, data_dim) centroids\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data. Returning zero centroids.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i, :]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning a random data point as its centroid.\")\n",
        "                if len(data_points) > 0:\n",
        "                    # Seed this random choice for consistency if multiple empty clusters\n",
        "                    rng_empty_fallback = np.random.RandomState(self.seed + j + 1000) # Offset seed\n",
        "                    centroids[j] = data_points[rng_empty_fallback.choice(len(data_points)), :]\n",
        "                # else: centroids[j] remains zeros\n",
        "        return centroids\n",
        "\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None): # x_orig_data is n x dim\n",
        "        np.random.seed(self.seed) # Ensure reproducibility for operations within train\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input data feature dimension {x_orig_data.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d}\")\n",
        "\n",
        "        ITER = self.niter\n",
        "        # BCLS Algorithm Hyperparameters (taken from the provided snippet)\n",
        "        gamma = 0.00001  # Regularization for W\n",
        "        lam = self.lambda_bcls # Controls balance term in objective (sum_Y**2)\n",
        "        mu = 0.01        # ALM parameter\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        c = self.k  # number of clusters\n",
        "\n",
        "        # Initialize Y\n",
        "        Y = self.init1(n, c) # Y is n x c\n",
        "\n",
        "        # Center the data (BCLS works with centered data)\n",
        "        meanX = np.mean(x_orig_data, axis=0, keepdims=True) # 1 x dim\n",
        "        x_centered = x_orig_data - meanX # n x dim\n",
        "\n",
        "        # ALM variables\n",
        "        Lambda_alm = np.zeros((n, c)) # Lagrange multipliers for Y - Z = 0\n",
        "        rho = 1.005                # Update factor for mu\n",
        "\n",
        "        # Precompute part of W update\n",
        "        # P_inv = x_centered.T @ x_centered + gamma * np.eye(dim)\n",
        "        # P = np.linalg.inv(P_inv)\n",
        "        # Using pseudo-inverse for potentially better stability if P_inv is singular/ill-conditioned\n",
        "        try:\n",
        "            P = np.linalg.inv(x_centered.T @ x_centered + gamma * np.eye(dim))\n",
        "        except np.linalg.LinAlgError:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Standard inverse failed for P. Using pseudo-inverse.\")\n",
        "            P = np.linalg.pinv(x_centered.T @ x_centered + gamma * np.eye(dim))\n",
        "\n",
        "\n",
        "        obj_history = np.zeros(ITER)\n",
        "        # Optional: if you want to track SSE/BalanceLoss per iteration (on centered data)\n",
        "        # sse_iter_history = np.zeros(ITER)\n",
        "        # balance_loss_iter_history = np.zeros(ITER)\n",
        "\n",
        "\n",
        "        for iter_idx in range(ITER):\n",
        "            # --- Solve W and b ---\n",
        "            # W: dim x c, b: 1 x c\n",
        "            W = P @ (x_centered.T @ Y)\n",
        "            b = np.mean(Y, axis=0, keepdims=True) # Or (1/n) * (np.ones((1,n)) @ Y)\n",
        "\n",
        "            # E = XW + 1b' - Y (Error term for reconstruction using centered X)\n",
        "            # E_recon: n x c\n",
        "            E_recon = x_centered @ W + np.ones((n, 1)) @ b - Y\n",
        "\n",
        "            # --- Solve Z (auxiliary variable for Y) ---\n",
        "            # Z: n x c\n",
        "            # Denominator matrix for Z update:\n",
        "            # Factor = mu**2 + 2 * n * lam * mu  (scalar)\n",
        "            # Coeff_matrix_inv = (-2 * lam * np.ones((n,n)) + (mu + 2 * n * lam) * np.eye(n)) / Factor\n",
        "            # Z = Coeff_matrix_inv @ (mu * Y + Lambda_alm)\n",
        "            # Simpler if Z is updated element-wise or if structure allows.\n",
        "            # The provided formula for Z seems like a direct solution from a specific formulation.\n",
        "            # Let's assume the formula is correct as given:\n",
        "            # Note: (mu**2 + 2 * n * lam * mu) is a scalar.\n",
        "            # The matrix to invert for Z is effectively ( (mu + 2*n*lam)*I - 2*lam*J ), where J is all-ones matrix.\n",
        "            # This matrix has a specific inverse (Sherman-Woodbury).\n",
        "            # For now, using the provided direct calculation:\n",
        "            mat_for_Z_inv_num = -2 * lam * np.ones((n, n)) + (mu + 2 * n * lam) * np.eye(n)\n",
        "            mat_for_Z_inv_den = (mu**2 + 2 * n * lam * mu)\n",
        "            if np.abs(mat_for_Z_inv_den) < 1e-9: # Avoid division by zero\n",
        "                 if self.verbose: print(f\"Warning: Denominator for Z is near zero at iter {iter_idx}\")\n",
        "                 Z = Y # Fallback or handle error\n",
        "            else:\n",
        "                 Z = (mat_for_Z_inv_num / mat_for_Z_inv_den) @ (mu * Y + Lambda_alm)\n",
        "\n",
        "\n",
        "            # --- Solve Y (indicator matrix) ---\n",
        "            # V: n x c\n",
        "            V_update = (1 / (2 + mu)) * (2 * x_centered @ W + 2 * np.ones((n, 1)) @ b + mu * Z - Lambda_alm)\n",
        "\n",
        "            # Update Y by selecting the max element in each row of V_update\n",
        "            current_labels = np.argmax(V_update, axis=1) # n-element array of 0-indexed labels\n",
        "            Y = np.zeros((n, c))\n",
        "            Y[np.arange(n), current_labels] = 1\n",
        "\n",
        "            # --- Update Lambda (Lagrange multipliers) and mu (penalty parameter) for ALM ---\n",
        "            Lambda_alm = Lambda_alm + mu * (Y - Z)\n",
        "            mu = min(mu * rho, 1e5) # Cap mu to avoid very large values\n",
        "\n",
        "            # --- Calculate Objective Value (for centered data) ---\n",
        "            sum_Y_elements = np.sum(Y) # Sum of all elements in Y (should be n if Y is strictly one-hot)\n",
        "            obj_history[iter_idx] = np.trace(E_recon.T @ E_recon) + \\\n",
        "                                    gamma * np.trace(W.T @ W) + \\\n",
        "                                    lam * (sum_Y_elements**2) # Or lam * np.sum( (np.sum(Y, axis=0) - n/c)**2 ) if balance is per cluster size\n",
        "\n",
        "\n",
        "            # --- In-loop SSE and Balance Loss (on centered data, for monitoring if needed) ---\n",
        "            # These are calculated based on current Y and centered data.\n",
        "            # Centroids for centered data: c x dim\n",
        "            temp_centroids_centered = self.compute_centroids_from_data(x_centered, current_labels, c, dim)\n",
        "\n",
        "            sse_iter = 0\n",
        "            for i in range(n):\n",
        "                cluster_idx = current_labels[i]\n",
        "                # Using np.sum for squared norm for clarity with dimensions\n",
        "                sse_iter += np.sum((x_centered[i, :] - temp_centroids_centered[cluster_idx, :])**2)\n",
        "            # sse_iter_history[iter_idx] = sse_iter\n",
        "\n",
        "            cluster_sizes_iter = np.sum(Y, axis=0) # n_elements per cluster (1 x c)\n",
        "            ideal_size_iter = n / c\n",
        "            balance_loss_iter = np.sum((cluster_sizes_iter - ideal_size_iter)**2)\n",
        "            # balance_loss_iter_history[iter_idx] = balance_loss_iter\n",
        "\n",
        "            if self.verbose and (iter_idx % 10 == 0 or iter_idx == ITER -1):\n",
        "                print(f\"Iter {iter_idx+1}/{ITER}, BCLS Obj: {obj_history[iter_idx]:.4f}, \"\n",
        "                      f\"Iter SSE (centered): {sse_iter:.2f}, Iter Bal (centered): {balance_loss_iter:.2f}\")\n",
        "\n",
        "\n",
        "        # --- End of iterations ---\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        # Store final results\n",
        "        self.labels_ = np.argmax(Y, axis=1) # Final 0-indexed labels\n",
        "        self.Y_final_ = Y                   # Final one-hot indicator matrix\n",
        "        self.obj_history_ = obj_history\n",
        "        self.obj = obj_history # Compatibility\n",
        "\n",
        "        # Calculate final centroids in ORIGINAL data space\n",
        "        # Use x_orig_data and self.labels_\n",
        "        final_centroids_orig_space = self.compute_centroids_from_data(x_orig_data, self.labels_, c, dim)\n",
        "        self.centroids = final_centroids_orig_space # Store k x dim centroids\n",
        "\n",
        "        # Calculate final SSE using ORIGINAL data and ORIGINAL space centroids\n",
        "        final_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point_orig = x_orig_data[i, :]\n",
        "                centroid_orig = self.centroids[cluster_idx, :]\n",
        "                final_sse += np.sum((point_orig - centroid_orig)**2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        # Calculate final Balance Loss\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            final_cluster_sizes = np.bincount(self.labels_, minlength=c)\n",
        "            ideal_size = n / c\n",
        "            final_balance_loss = np.sum((final_cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BCLS training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BCLS objective value: {self.obj_history_[-1]:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of final centroids (original space): {self.centroids.shape}\")\n",
        "            print(f\"Final SSE (original space): {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "\n",
        "    def compute_centroids(self, x_transposed, F_indicator):\n",
        "        \"\"\"\n",
        "        Computes centroids.\n",
        "        x_transposed: (dim, n_samples) data matrix (e.g., centered data transposed)\n",
        "        F_indicator: (n_samples, k) one-hot cluster indicator matrix\n",
        "        Returns: (k, dim) centroids\n",
        "        DEPRECATED in favor of compute_centroids_from_data for clarity, but kept if used elsewhere.\n",
        "        This version is slightly different from compute_centroids_from_data input format.\n",
        "        \"\"\"\n",
        "        num_clusters = F_indicator.shape[1]\n",
        "        data_dim = x_transposed.shape[0]\n",
        "        n_samples_check = x_transposed.shape[1]\n",
        "\n",
        "        if F_indicator.shape[0] != n_samples_check:\n",
        "            raise ValueError(\"Mismatch in number of samples between x_transposed and F_indicator.\")\n",
        "\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        # Determine labels from F_indicator\n",
        "        labels = np.argmax(F_indicator, axis=1) # (n_samples,)\n",
        "\n",
        "        for i in range(n_samples_check):\n",
        "            cluster_label = labels[i]\n",
        "            centroids[cluster_label] += x_transposed[:, i] # x_transposed[:, i] is a data point (dim,)\n",
        "            counts[cluster_label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning (compute_centroids): Cluster {j} is empty. Assigning random point.\")\n",
        "                if n_samples_check > 0:\n",
        "                    rng_empty_fallback = np.random.RandomState(self.seed + j + 2000)\n",
        "                    centroids[j] = x_transposed[:, rng_empty_fallback.choice(n_samples_check)]\n",
        "        return centroids\n",
        "\n",
        "class CDKM_PurePy:\n",
        "    def __init__(self, X: np.ndarray, c_true: int, debug: int = 0):\n",
        "        self.X = X.astype(np.float64)  # shape (N, dim)\n",
        "        self.N, self.dim = self.X.shape\n",
        "        self.c_true = c_true\n",
        "        self.debug = debug\n",
        "\n",
        "        self.Y = []             # replicate list of label vectors\n",
        "        self.n_iter_ = []       # number of iterations per replicate\n",
        "\n",
        "        if debug:\n",
        "            print(f\"N = {self.N}, dim = {self.dim}, k = {self.c_true}\")\n",
        "\n",
        "    def opt(self, init_Y: np.ndarray, ITER: int):\n",
        "        \"\"\"\n",
        "        init_Y: (rep, N) array of integer labels\n",
        "        \"\"\"\n",
        "        rep = init_Y.shape[0]\n",
        "        for rep_i in range(rep):\n",
        "            y = init_Y[rep_i].copy()\n",
        "            n_iter = self.opt_once(y, ITER)\n",
        "            self.Y.append(y)\n",
        "            self.n_iter_.append(n_iter)\n",
        "\n",
        "    def opt_once(self, y: np.ndarray, ITER: int) -> int:\n",
        "        \"\"\"\n",
        "        y: shape (N,), initial cluster assignment\n",
        "        \"\"\"\n",
        "        X = self.X\n",
        "        N, dim, c_true = self.N, self.dim, self.c_true\n",
        "\n",
        "        xnorm = np.sum(X**2, axis=1)  # shape (N,)\n",
        "        Sx = np.zeros((dim, c_true))\n",
        "        n = np.zeros(c_true)\n",
        "\n",
        "        for i in range(N):\n",
        "            Sx[:, y[i]] += X[i]\n",
        "            n[y[i]] += 1\n",
        "\n",
        "        s = np.sum(Sx**2, axis=0)  # squared norm of each cluster sum vector\n",
        "\n",
        "        for iter in range(ITER):\n",
        "            converge = True\n",
        "            for i in range(N):\n",
        "                c_old = y[i]\n",
        "                if n[c_old] == 1:\n",
        "                    continue\n",
        "\n",
        "                xi = X[i]\n",
        "                xiSx = xi @ Sx  # (c,)\n",
        "                tmp1 = s + 2 * xiSx + xnorm[i]\n",
        "                tmp1 = tmp1 / (n + 1)\n",
        "                tmp2 = s / n\n",
        "\n",
        "                delta = tmp1 - tmp2\n",
        "                delta[c_old] = s[c_old] / n[c_old] - \\\n",
        "                    (s[c_old] - 2 * xiSx[c_old] + xnorm[i]) / (n[c_old] - 1)\n",
        "\n",
        "                c_new = np.argmax(delta)\n",
        "\n",
        "                if c_new != c_old:\n",
        "                    converge = False\n",
        "                    y[i] = c_new\n",
        "\n",
        "                    Sx[:, c_old] -= xi\n",
        "                    Sx[:, c_new] += xi\n",
        "\n",
        "                    s[c_old] = np.sum(Sx[:, c_old]**2)\n",
        "                    s[c_new] = np.sum(Sx[:, c_new]**2)\n",
        "\n",
        "                    n[c_old] -= 1\n",
        "                    n[c_new] += 1\n",
        "\n",
        "                if self.debug and i % 10000 == 0:\n",
        "                    print(f\"i = {i}\")\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"iter = {iter}\")\n",
        "\n",
        "            if converge:\n",
        "                break\n",
        "\n",
        "        # if iter + 1 == ITER:\n",
        "            # print(\"not converge\")\n",
        "\n",
        "        return iter + 1\n",
        "\n",
        "    @property\n",
        "    def y_pre(self):\n",
        "        return self.Y\n",
        "\n",
        "\n",
        "class CDKM:\n",
        "    def __init__(self, d, k, niter=200, nredo=10, verbose=False, seed=1234, debug=0):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.seed = seed\n",
        "        self.debug = debug\n",
        "        self.centroids = None\n",
        "        self.labels_ = None\n",
        "        self.Y_final_ = None\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "        self.n_iter_ = None\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        np.random.seed(self.seed)\n",
        "        start_time = time.time()\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        if dim != self.d:\n",
        "            raise ValueError(f\"Data dimension {dim} does not match expected {self.d}.\")\n",
        "\n",
        "        if init_centroids is None:\n",
        "            init_Y = initial_Y(x_orig_data, self.k, self.nredo, \"random\")\n",
        "        else:\n",
        "            init_Y = init_centroids\n",
        "\n",
        "        model = CDKM_PurePy(x_orig_data, self.k, debug=self.debug)\n",
        "        model.opt(init_Y, ITER=self.niter)\n",
        "        Y = model.y_pre\n",
        "        self.n_iter_ = model.n_iter_\n",
        "\n",
        "        centroids = compute_cluster_centers_cdkm(x_orig_data, Y)\n",
        "        labels = np.argmax(one_hot(Y[0], self.k), axis=1)\n",
        "\n",
        "        # Compute SSE\n",
        "        sse = np.sum((x_orig_data - centroids[labels])**2)\n",
        "\n",
        "        # Compute balance loss\n",
        "        counts = np.bincount(labels, minlength=self.k)\n",
        "        ideal_size = n / self.k\n",
        "        balance_loss = np.sum((counts - ideal_size)**2)\n",
        "\n",
        "        self.Y_final_ = one_hot(Y[0], self.k)\n",
        "        self.centroids = centroids\n",
        "        self.labels_ = labels\n",
        "        self.sse_ = sse\n",
        "        self.balance_loss_ = balance_loss\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"CDKM finished in {self.runtime_:.4f}s; \"\n",
        "                  f\"SSE = {self.sse_:.4f}; \"\n",
        "                  f\"Balance Loss = {self.balance_loss_:.4f}; \"\n",
        "                  f\"Iterations = {self.n_iter_}\")\n",
        "\n",
        "class BKNC:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=False,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data (n_features)\n",
        "        # k: number of clusters (c in BKNC)\n",
        "        self.d_features = d\n",
        "        self.k = k  # c in BKNC\n",
        "        self.niter = niter # Niter in BKNC\n",
        "        self.lambda_ = lambda_ # lambda in BKNC\n",
        "        self.seed = seed\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Other Faiss Kmeans parameters - not directly used by BKNC logic\n",
        "        self.nredo = nredo\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu # BKNC as implemented here is CPU-only\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        # BKNC specific results\n",
        "        self.F_ = None          # The F matrix from BKNC (n_samples x k)\n",
        "        self.R_ = None          # The R matrix (k x k)\n",
        "        self.Y_ = None          # The Y matrix (one-hot labels, n_samples x k)\n",
        "        self.labels_ = None     # Final cluster assignments (idx, shape: n_samples)\n",
        "        self.obj_history_ = []  # History of the objective function trace(F'X_m'X_mF)\n",
        "        self.final_obj_ = None\n",
        "        self.runtime_ = 0\n",
        "\n",
        "        # For compatibility with original FCFC structure\n",
        "        self.centroids = None # Will be populated with cluster means\n",
        "        self.obj = None # Can store obj_history_ here\n",
        "\n",
        "        # Final metrics as requested\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "\n",
        "\n",
        "    def _initialize_Y_bknc(self, n_samples, c):\n",
        "        \"\"\"\n",
        "        Equivalent to MATLAB's init function for Y.\n",
        "        Creates an n_samples x c one-hot encoded matrix from random labels.\n",
        "        \"\"\"\n",
        "        # labels are 0 to c-1\n",
        "        # This internal seeding should be fine as long as the main train method sets the overall seed.\n",
        "        # If this method were called multiple times independently *within* one train call,\n",
        "        # and expected different Ys, then it would need a different seeding strategy.\n",
        "        # For now, it's called once per train.\n",
        "        labels = np.random.randint(0, c, size=n_samples)\n",
        "        Y = np.zeros((n_samples, c), dtype=int)\n",
        "        Y[np.arange(n_samples), labels] = 1\n",
        "        return Y\n",
        "\n",
        "    def _calculate_cluster_centroids(self, data, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates the mean of points in each cluster.\n",
        "        data: (n_samples, n_features)\n",
        "        labels: (n_samples,)\n",
        "        num_clusters: k\n",
        "        data_dim: d_features\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim))\n",
        "        if labels is None: # Should not happen if called after labels are set\n",
        "             if self.verbose:\n",
        "                print(\"Warning: Labels are None in _calculate_cluster_centroids. Returning zero centroids.\")\n",
        "             return centroids\n",
        "\n",
        "        for i in range(num_clusters):\n",
        "            cluster_points = data[labels == i]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[i] = np.mean(cluster_points, axis=0)\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {i} is empty during centroid calculation. Assigning a random data point as its centroid.\")\n",
        "                if len(data) > 0:\n",
        "                    # Use a random number generator seeded by self.seed for consistent fallback\n",
        "                    rng_fallback = np.random.RandomState(self.seed + i) # Add i for variety if multiple fallbacks\n",
        "                    centroids[i] = data[rng_fallback.choice(len(data))]\n",
        "                else: # No data points at all (edge case)\n",
        "                    centroids[i] = np.zeros(data_dim)\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Implements the BKNC algorithm.\n",
        "        x: data matrix (n_samples, n_features)\n",
        "        weights: Not used by BKNC.\n",
        "        init_centroids: Not used by BKNC.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility for the entire train method\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "\n",
        "        X_m = x.T  # (n_features, n_samples) - X_m is the MATLAB-like X\n",
        "        n_features_internal, n_samples = X_m.shape # n_features_internal is self.d_features\n",
        "        c = self.k # Number of clusters\n",
        "\n",
        "        # Initialize Y (n_samples, c)\n",
        "        # _initialize_Y_bknc uses np.random, which is now seeded by self.seed\n",
        "        Y = self._initialize_Y_bknc(n_samples, c)\n",
        "\n",
        "        # Initialize R (c, c) as a random orthogonal matrix\n",
        "        # np.random.rand is also affected by the global seed set above\n",
        "        R = orth(np.random.rand(c, c))\n",
        "\n",
        "        obj_log = np.zeros(self.niter)\n",
        "\n",
        "        # F_loop initialization is also seeded\n",
        "        for iter_num in range(self.niter):\n",
        "            F_loop = orth(np.random.rand(n_samples, c))\n",
        "            G = Y @ R.T\n",
        "\n",
        "            for _ in range(10):\n",
        "                TempM_F = X_m @ F_loop\n",
        "                M_calc_F = 2 * X_m.T @ TempM_F + self.lambda_ * G\n",
        "                U_f, _, Vh_f = np.linalg.svd(M_calc_F, full_matrices=False)\n",
        "                F_loop = U_f @ Vh_f\n",
        "            F_current = F_loop\n",
        "\n",
        "            N_calc_R = F_current.T @ Y\n",
        "            U_r, _, Vh_r = np.linalg.svd(N_calc_R, full_matrices=False)\n",
        "            R = U_r @ Vh_r\n",
        "\n",
        "            P_calc_Y = R.T @ F_current.T\n",
        "            idx = np.argmax(P_calc_Y, axis=0)\n",
        "            Y = np.zeros((n_samples, c), dtype=int)\n",
        "            Y[np.arange(n_samples), idx] = 1\n",
        "\n",
        "            TempF_obj = X_m @ F_current\n",
        "            obj_log[iter_num] = np.trace(TempF_obj.T @ TempF_obj)\n",
        "\n",
        "            if self.verbose and (iter_num % 5 == 0 or iter_num == self.niter -1):\n",
        "                print(f\"Iter {iter_num+1}/{self.niter}, BKNC Obj: {obj_log[iter_num]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time()\n",
        "\n",
        "        # Store BKNC results\n",
        "        self.F_ = F_current\n",
        "        self.R_ = R\n",
        "        self.Y_ = Y # This is the one-hot version of labels from the last iteration\n",
        "        self.labels_ = idx # finalInd in MATLAB (0-indexed labels)\n",
        "        self.obj_history_ = obj_log\n",
        "        self.final_obj_ = obj_log[-1]\n",
        "        self.obj = self.obj_history_ # Compatibility\n",
        "\n",
        "        # --- Calculate final centroids, SSE, and Balance Loss ---\n",
        "        # self.centroids are calculated based on original data `x` and final `self.labels_`\n",
        "        self.centroids = self._calculate_cluster_centroids(x, self.labels_, self.k, self.d_features)\n",
        "\n",
        "        # Calculate SSE\n",
        "        current_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n_samples):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point = x[i, :]\n",
        "                centroid_val = self.centroids[cluster_idx, :]\n",
        "                current_sse += np.sum((point - centroid_val)**2) # Squared Euclidean distance\n",
        "        self.sse_ = current_sse\n",
        "\n",
        "        # Calculate Balance Loss\n",
        "        current_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            cluster_sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal_size = n_samples / self.k\n",
        "            current_balance_loss = np.sum((cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = current_balance_loss\n",
        "\n",
        "        # Final runtime calculation\n",
        "        self.runtime_ = time.time() - start_time # Corrected runtime calculation\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BKNC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BKNC objective (trace): {self.final_obj_:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of calculated centroids: {self.centroids.shape}\")\n",
        "            print(f\"Final SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class MyKMeans:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=False,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data (n_features)\n",
        "        # k: number of clusters (c in BKNC)\n",
        "        self.d_features = d\n",
        "        self.k = k  # c in BKNC\n",
        "        self.niter = niter # Niter in BKNC\n",
        "        self.lambda_ = lambda_ # lambda in BKNC\n",
        "        self.lambda_reformed = (1-lambda_)/lambda_\n",
        "        self.seed = seed\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Other Faiss Kmeans parameters - not directly used by BKNC logic\n",
        "        self.nredo = nredo\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu # BKNC as implemented here is CPU-only\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        # BKNC specific results\n",
        "        self.F_ = None          # The F matrix from BKNC (n_samples x k)\n",
        "        self.R_ = None          # The R matrix (k x k)\n",
        "        self.Y_ = None          # The Y matrix (one-hot labels, n_samples x k)\n",
        "        self.labels_ = None     # Final cluster assignments (idx, shape: n_samples)\n",
        "        self.obj_history_ = []  # History of the objective function trace(F'X_m'X_mF)\n",
        "        self.final_obj_ = None\n",
        "        self.runtime_ = 0\n",
        "\n",
        "        # For compatibility with original FCFC structure\n",
        "        self.centroids = None # Will be populated with cluster means\n",
        "        self.obj = None # Can store obj_history_ here\n",
        "\n",
        "        # Final metrics as requested\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "\n",
        "\n",
        "    def _initialize_Y_bknc(self, n_samples, c):\n",
        "        \"\"\"\n",
        "        Equivalent to MATLAB's init function for Y.\n",
        "        Creates an n_samples x c one-hot encoded matrix from random labels.\n",
        "        \"\"\"\n",
        "        # labels are 0 to c-1\n",
        "        # This internal seeding should be fine as long as the main train method sets the overall seed.\n",
        "        # If this method were called multiple times independently *within* one train call,\n",
        "        # and expected different Ys, then it would need a different seeding strategy.\n",
        "        # For now, it's called once per train.\n",
        "        labels = np.random.randint(0, c, size=n_samples)\n",
        "        Y = np.zeros((n_samples, c), dtype=int)\n",
        "        Y[np.arange(n_samples), labels] = 1\n",
        "        return Y\n",
        "\n",
        "    def _calculate_cluster_centroids(self, data, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates the mean of points in each cluster.\n",
        "        data: (n_samples, n_features)\n",
        "        labels: (n_samples,)\n",
        "        num_clusters: k\n",
        "        data_dim: d_features\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim))\n",
        "        if labels is None: # Should not happen if called after labels are set\n",
        "             if self.verbose:\n",
        "                print(\"Warning: Labels are None in _calculate_cluster_centroids. Returning zero centroids.\")\n",
        "             return centroids\n",
        "\n",
        "        for i in range(num_clusters):\n",
        "            cluster_points = data[labels == i]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[i] = np.mean(cluster_points, axis=0)\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {i} is empty during centroid calculation. Assigning a random data point as its centroid.\")\n",
        "                if len(data) > 0:\n",
        "                    # Use a random number generator seeded by self.seed for consistent fallback\n",
        "                    rng_fallback = np.random.RandomState(self.seed + i) # Add i for variety if multiple fallbacks\n",
        "                    centroids[i] = data[rng_fallback.choice(len(data))]\n",
        "                else: # No data points at all (edge case)\n",
        "                    centroids[i] = np.zeros(data_dim)\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Implements the BKNC algorithm.\n",
        "        x: data matrix (n_samples, n_features)\n",
        "        weights: Not used by BKNC.\n",
        "        init_centroids: Not used by BKNC.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility for the entire train method\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "\n",
        "        X_m = x.T  # (n_features, n_samples) - X_m is the MATLAB-like X\n",
        "        n_features_internal, n_samples = X_m.shape # n_features_internal is self.d_features\n",
        "        c = self.k # Number of clusters\n",
        "\n",
        "        # Initialize Y (n_samples, c)\n",
        "        # _initialize_Y_bknc uses np.random, which is now seeded by self.seed\n",
        "        Y = self._initialize_Y_bknc(n_samples, c)\n",
        "\n",
        "        # Initialize R (c, c) as a random orthogonal matrix\n",
        "        # np.random.rand is also affected by the global seed set above\n",
        "        R = orth(np.random.rand(c, c))\n",
        "\n",
        "        obj_log = np.zeros(self.niter)\n",
        "\n",
        "        # F_loop initialization is also seeded\n",
        "        for iter_num in range(self.niter):\n",
        "            F_loop = orth(np.random.rand(n_samples, c))\n",
        "            G = Y @ R.T\n",
        "\n",
        "            for _ in range(10):\n",
        "                TempM_F = X_m @ F_loop\n",
        "                M_calc_F = 2 * X_m.T @ TempM_F + self.lambda_reformed * G\n",
        "                U_f, _, Vh_f = np.linalg.svd(M_calc_F, full_matrices=False)\n",
        "                F_loop = U_f @ Vh_f\n",
        "            F_current = F_loop\n",
        "\n",
        "            N_calc_R = F_current.T @ Y\n",
        "            U_r, _, Vh_r = np.linalg.svd(N_calc_R, full_matrices=False)\n",
        "            R = U_r @ Vh_r\n",
        "\n",
        "            P_calc_Y = R.T @ F_current.T\n",
        "            idx = np.argmax(P_calc_Y, axis=0)\n",
        "            Y = np.zeros((n_samples, c), dtype=int)\n",
        "            Y[np.arange(n_samples), idx] = 1\n",
        "\n",
        "            TempF_obj = X_m @ F_current\n",
        "            obj_log[iter_num] = np.trace(TempF_obj.T @ TempF_obj)\n",
        "\n",
        "            if self.verbose and (iter_num % 5 == 0 or iter_num == self.niter -1):\n",
        "                print(f\"Iter {iter_num+1}/{self.niter}, BKNC Obj: {obj_log[iter_num]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time()\n",
        "\n",
        "        # Store BKNC results\n",
        "        self.F_ = F_current\n",
        "        self.R_ = R\n",
        "        self.Y_ = Y # This is the one-hot version of labels from the last iteration\n",
        "        self.labels_ = idx # finalInd in MATLAB (0-indexed labels)\n",
        "        self.obj_history_ = obj_log\n",
        "        self.final_obj_ = obj_log[-1]\n",
        "        self.obj = self.obj_history_ # Compatibility\n",
        "\n",
        "        # --- Calculate final centroids, SSE, and Balance Loss ---\n",
        "        # self.centroids are calculated based on original data `x` and final `self.labels_`\n",
        "        self.centroids = self._calculate_cluster_centroids(x, self.labels_, self.k, self.d_features)\n",
        "\n",
        "        # Calculate SSE\n",
        "        current_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n_samples):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point = x[i, :]\n",
        "                centroid_val = self.centroids[cluster_idx, :]\n",
        "                current_sse += np.sum((point - centroid_val)**2) # Squared Euclidean distance\n",
        "        self.sse_ = current_sse\n",
        "\n",
        "        # Calculate Balance Loss\n",
        "        current_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            cluster_sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal_size = n_samples / self.k\n",
        "            current_balance_loss = np.sum((cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = current_balance_loss\n",
        "\n",
        "        # Final runtime calculation\n",
        "        self.runtime_ = time.time() - start_time # Corrected runtime calculation\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BKNC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BKNC objective (trace): {self.final_obj_:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of calculated centroids: {self.centroids.shape}\")\n",
        "            print(f\"Final SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "# Helper functions (moved outside the class, or could be static methods)\n",
        "def initial_Y(X, c, rep, way=\"random\"):\n",
        "        N = X.shape[0]\n",
        "        Y = np.zeros((rep, N), dtype=np.int32)\n",
        "\n",
        "        if way == \"random\":\n",
        "            for rep_i in range(rep):\n",
        "                Y[rep_i] = np.random.randint(0, c, N)\n",
        "\n",
        "        elif way == \"k-means++\":\n",
        "            for rep_i in range(rep):\n",
        "                Y[rep_i] = KMeans(n_clusters=c, init=\"k-means++\", n_init=1, max_iter=1).fit(X).labels_\n",
        "\n",
        "        else:\n",
        "            assert 2 == 1\n",
        "\n",
        "        return Y\n",
        "def one_hot(y: np.ndarray, k: int):\n",
        "    n = len(y)\n",
        "    Y = np.zeros((n, k), dtype=np.float32)\n",
        "    Y[np.arange(n), y] = 1.0\n",
        "    return Y\n",
        "def compute_cluster_centers_cdkm(X, Y):\n",
        "    \"\"\"\n",
        "    X: (n, d)\n",
        "    Y: list of cluster label arrays, each of shape (n,)\n",
        "    \"\"\"\n",
        "    y = Y[0]  # shape (n,)\n",
        "    n, k = X.shape[0], np.max(y) + 1\n",
        "    Y0 = np.zeros((n, k), dtype=np.float64)\n",
        "    Y0[np.arange(n), y] = 1.0  # one-hot\n",
        "\n",
        "    weights = np.sum(Y0, axis=0)  # (k,)\n",
        "    weights[weights == 0] = 1e-10\n",
        "\n",
        "    centers = (Y0.T @ X) / weights[:, None]\n",
        "    return centers\n",
        "\n",
        "\n",
        "def get_centroid(data, label, K, n, d_features):\n",
        "    \"\"\"\n",
        "    Update centroids after the assignment phase.\n",
        "    data: (n, d_features)\n",
        "    label: (n,)\n",
        "    K: number of clusters\n",
        "    n: number of samples\n",
        "    d_features: number of features\n",
        "    \"\"\"\n",
        "    centroids = np.zeros((K, d_features))\n",
        "    for k_idx in range(K):\n",
        "        members = (label == k_idx)\n",
        "        if np.any(members):\n",
        "            # Np.sum on boolean array members gives count of True values\n",
        "            centroids[k_idx, :] = np.sum(data[members, :], axis=0) / np.sum(members)\n",
        "        else:\n",
        "            # Handle empty cluster: assign a random point from data\n",
        "            # This random choice is now affected by the seed set in train()\n",
        "            if n > 0 : # Ensure data is not empty\n",
        "                 centroids[k_idx, :] = data[np.random.choice(n), :]\n",
        "            # else: centroid remains zeros if data is empty (edge case)\n",
        "    return centroids\n",
        "\n",
        "\n",
        "def get_distance(data, centroids, K, n, d_features, size_cluster, lambda_param):\n",
        "    \"\"\"\n",
        "    Objective function term for assignment:\n",
        "    D(i,j) = distance(i-th data point, j-th centroid)^2 + lambda_param * size_of_jth_cluster\n",
        "    data: (n, d_features)\n",
        "    centroids: (K, d_features)\n",
        "    size_cluster: (K,) - current size of each cluster\n",
        "    lambda_param: balance weight\n",
        "    Returns: D_matrix (n, K)\n",
        "    \"\"\"\n",
        "    D_matrix = np.zeros((n, K))\n",
        "    for k_idx in range(K):\n",
        "        # Squared Euclidean distance\n",
        "        dist_sq = np.sum((data - centroids[k_idx, :])**2, axis=1)\n",
        "        D_matrix[:, k_idx] = dist_sq + lambda_param * size_cluster[k_idx]\n",
        "    return D_matrix\n",
        "\n",
        "\n",
        "def initial_centroid(x_data, K, n_samples):\n",
        "    \"\"\"\n",
        "    Initialize centroids randomly by choosing K unique points from the data.\n",
        "    x_data: (n_samples, d_features)\n",
        "    K: number of clusters\n",
        "    n_samples: number of samples\n",
        "    \"\"\"\n",
        "    if K > n_samples:\n",
        "        raise ValueError(\"K (number of clusters) cannot be greater than n_samples.\")\n",
        "    # This random choice is now affected by the seed set in train()\n",
        "    indices = np.random.choice(n_samples, K, replace=False)\n",
        "    return x_data[indices, :]\n",
        "\n",
        "# 优化后的数据加载函数\n",
        "def load_data_chunked(path, dtype='float32', chunksize=1000):\n",
        "    \"\"\"分块加载大数据集避免内存溢出\"\"\"\n",
        "    chunks = []\n",
        "    for chunk in pd.read_csv(path, header=None, chunksize=chunksize):\n",
        "        chunks.append(chunk.astype(dtype))\n",
        "    return np.concatenate(chunks, axis=0)\n",
        "\n",
        "def run_experiment(model_class, model_name, dataset_path, dimensions, n_clusters, n_runs=10):\n",
        "    \"\"\"运行实验，返回sse和balance_loss列表\"\"\"\n",
        "    try:\n",
        "        X_data = load_data_chunked(dataset_path)\n",
        "        sse_list = []\n",
        "        balance_loss_list = []\n",
        "\n",
        "        for run in range(n_runs):\n",
        "            if model_name == \"FCFC\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, lambda_=0.1, seed=1234+run, verbose=False)\n",
        "            elif model_name == \"BCLS\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, lambda_=0.1, seed=1234+run, verbose=False)\n",
        "            elif model_name == \"Lloyd\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, seed=1234+run, verbose=False)\n",
        "            elif model_name == \"CDKM\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, seed=1234+run, verbose=False)\n",
        "            elif model_name == \"BKNC\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, lambda_=0.1, seed=1234+run, verbose=False)\n",
        "            else: # MyKMeans\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, lambda_=0.1, seed=1234+run, verbose=False)\n",
        "\n",
        "            model.train(X_data) if hasattr(model, 'train') else model.fit(X_data)\n",
        "\n",
        "            sse_list.append(model.sse_)\n",
        "            # 这里假设所有模型都有 balance_loss_ 属性，或你可用 getattr(model, 'balance_loss_', np.nan) 容错\n",
        "            balance_loss_list.append(getattr(model, 'balance_loss_', np.nan))\n",
        "\n",
        "        print(f\"sse_list: {sse_list}\")\n",
        "        print(f\"balance_loss_list: {balance_loss_list}\")\n",
        "        return sse_list, balance_loss_list\n",
        "\n",
        "    finally:\n",
        "        if 'X_data' in locals(): del X_data\n",
        "        if 'model' in locals(): del model\n",
        "        import gc\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 定义要测试的模型和数据集\n",
        "    models = [\n",
        "        (Lloyd, \"Lloyd\"),\n",
        "        (CDKM, \"CDKM\"),\n",
        "        (BCLS, \"BCLS\"),\n",
        "        (FCFC, \"FCFC\"),\n",
        "        (BKNC, \"BKNC\"),\n",
        "        (MyKMeans, \"MyKMeans\")\n",
        "    ]\n",
        "\n",
        "    # 定义数据集路径和对应的维度\n",
        "    datasets = [\n",
        "        (\"/content/sample_data/Huatuo_1024d_10k.csv\", 1024),\n",
        "        (\"/content/sample_data/LiveChat_1024d_10k.csv\", 1024),\n",
        "        (\"/content/sample_data/deep_96d_10k.csv\", 96),\n",
        "        (\"/content/sample_data/glove_300d_10k.csv\", 300),\n",
        "        (\"/content/sample_data/sift_128d_10k.csv\", 128)\n",
        "    ]\n",
        "    k = 5\n",
        "\n",
        "    n_runs = 10  # 运行次数\n",
        "\n",
        "    # 结果DataFrame行数 = datasets数 * models数\n",
        "    result_rows = []\n",
        "    for dataset_path, dim in datasets:\n",
        "        dataset_name = Path(dataset_path).stem\n",
        "        for model_class, model_name in models:\n",
        "            print(f\"Running {model_name} on {dataset_name}...\")\n",
        "            sse_list, balance_loss_list = run_experiment(model_class, model_name, dataset_path, dim, k, n_runs=n_runs)\n",
        "\n",
        "            sse_mean = np.mean(sse_list)\n",
        "            sse_var = np.var(sse_list)\n",
        "            balance_mean = np.mean(balance_loss_list)\n",
        "            balance_var = np.var(balance_loss_list)\n",
        "\n",
        "            result_rows.append({\n",
        "                'Dataset': dataset_name,\n",
        "                'Model': model_name,\n",
        "                'SSE_Mean': sse_mean,\n",
        "                'SSE_Var': sse_var,\n",
        "                'BalanceLoss_Mean': balance_mean,\n",
        "                'BalanceLoss_Var': balance_var,\n",
        "            })\n",
        "\n",
        "            print(f\"SSE_Mean: {sse_mean}\")\n",
        "            print(f\"SSE_Var: {sse_var}\")\n",
        "            print(f\"BalanceLoss_Mean: {balance_mean}\")\n",
        "            print(f\"BalanceLoss_Var: {balance_var}\")\n",
        "\n",
        "    results_df = pd.DataFrame(result_rows)\n",
        "\n",
        "    results_df.to_csv('/content/sample_data/metrics_summary.csv', index=False)\n",
        "    print(\"\\nExperiment results saved to 'metrics_summary.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1️⃣ 读 CSV\n",
        "df = pd.read_csv('/content/sample_data/metrics_summary.csv')\n",
        "\n",
        "# 2️⃣ 提取原始 Dataset 和 Model 的顺序\n",
        "df['Dataset_raw'] = df['Dataset'].str.split('-').str[0].str.split('_').str[0]\n",
        "df['Model_raw'] = df['Model']\n",
        "\n",
        "# 3️⃣ 替换 Dataset 和 Model 名\n",
        "df['Dataset'] = df['Dataset_raw'].replace({\n",
        "    'deep': 'Deep',\n",
        "    'sift': 'SIFT',\n",
        "    'glove': 'GloVe'\n",
        "})\n",
        "df['Model'] = df['Model_raw'].replace({'MyKMeans': 'Tub-means'})\n",
        "\n",
        "# 4️⃣ 用 CSV 中 Dataset 出现顺序定义 Categorical\n",
        "dataset_order = df.drop_duplicates('Dataset_raw')['Dataset'].tolist()\n",
        "df['Dataset'] = pd.Categorical(df['Dataset'], categories=dataset_order, ordered=True)\n",
        "\n",
        "# 5️⃣ 保留原行顺序标记（保证 Model 顺序）\n",
        "df['row_order'] = np.arange(len(df))\n",
        "\n",
        "# 6️⃣ 按 Dataset、原行号 排序\n",
        "df = df.sort_values(['Dataset', 'row_order']).drop(columns=['row_order', 'Dataset_raw', 'Model_raw'])\n",
        "\n",
        "# ✅ 从这里开始是你原来的生成表格逻辑\n",
        "def format_row(dataset, model, sse_mean, sse_std, balance_mean, balance_std, last=False, multirow=False, multirow_count=None):\n",
        "    sse_mean_str = \"{:.2E}\".format(sse_mean)\n",
        "    sse_std_str = \"{:.2E}\".format(sse_std)\n",
        "    balance_mean_str = \"{:.2E}\".format(balance_mean)\n",
        "    balance_std_str = \"{:.2E}\".format(balance_std)\n",
        "\n",
        "    if multirow:\n",
        "        row = f\"\\\\multirow{{{multirow_count}}}{{*}}{{\\\\ {dataset}}}\\n& \\\\ {model} & {sse_mean_str} & {sse_std_str} && {balance_mean_str} & {balance_std_str} \\\\\\\\\"\n",
        "    elif last:\n",
        "        row = f\"& \\\\ {model} & {sse_mean_str} & {sse_std_str} && {balance_mean_str} & {balance_std_str} \\\\\\\\ \\\\midrule \\n\"\n",
        "    else:\n",
        "        row = f\"& \\\\ {model} & {sse_mean_str} & {sse_std_str} && {balance_mean_str} & {balance_std_str} \\\\\\\\\"\n",
        "    return row\n",
        "\n",
        "# 7️⃣ 分块生成\n",
        "lines = []\n",
        "for dataset, group in df.groupby('Dataset'):\n",
        "    models = group['Model'].tolist()\n",
        "    sse_mean = group['SSE_Mean'].tolist()\n",
        "    sse_std = group['SSE_Var'].tolist()\n",
        "    balance_mean = group['BalanceLoss_Mean'].tolist()\n",
        "    balance_std = group['BalanceLoss_Var'].tolist()\n",
        "\n",
        "    for i in range(len(models)):\n",
        "        is_first = (i == 0)\n",
        "        is_last = (i == len(models) - 1)  # 注意这里用实际长度\n",
        "        line = format_row(dataset, models[i], sse_mean[i], sse_std[i], balance_mean[i], balance_std[i],\n",
        "                          last=is_last, multirow=is_first, multirow_count=len(models) if is_first else None)\n",
        "        lines.append(line)\n",
        "\n",
        "# 8️⃣ 输出\n",
        "latex_table_body = \"\\n\".join(lines)\n",
        "print(latex_table_body)\n",
        "\n",
        "with open(\"latex_table_rows.tex\", \"w\") as f:\n",
        "    f.write(latex_table_body)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zALISSCwuPk",
        "outputId": "af68e6b8-aa65-4008-b810-65ec21697689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\multirow{6}{*}{\\ Huatuo}\n",
            "& \\ Lloyd & 2.66E+03 & 8.99E+01 && 3.50E+06 & 7.32E+12 \\\\\n",
            "& \\ CDKM & 2.66E+03 & 2.67E+01 && 2.47E+06 & 1.92E+12 \\\\\n",
            "& \\ BCLS & 3.13E+03 & 2.28E+00 && 3.86E+05 & 6.39E+10 \\\\\n",
            "& \\ FCFC & 3.14E+03 & 0.00E+00 && 8.00E+07 & 6.22E-16 \\\\\n",
            "& \\ BKNC & 2.67E+03 & 5.19E+01 && 5.37E+05 & 1.34E+10 \\\\\n",
            "& \\ Tub-means & 3.11E+03 & 2.56E+02 && 4.62E+05 & 3.70E+10 \\\\ \\midrule \n",
            "\n",
            "\\multirow{6}{*}{\\ LiveChat}\n",
            "& \\ Lloyd & 1.93E+03 & 2.37E+01 && 4.48E+06 & 5.04E+12 \\\\\n",
            "& \\ CDKM & 1.93E+03 & 4.99E-01 && 3.73E+06 & 7.43E+11 \\\\\n",
            "& \\ BCLS & 2.03E+03 & 1.58E-01 && 4.72E+05 & 5.40E+10 \\\\\n",
            "& \\ FCFC & 2.04E+03 & 2.07E-25 && 8.00E+07 & 3.33E-16 \\\\\n",
            "& \\ BKNC & 1.94E+03 & 2.14E+01 && 1.13E+06 & 7.25E+10 \\\\\n",
            "& \\ Tub-means & 2.04E+03 & 4.27E+00 && 1.72E+04 & 7.41E+08 \\\\ \\midrule \n",
            "\n",
            "\\multirow{6}{*}{\\ Deep}\n",
            "& \\ Lloyd & 8.12E+03 & 2.51E+02 && 1.29E+06 & 2.76E+11 \\\\\n",
            "& \\ CDKM & 8.11E+03 & 5.09E+02 && 1.45E+06 & 1.88E+12 \\\\\n",
            "& \\ BCLS & 8.90E+03 & 9.15E+03 && 1.62E+07 & 4.63E+13 \\\\\n",
            "& \\ FCFC & 9.31E+03 & 3.31E-24 && 8.00E+07 & 2.66E-16 \\\\\n",
            "& \\ BKNC & 8.25E+03 & 1.94E+03 && 3.52E+05 & 3.34E+10 \\\\\n",
            "& \\ Tub-means & 8.52E+03 & 2.90E+03 && 1.50E+05 & 8.29E+09 \\\\ \\midrule \n",
            "\n",
            "\\multirow{6}{*}{\\ GloVe}\n",
            "& \\ Lloyd & 3.64E+05 & 1.82E+05 && 1.11E+06 & 2.56E+11 \\\\\n",
            "& \\ CDKM & 3.64E+05 & 3.89E+04 && 1.09E+06 & 2.29E+11 \\\\\n",
            "& \\ BCLS & 3.85E+05 & 6.86E+05 && 1.28E+07 & 5.47E+13 \\\\\n",
            "& \\ FCFC & 3.84E+05 & 7.52E+06 && 5.06E+07 & 4.78E+14 \\\\\n",
            "& \\ BKNC & 3.66E+05 & 5.65E+05 && 1.99E+05 & 1.04E+10 \\\\\n",
            "& \\ Tub-means & 3.66E+05 & 6.90E+05 && 1.89E+05 & 8.90E+09 \\\\ \\midrule \n",
            "\n",
            "\\multirow{6}{*}{\\ SIFT}\n",
            "& \\ Lloyd & 6.55E+08 & 1.02E+13 && 4.60E+05 & 2.59E+11 \\\\\n",
            "& \\ CDKM & 6.51E+08 & 1.33E+12 && 7.34E+05 & 1.80E+11 \\\\\n",
            "& \\ BCLS & 8.46E+08 & 7.07E+13 && 2.26E+07 & 4.96E+13 \\\\\n",
            "& \\ FCFC & 6.55E+08 & 7.61E+12 && 4.63E+05 & 1.84E+11 \\\\\n",
            "& \\ BKNC & 6.59E+08 & 3.68E+13 && 2.07E+05 & 9.23E+09 \\\\\n",
            "& \\ Tub-means & 6.59E+08 & 3.67E+13 && 2.07E+05 & 9.30E+09 \\\\ \\midrule \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-2134466040.py:46: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  for dataset, group in df.groupby('Dataset'):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from scipy.linalg import orth # For creating orthogonal matrices\n",
        "\n",
        "class FCFC:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data\n",
        "        # k: number of clusters\n",
        "        self.d_features = d # Renamed for clarity, consistent with other classes\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.max_iter = niter # Keep for consistency with existing loop\n",
        "\n",
        "        # Other parameters (some might not be used by this specific FCFC logic but kept for interface)\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu # This FCFC implementation is CPU-based\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "        self.lambda_ = lambda_  # Balance parameter for the objective function in get_distance\n",
        "\n",
        "        # Results storage\n",
        "        self.centroids = None               # Final cluster centroids (k, d_features)\n",
        "        self.labels_ = None                 # Final cluster assignments for each point (n_samples,)\n",
        "        self.runtime_ = None                # Total training time\n",
        "\n",
        "        self.objective_history_ = None      # History of sum_dis (sum of D(i,j) values)\n",
        "        self.sse_history_ = None            # History of Sum of Squared Errors per iteration\n",
        "        self.balance_loss_history_ = None   # History of Balance Loss per iteration\n",
        "\n",
        "        self.final_objective_ = None        # Final value from objective_history_\n",
        "        self.final_sse_ = None              # Final Sum of Squared Errors\n",
        "        self.final_balance_loss_ = None     # Final Balance Loss\n",
        "        self.final_cluster_sizes_ = None    # Final size of each cluster (k,)\n",
        "\n",
        "        self.sse_ = 0\n",
        "        self.balance_loss_ = 0\n",
        "        self.normalized_entropy_ = None  # Normalized entropy\n",
        "        self.cluster_variance_ = None    # Cluster size variance\n",
        "\n",
        "\n",
        "        # For compatibility, self.obj can point to the primary objective history\n",
        "        self.obj = None\n",
        "\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids_arg=None): # Renamed init_centroids to avoid conflict\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility\n",
        "        start_time = time.time()\n",
        "\n",
        "        K = self.k\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "        n_samples, d_features_data = x.shape # n_samples, d in original code\n",
        "\n",
        "        # Initialize arrays for storing per-iteration metrics\n",
        "        # sse_history stores the traditional SSE\n",
        "        # balance_loss_history stores the balance penalty\n",
        "        # objective_value_history stores the sum of D(i,j) which is being minimized directly\n",
        "        sse_history = np.zeros(self.max_iter)\n",
        "        balance_loss_history = np.zeros(self.max_iter)\n",
        "        objective_value_history = np.zeros(self.max_iter) # Corresponds to pre_dis\n",
        "\n",
        "        # Initialize centroids\n",
        "        # If init_centroids_arg is provided, use it, otherwise use random initialization\n",
        "        if init_centroids_arg is not None:\n",
        "            if init_centroids_arg.shape != (K, d_features_data):\n",
        "                raise ValueError(f\"Provided init_centroids shape {init_centroids_arg.shape} \"\n",
        "                                 f\"is not ({K}, {d_features_data})\")\n",
        "            current_centroids = np.copy(init_centroids_arg)\n",
        "        else:\n",
        "            current_centroids = initial_centroid(x, K, n_samples) # Uses np.random internally\n",
        "\n",
        "        # size_cluster is 1*K vector, stores size of each cluster for the get_distance objective\n",
        "        # Initialized to ones to avoid issues if lambda_ > 0 and a cluster is initially empty,\n",
        "        # though it gets updated immediately in the first iteration.\n",
        "        # A more common initialization might be n_samples/K or based on initial assignment.\n",
        "        # Let's base it on an initial quick assignment or n_samples/K to be more robust.\n",
        "        # For simplicity of matching the provided code, it starts with ones and is quickly updated.\n",
        "        current_size_cluster = np.ones(K) # Will be updated after first assignment\n",
        "\n",
        "        current_labels = np.zeros(n_samples, dtype=int) # To store labels for each point\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Assignment step: Calculate D matrix and assign points to clusters\n",
        "            # D(point, cluster_j) = distance(point, centroid_j)^2 + lambda * size_cluster_j\n",
        "            D_matrix = get_distance(x, current_centroids, K, n_samples, d_features_data,\n",
        "                                    current_size_cluster, self.lambda_)\n",
        "\n",
        "            min_dist_to_centroid_plus_balance = np.min(D_matrix, axis=1) # (n_samples,)\n",
        "            assigned_labels = np.argmin(D_matrix, axis=1)           # (n_samples,)\n",
        "            sum_objective_values = np.sum(min_dist_to_centroid_plus_balance)\n",
        "\n",
        "            current_labels = assigned_labels\n",
        "            objective_value_history[i] = sum_objective_values\n",
        "\n",
        "            # Update step: Recalculate centroids and cluster sizes\n",
        "            # current_size_cluster is based on the new assignments\n",
        "            current_size_cluster = np.bincount(current_labels, minlength=K)\n",
        "            current_centroids = get_centroid(x, current_labels, K, n_samples, d_features_data)\n",
        "\n",
        "            # Calculate SSE and Balance Loss for this iteration (for monitoring)\n",
        "            iter_sse = 0\n",
        "            iter_balance_penalty_terms = np.zeros(K)\n",
        "\n",
        "            for j in range(K):\n",
        "                cluster_points = x[current_labels == j, :]\n",
        "                if cluster_points.shape[0] > 0: # If cluster is not empty\n",
        "                    # SSE part: sum of squared distances to its actual centroid\n",
        "                    iter_sse += np.sum(np.sum((cluster_points - current_centroids[j, :])**2, axis=1))\n",
        "                # Balance loss part (using current_size_cluster which is already updated)\n",
        "                iter_balance_penalty_terms[j] = (current_size_cluster[j] - n_samples / K)**2\n",
        "\n",
        "            sse_history[i] = iter_sse\n",
        "            balance_loss_history[i] = np.sum(iter_balance_penalty_terms)\n",
        "\n",
        "            if self.verbose and (i % 5 == 0 or i == self.max_iter -1) :\n",
        "                print(f\"Iter {i+1}/{self.max_iter}: Objective={objective_value_history[i]:.4f}, \"\n",
        "                      f\"SSE={sse_history[i]:.4f}, BalanceLoss={balance_loss_history[i]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        # Store results\n",
        "        self.centroids = current_centroids\n",
        "        self.labels_ = current_labels\n",
        "        self.final_cluster_sizes_ = current_size_cluster\n",
        "\n",
        "        self.objective_history_ = objective_value_history\n",
        "        self.sse_history_ = sse_history\n",
        "        self.balance_loss_history_ = balance_loss_history\n",
        "\n",
        "        self.final_objective_ = objective_value_history[-1]\n",
        "        self.final_sse_ = sse_history[-1]\n",
        "        self.final_balance_loss_ = balance_loss_history[-1]\n",
        "\n",
        "        self.obj = self.objective_history_ # Storing the history of the optimized objective\n",
        "\n",
        "        self.sse_ = self.final_sse_\n",
        "        self.balance_loss_ = self.final_balance_loss_\n",
        "\n",
        "        # Entropy & CV calculation\n",
        "        eps = 1e-10\n",
        "        l = n_samples\n",
        "        c = self.k\n",
        "        size0 = current_size_cluster\n",
        "\n",
        "        entro = 0.0\n",
        "        cv = 0.0\n",
        "        for jj in range(c):\n",
        "            frac = size0[jj] / l\n",
        "            entro += frac * np.log((size0[jj]+eps)/l)\n",
        "            cv += np.sqrt((size0[jj] - l / c) ** 2)\n",
        "\n",
        "        self.normalized_entropy_ = -entro / np.log(c)\n",
        "        self.cluster_variance_ = (c / l) * cv\n",
        "\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"FCFC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final Objective (sum of D(i,j)): {self.final_objective_:.4f}\")\n",
        "            print(f\"Final SSE: {self.final_sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.final_balance_loss_:.4f}\")\n",
        "            print(f\"Final cluster sizes: {self.final_cluster_sizes_}\")\n",
        "\n",
        "class Lloyd:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1000000000,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.seed = seed\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.centroids = None\n",
        "        self.obj_history_ = None\n",
        "        self.labels_ = None\n",
        "\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.normalized_entropy_ = None  # Normalized entropy\n",
        "        self.cluster_variance_ = None    # Cluster size variance\n",
        "        self.runtime_ = None\n",
        "        self.obj = None\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning random point.\")\n",
        "                centroids[j] = data_points[np.random.randint(len(data_points))]\n",
        "\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        start_time = time.time()\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input dimension {x_orig_data.shape[1]} != {self.d}\")\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        x = np.ascontiguousarray(x_orig_data, dtype='float32')\n",
        "\n",
        "        kmeans = faiss.Kmeans(\n",
        "            d=self.d,\n",
        "            k=self.k,\n",
        "            niter=self.niter,\n",
        "            nredo=self.nredo,\n",
        "            verbose=self.verbose,\n",
        "            min_points_per_centroid=self.min_points_per_centroid,\n",
        "            max_points_per_centroid=self.max_points_per_centroid,\n",
        "            seed=self.seed,\n",
        "            gpu=self.gpu,\n",
        "            spherical=self.spherical,\n",
        "            update_index=self.update_index,\n",
        "            frozen_centroids=self.frozen_centroids\n",
        "        )\n",
        "\n",
        "        kmeans.train(x, init_centroids=init_centroids)\n",
        "\n",
        "        _, self.labels_ = kmeans.index.search(x, 1)\n",
        "        self.labels_ = self.labels_.flatten()\n",
        "\n",
        "        self.centroids = kmeans.centroids\n",
        "        self.obj_history_ = kmeans.obj if kmeans.obj is not None and len(kmeans.obj) > 0 else np.zeros(self.niter)\n",
        "        self.obj = kmeans.obj[-1] if kmeans.obj is not None and len(kmeans.obj) > 0 else None\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "\n",
        "        # Print every 5th iteration's objective value\n",
        "        if self.verbose and self.obj_history_ is not None and len(self.obj_history_) > 0:\n",
        "            print(\"\\n--- Objective Value (every 5 iterations) ---\")\n",
        "            for i, val in enumerate(self.obj_history_):\n",
        "                if (i + 1) % 5 == 0 or i == len(self.obj_history_) - 1:\n",
        "                    print(f\"  Iter {i+1:2d}: {val:.6f}\")\n",
        "\n",
        "        final_sse = 0\n",
        "        for i in range(n):\n",
        "            cluster_idx = self.labels_[i]\n",
        "            final_sse += np.sum((x_orig_data[i] - self.centroids[cluster_idx]) ** 2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal = n / self.k\n",
        "            final_balance_loss = np.sum((sizes - ideal) ** 2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        # Entropy & CV calculation\n",
        "        eps = 1e-10\n",
        "        l = n\n",
        "        c = self.k\n",
        "        size0 = np.bincount(self.labels_, minlength=c)\n",
        "\n",
        "        entro = 0.0\n",
        "        cv = 0.0\n",
        "        for jj in range(c):\n",
        "            frac = size0[jj] / l\n",
        "            entro += frac * np.log((size0[jj]+eps)/l)\n",
        "            cv += np.sqrt((size0[jj] - l / c) ** 2)\n",
        "\n",
        "        self.normalized_entropy_ = -entro / np.log(c)\n",
        "        self.cluster_variance_ = (c / l) * cv\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Lloyd training finished in {self.runtime_:.4f}s\")\n",
        "            print(f\"Final obj: {self.obj}\")\n",
        "            print(f\"Cluster sizes: {dict(zip(*np.unique(self.labels_, return_counts=True)))}\")\n",
        "            print(f\"SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class BCLS:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=True,\n",
        "                 min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                 seed=1234, gpu=False, spherical=False,\n",
        "                 update_index=True, frozen_centroids=False,\n",
        "                 lambda_=1.0): # lambda_ from FCFC, but BCLS uses 'lam' internally\n",
        "        # d: dimensionality of data\n",
        "        # k: number of clusters\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo # Not used by BCLS algorithm itself\n",
        "        self.verbose = verbose\n",
        "        # The following Faiss-like parameters are not directly used by BCLS's core logic:\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        self.seed = seed\n",
        "        self.lambda_bcls = lambda_ # BCLS specific lambda for sum_Y term in objective\n",
        "                                  # If the lambda_ parameter was meant for this, it's used as 'lam' below.\n",
        "                                  # If it was for something else, then 'lam' needs its own source.\n",
        "                                  # Assuming lambda_ is the 'lam' for BCLS objective.\n",
        "\n",
        "        # Results storage\n",
        "        self.centroids = None       # Will store centroids in original data space\n",
        "        self.obj_history_ = None    # Stores Obj2 from the loop\n",
        "        self.labels_ = None         # Final cluster assignments (0-indexed)\n",
        "        self.Y_final_ = None        # Final Y matrix (one-hot indicators)\n",
        "\n",
        "        # Final metrics\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.normalized_entropy_ = None  # Normalized entropy\n",
        "        self.cluster_variance_ = None    # Cluster size variance\n",
        "        self.runtime_ = None\n",
        "\n",
        "        # For compatibility with previous structure if any part expects 'obj'\n",
        "        self.obj = None\n",
        "\n",
        "\n",
        "    def init1(self, n_samples, num_clusters):\n",
        "        \"\"\"\n",
        "        Initializes the Y matrix (n_samples x num_clusters) with one-hot encoding.\n",
        "        Labels are 1 to num_clusters, then converted to 0-indexed for Python.\n",
        "        \"\"\"\n",
        "        # np.random is affected by self.seed if set before calling train\n",
        "        labels_1_indexed = np.random.randint(1, num_clusters + 1, size=n_samples)\n",
        "        F = np.zeros((n_samples, num_clusters))\n",
        "        F[np.arange(n_samples), labels_1_indexed - 1] = 1\n",
        "        # F = csr_matrix(F) # Can be sparse if n and k are very large\n",
        "        return F\n",
        "\n",
        "    def compute_centroids_from_data(self, data_points, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates centroids from data points and their labels.\n",
        "        data_points: (n_samples, n_features) - original or centered\n",
        "        labels: (n_samples,) - 0-indexed\n",
        "        num_clusters: k\n",
        "        data_dim: d\n",
        "        Returns: (num_clusters, data_dim) centroids\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        if labels is None:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Labels are None in compute_centroids_from_data. Returning zero centroids.\")\n",
        "            return centroids\n",
        "\n",
        "        for i in range(len(data_points)):\n",
        "            label = labels[i]\n",
        "            centroids[label] += data_points[i, :]\n",
        "            counts[label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {j} is empty. Assigning a random data point as its centroid.\")\n",
        "                if len(data_points) > 0:\n",
        "                    # Seed this random choice for consistency if multiple empty clusters\n",
        "                    rng_empty_fallback = np.random.RandomState(self.seed + j + 1000) # Offset seed\n",
        "                    centroids[j] = data_points[rng_empty_fallback.choice(len(data_points)), :]\n",
        "                # else: centroids[j] remains zeros\n",
        "        return centroids\n",
        "\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None): # x_orig_data is n x dim\n",
        "        np.random.seed(self.seed) # Ensure reproducibility for operations within train\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x_orig_data.shape[1] != self.d:\n",
        "            raise ValueError(f\"Input data feature dimension {x_orig_data.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d}\")\n",
        "\n",
        "        ITER = self.niter\n",
        "        # BCLS Algorithm Hyperparameters (taken from the provided snippet)\n",
        "        gamma = 0.00001  # Regularization for W\n",
        "        lam = self.lambda_bcls # Controls balance term in objective (sum_Y**2)\n",
        "        mu = 0.01        # ALM parameter\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        c = self.k  # number of clusters\n",
        "\n",
        "        # Initialize Y\n",
        "        Y = self.init1(n, c) # Y is n x c\n",
        "\n",
        "        # Center the data (BCLS works with centered data)\n",
        "        meanX = np.mean(x_orig_data, axis=0, keepdims=True) # 1 x dim\n",
        "        x_centered = x_orig_data - meanX # n x dim\n",
        "\n",
        "        # ALM variables\n",
        "        Lambda_alm = np.zeros((n, c)) # Lagrange multipliers for Y - Z = 0\n",
        "        rho = 1.005                # Update factor for mu\n",
        "\n",
        "        # Precompute part of W update\n",
        "        # P_inv = x_centered.T @ x_centered + gamma * np.eye(dim)\n",
        "        # P = np.linalg.inv(P_inv)\n",
        "        # Using pseudo-inverse for potentially better stability if P_inv is singular/ill-conditioned\n",
        "        try:\n",
        "            P = np.linalg.inv(x_centered.T @ x_centered + gamma * np.eye(dim))\n",
        "        except np.linalg.LinAlgError:\n",
        "            if self.verbose:\n",
        "                print(\"Warning: Standard inverse failed for P. Using pseudo-inverse.\")\n",
        "            P = np.linalg.pinv(x_centered.T @ x_centered + gamma * np.eye(dim))\n",
        "\n",
        "\n",
        "        obj_history = np.zeros(ITER)\n",
        "        # Optional: if you want to track SSE/BalanceLoss per iteration (on centered data)\n",
        "        # sse_iter_history = np.zeros(ITER)\n",
        "        # balance_loss_iter_history = np.zeros(ITER)\n",
        "\n",
        "\n",
        "        for iter_idx in range(ITER):\n",
        "            # --- Solve W and b ---\n",
        "            # W: dim x c, b: 1 x c\n",
        "            W = P @ (x_centered.T @ Y)\n",
        "            b = np.mean(Y, axis=0, keepdims=True) # Or (1/n) * (np.ones((1,n)) @ Y)\n",
        "\n",
        "            # E = XW + 1b' - Y (Error term for reconstruction using centered X)\n",
        "            # E_recon: n x c\n",
        "            E_recon = x_centered @ W + np.ones((n, 1)) @ b - Y\n",
        "\n",
        "            # --- Solve Z (auxiliary variable for Y) ---\n",
        "            # Z: n x c\n",
        "            # Denominator matrix for Z update:\n",
        "            # Factor = mu**2 + 2 * n * lam * mu  (scalar)\n",
        "            # Coeff_matrix_inv = (-2 * lam * np.ones((n,n)) + (mu + 2 * n * lam) * np.eye(n)) / Factor\n",
        "            # Z = Coeff_matrix_inv @ (mu * Y + Lambda_alm)\n",
        "            # Simpler if Z is updated element-wise or if structure allows.\n",
        "            # The provided formula for Z seems like a direct solution from a specific formulation.\n",
        "            # Let's assume the formula is correct as given:\n",
        "            # Note: (mu**2 + 2 * n * lam * mu) is a scalar.\n",
        "            # The matrix to invert for Z is effectively ( (mu + 2*n*lam)*I - 2*lam*J ), where J is all-ones matrix.\n",
        "            # This matrix has a specific inverse (Sherman-Woodbury).\n",
        "            # For now, using the provided direct calculation:\n",
        "            mat_for_Z_inv_num = -2 * lam * np.ones((n, n)) + (mu + 2 * n * lam) * np.eye(n)\n",
        "            mat_for_Z_inv_den = (mu**2 + 2 * n * lam * mu)\n",
        "            if np.abs(mat_for_Z_inv_den) < 1e-9: # Avoid division by zero\n",
        "                 if self.verbose: print(f\"Warning: Denominator for Z is near zero at iter {iter_idx}\")\n",
        "                 Z = Y # Fallback or handle error\n",
        "            else:\n",
        "                 Z = (mat_for_Z_inv_num / mat_for_Z_inv_den) @ (mu * Y + Lambda_alm)\n",
        "\n",
        "\n",
        "            # --- Solve Y (indicator matrix) ---\n",
        "            # V: n x c\n",
        "            V_update = (1 / (2 + mu)) * (2 * x_centered @ W + 2 * np.ones((n, 1)) @ b + mu * Z - Lambda_alm)\n",
        "\n",
        "            # Update Y by selecting the max element in each row of V_update\n",
        "            current_labels = np.argmax(V_update, axis=1) # n-element array of 0-indexed labels\n",
        "            Y = np.zeros((n, c))\n",
        "            Y[np.arange(n), current_labels] = 1\n",
        "\n",
        "            # --- Update Lambda (Lagrange multipliers) and mu (penalty parameter) for ALM ---\n",
        "            Lambda_alm = Lambda_alm + mu * (Y - Z)\n",
        "            mu = min(mu * rho, 1e5) # Cap mu to avoid very large values\n",
        "\n",
        "            # --- Calculate Objective Value (for centered data) ---\n",
        "            sum_Y_elements = np.sum(Y) # Sum of all elements in Y (should be n if Y is strictly one-hot)\n",
        "            obj_history[iter_idx] = np.trace(E_recon.T @ E_recon) + \\\n",
        "                                    gamma * np.trace(W.T @ W) + \\\n",
        "                                    lam * (sum_Y_elements**2) # Or lam * np.sum( (np.sum(Y, axis=0) - n/c)**2 ) if balance is per cluster size\n",
        "\n",
        "\n",
        "            # --- In-loop SSE and Balance Loss (on centered data, for monitoring if needed) ---\n",
        "            # These are calculated based on current Y and centered data.\n",
        "            # Centroids for centered data: c x dim\n",
        "            temp_centroids_centered = self.compute_centroids_from_data(x_centered, current_labels, c, dim)\n",
        "\n",
        "            sse_iter = 0\n",
        "            for i in range(n):\n",
        "                cluster_idx = current_labels[i]\n",
        "                # Using np.sum for squared norm for clarity with dimensions\n",
        "                sse_iter += np.sum((x_centered[i, :] - temp_centroids_centered[cluster_idx, :])**2)\n",
        "            # sse_iter_history[iter_idx] = sse_iter\n",
        "\n",
        "            cluster_sizes_iter = np.sum(Y, axis=0) # n_elements per cluster (1 x c)\n",
        "            ideal_size_iter = n / c\n",
        "            balance_loss_iter = np.sum((cluster_sizes_iter - ideal_size_iter)**2)\n",
        "            # balance_loss_iter_history[iter_idx] = balance_loss_iter\n",
        "\n",
        "            if self.verbose and (iter_idx % 10 == 0 or iter_idx == ITER -1):\n",
        "                print(f\"Iter {iter_idx+1}/{ITER}, BCLS Obj: {obj_history[iter_idx]:.4f}, \"\n",
        "                      f\"Iter SSE (centered): {sse_iter:.2f}, Iter Bal (centered): {balance_loss_iter:.2f}\")\n",
        "\n",
        "\n",
        "        # --- End of iterations ---\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        # Store final results\n",
        "        self.labels_ = np.argmax(Y, axis=1) # Final 0-indexed labels\n",
        "        self.Y_final_ = Y                   # Final one-hot indicator matrix\n",
        "        self.obj_history_ = obj_history\n",
        "        self.obj = obj_history # Compatibility\n",
        "\n",
        "        # Calculate final centroids in ORIGINAL data space\n",
        "        # Use x_orig_data and self.labels_\n",
        "        final_centroids_orig_space = self.compute_centroids_from_data(x_orig_data, self.labels_, c, dim)\n",
        "        self.centroids = final_centroids_orig_space # Store k x dim centroids\n",
        "\n",
        "        # Calculate final SSE using ORIGINAL data and ORIGINAL space centroids\n",
        "        final_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point_orig = x_orig_data[i, :]\n",
        "                centroid_orig = self.centroids[cluster_idx, :]\n",
        "                final_sse += np.sum((point_orig - centroid_orig)**2)\n",
        "        self.sse_ = final_sse\n",
        "\n",
        "        # Calculate final Balance Loss\n",
        "        final_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            final_cluster_sizes = np.bincount(self.labels_, minlength=c)\n",
        "            ideal_size = n / c\n",
        "            final_balance_loss = np.sum((final_cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = final_balance_loss\n",
        "\n",
        "        # Entropy & CV calculation\n",
        "        eps = 1e-10\n",
        "        l = n\n",
        "        c = self.k\n",
        "        size0 = np.bincount(self.labels_, minlength=c)\n",
        "\n",
        "        entro = 0.0\n",
        "        cv = 0.0\n",
        "        for jj in range(c):\n",
        "            frac = size0[jj] / l\n",
        "            entro += frac * np.log((size0[jj]+eps)/l)\n",
        "            cv += np.sqrt((size0[jj] - l / c) ** 2)\n",
        "\n",
        "        self.normalized_entropy_ = -entro / np.log(c)\n",
        "        self.cluster_variance_ = (c / l) * cv\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BCLS training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BCLS objective value: {self.obj_history_[-1]:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of final centroids (original space): {self.centroids.shape}\")\n",
        "            print(f\"Final SSE (original space): {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "\n",
        "    def compute_centroids(self, x_transposed, F_indicator):\n",
        "        \"\"\"\n",
        "        Computes centroids.\n",
        "        x_transposed: (dim, n_samples) data matrix (e.g., centered data transposed)\n",
        "        F_indicator: (n_samples, k) one-hot cluster indicator matrix\n",
        "        Returns: (k, dim) centroids\n",
        "        DEPRECATED in favor of compute_centroids_from_data for clarity, but kept if used elsewhere.\n",
        "        This version is slightly different from compute_centroids_from_data input format.\n",
        "        \"\"\"\n",
        "        num_clusters = F_indicator.shape[1]\n",
        "        data_dim = x_transposed.shape[0]\n",
        "        n_samples_check = x_transposed.shape[1]\n",
        "\n",
        "        if F_indicator.shape[0] != n_samples_check:\n",
        "            raise ValueError(\"Mismatch in number of samples between x_transposed and F_indicator.\")\n",
        "\n",
        "        centroids = np.zeros((num_clusters, data_dim), dtype=np.float32)\n",
        "        counts = np.zeros(num_clusters, dtype=int)\n",
        "\n",
        "        # Determine labels from F_indicator\n",
        "        labels = np.argmax(F_indicator, axis=1) # (n_samples,)\n",
        "\n",
        "        for i in range(n_samples_check):\n",
        "            cluster_label = labels[i]\n",
        "            centroids[cluster_label] += x_transposed[:, i] # x_transposed[:, i] is a data point (dim,)\n",
        "            counts[cluster_label] += 1\n",
        "\n",
        "        for j in range(num_clusters):\n",
        "            if counts[j] > 0:\n",
        "                centroids[j] /= counts[j]\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning (compute_centroids): Cluster {j} is empty. Assigning random point.\")\n",
        "                if n_samples_check > 0:\n",
        "                    rng_empty_fallback = np.random.RandomState(self.seed + j + 2000)\n",
        "                    centroids[j] = x_transposed[:, rng_empty_fallback.choice(n_samples_check)]\n",
        "        return centroids\n",
        "\n",
        "class CDKM_PurePy:\n",
        "    def __init__(self, X: np.ndarray, c_true: int, debug: int = 0):\n",
        "        self.X = X.astype(np.float64)  # shape (N, dim)\n",
        "        self.N, self.dim = self.X.shape\n",
        "        self.c_true = c_true\n",
        "        self.debug = debug\n",
        "\n",
        "        self.Y = []             # replicate list of label vectors\n",
        "        self.n_iter_ = []       # number of iterations per replicate\n",
        "\n",
        "        if debug:\n",
        "            print(f\"N = {self.N}, dim = {self.dim}, k = {self.c_true}\")\n",
        "\n",
        "    def opt(self, init_Y: np.ndarray, ITER: int):\n",
        "        \"\"\"\n",
        "        init_Y: (rep, N) array of integer labels\n",
        "        \"\"\"\n",
        "        rep = init_Y.shape[0]\n",
        "        for rep_i in range(rep):\n",
        "            y = init_Y[rep_i].copy()\n",
        "            n_iter = self.opt_once(y, ITER)\n",
        "            self.Y.append(y)\n",
        "            self.n_iter_.append(n_iter)\n",
        "\n",
        "    def opt_once(self, y: np.ndarray, ITER: int) -> int:\n",
        "        \"\"\"\n",
        "        y: shape (N,), initial cluster assignment\n",
        "        \"\"\"\n",
        "        X = self.X\n",
        "        N, dim, c_true = self.N, self.dim, self.c_true\n",
        "\n",
        "        xnorm = np.sum(X**2, axis=1)  # shape (N,)\n",
        "        Sx = np.zeros((dim, c_true))\n",
        "        n = np.zeros(c_true)\n",
        "\n",
        "        for i in range(N):\n",
        "            Sx[:, y[i]] += X[i]\n",
        "            n[y[i]] += 1\n",
        "\n",
        "        s = np.sum(Sx**2, axis=0)  # squared norm of each cluster sum vector\n",
        "\n",
        "        for iter in range(ITER):\n",
        "            converge = True\n",
        "            for i in range(N):\n",
        "                c_old = y[i]\n",
        "                if n[c_old] == 1:\n",
        "                    continue\n",
        "\n",
        "                xi = X[i]\n",
        "                xiSx = xi @ Sx  # (c,)\n",
        "                tmp1 = s + 2 * xiSx + xnorm[i]\n",
        "                tmp1 = tmp1 / (n + 1)\n",
        "                tmp2 = s / n\n",
        "\n",
        "                delta = tmp1 - tmp2\n",
        "                delta[c_old] = s[c_old] / n[c_old] - \\\n",
        "                    (s[c_old] - 2 * xiSx[c_old] + xnorm[i]) / (n[c_old] - 1)\n",
        "\n",
        "                c_new = np.argmax(delta)\n",
        "\n",
        "                if c_new != c_old:\n",
        "                    converge = False\n",
        "                    y[i] = c_new\n",
        "\n",
        "                    Sx[:, c_old] -= xi\n",
        "                    Sx[:, c_new] += xi\n",
        "\n",
        "                    s[c_old] = np.sum(Sx[:, c_old]**2)\n",
        "                    s[c_new] = np.sum(Sx[:, c_new]**2)\n",
        "\n",
        "                    n[c_old] -= 1\n",
        "                    n[c_new] += 1\n",
        "\n",
        "                if self.debug and i % 10000 == 0:\n",
        "                    print(f\"i = {i}\")\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"iter = {iter}\")\n",
        "\n",
        "            if converge:\n",
        "                break\n",
        "\n",
        "        # if iter + 1 == ITER:\n",
        "            # print(\"not converge\")\n",
        "\n",
        "        return iter + 1\n",
        "\n",
        "    @property\n",
        "    def y_pre(self):\n",
        "        return self.Y\n",
        "\n",
        "\n",
        "class CDKM:\n",
        "    def __init__(self, d, k, niter=200, nredo=10, verbose=False, seed=1234, debug=0):\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "        self.niter = niter\n",
        "        self.nredo = nredo\n",
        "        self.verbose = verbose\n",
        "        self.seed = seed\n",
        "        self.debug = debug\n",
        "        self.centroids = None\n",
        "        self.labels_ = None\n",
        "        self.Y_final_ = None\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.runtime_ = None\n",
        "        self.n_iter_ = None\n",
        "\n",
        "        self.normalized_entropy_ = None  # Normalized entropy\n",
        "        self.cluster_variance_ = None    # Cluster size variance\n",
        "\n",
        "    def train(self, x_orig_data, weights=None, init_centroids=None):\n",
        "        np.random.seed(self.seed)\n",
        "        start_time = time.time()\n",
        "\n",
        "        n, dim = x_orig_data.shape\n",
        "        if dim != self.d:\n",
        "            raise ValueError(f\"Data dimension {dim} does not match expected {self.d}.\")\n",
        "\n",
        "        if init_centroids is None:\n",
        "            init_Y = initial_Y(x_orig_data, self.k, self.nredo, \"random\")\n",
        "        else:\n",
        "            init_Y = init_centroids\n",
        "\n",
        "        model = CDKM_PurePy(x_orig_data, self.k, debug=self.debug)\n",
        "        model.opt(init_Y, ITER=self.niter)\n",
        "        Y = model.y_pre\n",
        "        self.n_iter_ = model.n_iter_\n",
        "\n",
        "        centroids = compute_cluster_centers_cdkm(x_orig_data, Y)\n",
        "        labels = np.argmax(one_hot(Y[0], self.k), axis=1)\n",
        "\n",
        "        # Compute SSE\n",
        "        sse = np.sum((x_orig_data - centroids[labels])**2)\n",
        "\n",
        "        # Compute balance loss\n",
        "        counts = np.bincount(labels, minlength=self.k)\n",
        "        ideal_size = n / self.k\n",
        "        balance_loss = np.sum((counts - ideal_size)**2)\n",
        "\n",
        "        self.Y_final_ = one_hot(Y[0], self.k)\n",
        "        self.centroids = centroids\n",
        "        self.labels_ = labels\n",
        "        self.sse_ = sse\n",
        "        self.balance_loss_ = balance_loss\n",
        "        self.runtime_ = time.time() - start_time\n",
        "\n",
        "        # Entropy & CV calculation\n",
        "        eps = 1e-10\n",
        "        l = n\n",
        "        c = self.k\n",
        "        size0 = np.bincount(self.labels_, minlength=c)\n",
        "\n",
        "        entro = 0.0\n",
        "        cv = 0.0\n",
        "        for jj in range(c):\n",
        "            frac = size0[jj] / l\n",
        "            entro += frac * np.log((size0[jj]+eps)/l)\n",
        "            cv += np.sqrt((size0[jj] - l / c) ** 2)\n",
        "\n",
        "        self.normalized_entropy_ = -entro / np.log(c)\n",
        "        self.cluster_variance_ = (c / l) * cv\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"CDKM finished in {self.runtime_:.4f}s; \"\n",
        "                  f\"SSE = {self.sse_:.4f}; \"\n",
        "                  f\"Balance Loss = {self.balance_loss_:.4f}; \"\n",
        "                  f\"Iterations = {self.n_iter_}\")\n",
        "\n",
        "class BKNC:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=False,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data (n_features)\n",
        "        # k: number of clusters (c in BKNC)\n",
        "        self.d_features = d\n",
        "        self.k = k  # c in BKNC\n",
        "        self.niter = niter # Niter in BKNC\n",
        "        self.lambda_ = lambda_ # lambda in BKNC\n",
        "        self.seed = seed\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Other Faiss Kmeans parameters - not directly used by BKNC logic\n",
        "        self.nredo = nredo\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu # BKNC as implemented here is CPU-only\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        # BKNC specific results\n",
        "        self.F_ = None          # The F matrix from BKNC (n_samples x k)\n",
        "        self.R_ = None          # The R matrix (k x k)\n",
        "        self.Y_ = None          # The Y matrix (one-hot labels, n_samples x k)\n",
        "        self.labels_ = None     # Final cluster assignments (idx, shape: n_samples)\n",
        "        self.obj_history_ = []  # History of the objective function trace(F'X_m'X_mF)\n",
        "        self.final_obj_ = None\n",
        "        self.runtime_ = 0\n",
        "\n",
        "        # For compatibility with original FCFC structure\n",
        "        self.centroids = None # Will be populated with cluster means\n",
        "        self.obj = None # Can store obj_history_ here\n",
        "\n",
        "        # Final metrics as requested\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.normalized_entropy_ = None  # Normalized entropy\n",
        "        self.cluster_variance_ = None    # Cluster size variance\n",
        "\n",
        "\n",
        "    def _initialize_Y_bknc(self, n_samples, c):\n",
        "        \"\"\"\n",
        "        Equivalent to MATLAB's init function for Y.\n",
        "        Creates an n_samples x c one-hot encoded matrix from random labels.\n",
        "        \"\"\"\n",
        "        # labels are 0 to c-1\n",
        "        # This internal seeding should be fine as long as the main train method sets the overall seed.\n",
        "        # If this method were called multiple times independently *within* one train call,\n",
        "        # and expected different Ys, then it would need a different seeding strategy.\n",
        "        # For now, it's called once per train.\n",
        "        labels = np.random.randint(0, c, size=n_samples)\n",
        "        Y = np.zeros((n_samples, c), dtype=int)\n",
        "        Y[np.arange(n_samples), labels] = 1\n",
        "        return Y\n",
        "\n",
        "    def _calculate_cluster_centroids(self, data, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates the mean of points in each cluster.\n",
        "        data: (n_samples, n_features)\n",
        "        labels: (n_samples,)\n",
        "        num_clusters: k\n",
        "        data_dim: d_features\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim))\n",
        "        if labels is None: # Should not happen if called after labels are set\n",
        "             if self.verbose:\n",
        "                print(\"Warning: Labels are None in _calculate_cluster_centroids. Returning zero centroids.\")\n",
        "             return centroids\n",
        "\n",
        "        for i in range(num_clusters):\n",
        "            cluster_points = data[labels == i]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[i] = np.mean(cluster_points, axis=0)\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {i} is empty during centroid calculation. Assigning a random data point as its centroid.\")\n",
        "                if len(data) > 0:\n",
        "                    # Use a random number generator seeded by self.seed for consistent fallback\n",
        "                    rng_fallback = np.random.RandomState(self.seed + i) # Add i for variety if multiple fallbacks\n",
        "                    centroids[i] = data[rng_fallback.choice(len(data))]\n",
        "                else: # No data points at all (edge case)\n",
        "                    centroids[i] = np.zeros(data_dim)\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Implements the BKNC algorithm.\n",
        "        x: data matrix (n_samples, n_features)\n",
        "        weights: Not used by BKNC.\n",
        "        init_centroids: Not used by BKNC.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility for the entire train method\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "\n",
        "        X_m = x.T  # (n_features, n_samples) - X_m is the MATLAB-like X\n",
        "        n_features_internal, n_samples = X_m.shape # n_features_internal is self.d_features\n",
        "        c = self.k # Number of clusters\n",
        "\n",
        "        # Initialize Y (n_samples, c)\n",
        "        # _initialize_Y_bknc uses np.random, which is now seeded by self.seed\n",
        "        Y = self._initialize_Y_bknc(n_samples, c)\n",
        "\n",
        "        # Initialize R (c, c) as a random orthogonal matrix\n",
        "        # np.random.rand is also affected by the global seed set above\n",
        "        R = orth(np.random.rand(c, c))\n",
        "\n",
        "        obj_log = np.zeros(self.niter)\n",
        "\n",
        "        # F_loop initialization is also seeded\n",
        "        for iter_num in range(self.niter):\n",
        "            F_loop = orth(np.random.rand(n_samples, c))\n",
        "            G = Y @ R.T\n",
        "\n",
        "            for _ in range(10):\n",
        "                TempM_F = X_m @ F_loop\n",
        "                M_calc_F = 2 * X_m.T @ TempM_F + self.lambda_ * G\n",
        "                U_f, _, Vh_f = np.linalg.svd(M_calc_F, full_matrices=False)\n",
        "                F_loop = U_f @ Vh_f\n",
        "            F_current = F_loop\n",
        "\n",
        "            N_calc_R = F_current.T @ Y\n",
        "            U_r, _, Vh_r = np.linalg.svd(N_calc_R, full_matrices=False)\n",
        "            R = U_r @ Vh_r\n",
        "\n",
        "            P_calc_Y = R.T @ F_current.T\n",
        "            idx = np.argmax(P_calc_Y, axis=0)\n",
        "            Y = np.zeros((n_samples, c), dtype=int)\n",
        "            Y[np.arange(n_samples), idx] = 1\n",
        "\n",
        "            TempF_obj = X_m @ F_current\n",
        "            obj_log[iter_num] = np.trace(TempF_obj.T @ TempF_obj)\n",
        "\n",
        "            if self.verbose and (iter_num % 5 == 0 or iter_num == self.niter -1):\n",
        "                print(f\"Iter {iter_num+1}/{self.niter}, BKNC Obj: {obj_log[iter_num]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time()\n",
        "\n",
        "        # Store BKNC results\n",
        "        self.F_ = F_current\n",
        "        self.R_ = R\n",
        "        self.Y_ = Y # This is the one-hot version of labels from the last iteration\n",
        "        self.labels_ = idx # finalInd in MATLAB (0-indexed labels)\n",
        "        self.obj_history_ = obj_log\n",
        "        self.final_obj_ = obj_log[-1]\n",
        "        self.obj = self.obj_history_ # Compatibility\n",
        "\n",
        "        # --- Calculate final centroids, SSE, and Balance Loss ---\n",
        "        # self.centroids are calculated based on original data `x` and final `self.labels_`\n",
        "        self.centroids = self._calculate_cluster_centroids(x, self.labels_, self.k, self.d_features)\n",
        "\n",
        "        # Calculate SSE\n",
        "        current_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n_samples):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point = x[i, :]\n",
        "                centroid_val = self.centroids[cluster_idx, :]\n",
        "                current_sse += np.sum((point - centroid_val)**2) # Squared Euclidean distance\n",
        "        self.sse_ = current_sse\n",
        "\n",
        "        # Calculate Balance Loss\n",
        "        current_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            cluster_sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal_size = n_samples / self.k\n",
        "            current_balance_loss = np.sum((cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = current_balance_loss\n",
        "\n",
        "        # Final runtime calculation\n",
        "        self.runtime_ = time.time() - start_time # Corrected runtime calculation\n",
        "\n",
        "        # Entropy & CV calculation\n",
        "        eps = 1e-10\n",
        "        l = n_samples\n",
        "        c = self.k\n",
        "        size0 = np.bincount(self.labels_, minlength=c)\n",
        "\n",
        "        entro = 0.0\n",
        "        cv = 0.0\n",
        "        for jj in range(c):\n",
        "            frac = size0[jj] / l\n",
        "            entro += frac * np.log((size0[jj]+eps)/l)\n",
        "            cv += np.sqrt((size0[jj] - l / c) ** 2)\n",
        "\n",
        "        self.normalized_entropy_ = -entro / np.log(c)\n",
        "        self.cluster_variance_ = (c / l) * cv\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BKNC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BKNC objective (trace): {self.final_obj_:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of calculated centroids: {self.centroids.shape}\")\n",
        "            print(f\"Final SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "class MyKMeans:\n",
        "    def __init__(self, d, k, niter=25, nredo=1, verbose=False,\n",
        "                    min_points_per_centroid=1, max_points_per_centroid=1e9,\n",
        "                    seed=1234, gpu=False, spherical=False,\n",
        "                    update_index=True, frozen_centroids=False,\n",
        "                    lambda_=1.0):\n",
        "        # d: dimensionality of data (n_features)\n",
        "        # k: number of clusters (c in BKNC)\n",
        "        self.d_features = d\n",
        "        self.k = k  # c in BKNC\n",
        "        self.niter = niter # Niter in BKNC\n",
        "        self.lambda_ = lambda_ # lambda in BKNC\n",
        "        self.lambda_reformed = (1-lambda_)/lambda_\n",
        "        self.seed = seed\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Other Faiss Kmeans parameters - not directly used by BKNC logic\n",
        "        self.nredo = nredo\n",
        "        self.min_points_per_centroid = min_points_per_centroid\n",
        "        self.max_points_per_centroid = max_points_per_centroid\n",
        "        self.gpu = gpu # BKNC as implemented here is CPU-only\n",
        "        self.spherical = spherical\n",
        "        self.update_index = update_index\n",
        "        self.frozen_centroids = frozen_centroids\n",
        "\n",
        "        # BKNC specific results\n",
        "        self.F_ = None          # The F matrix from BKNC (n_samples x k)\n",
        "        self.R_ = None          # The R matrix (k x k)\n",
        "        self.Y_ = None          # The Y matrix (one-hot labels, n_samples x k)\n",
        "        self.labels_ = None     # Final cluster assignments (idx, shape: n_samples)\n",
        "        self.obj_history_ = []  # History of the objective function trace(F'X_m'X_mF)\n",
        "        self.final_obj_ = None\n",
        "        self.runtime_ = 0\n",
        "\n",
        "        # For compatibility with original FCFC structure\n",
        "        self.centroids = None # Will be populated with cluster means\n",
        "        self.obj = None # Can store obj_history_ here\n",
        "\n",
        "        # Final metrics as requested\n",
        "        self.sse_ = None\n",
        "        self.balance_loss_ = None\n",
        "        self.normalized_entropy_ = None  # Normalized entropy\n",
        "        self.cluster_variance_ = None    # Cluster size variance\n",
        "\n",
        "\n",
        "    def _initialize_Y_bknc(self, n_samples, c):\n",
        "        \"\"\"\n",
        "        Equivalent to MATLAB's init function for Y.\n",
        "        Creates an n_samples x c one-hot encoded matrix from random labels.\n",
        "        \"\"\"\n",
        "        # labels are 0 to c-1\n",
        "        # This internal seeding should be fine as long as the main train method sets the overall seed.\n",
        "        # If this method were called multiple times independently *within* one train call,\n",
        "        # and expected different Ys, then it would need a different seeding strategy.\n",
        "        # For now, it's called once per train.\n",
        "        labels = np.random.randint(0, c, size=n_samples)\n",
        "        Y = np.zeros((n_samples, c), dtype=int)\n",
        "        Y[np.arange(n_samples), labels] = 1\n",
        "        return Y\n",
        "\n",
        "    def _calculate_cluster_centroids(self, data, labels, num_clusters, data_dim):\n",
        "        \"\"\"\n",
        "        Calculates the mean of points in each cluster.\n",
        "        data: (n_samples, n_features)\n",
        "        labels: (n_samples,)\n",
        "        num_clusters: k\n",
        "        data_dim: d_features\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((num_clusters, data_dim))\n",
        "        if labels is None: # Should not happen if called after labels are set\n",
        "             if self.verbose:\n",
        "                print(\"Warning: Labels are None in _calculate_cluster_centroids. Returning zero centroids.\")\n",
        "             return centroids\n",
        "\n",
        "        for i in range(num_clusters):\n",
        "            cluster_points = data[labels == i]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[i] = np.mean(cluster_points, axis=0)\n",
        "            else:\n",
        "                if self.verbose:\n",
        "                    print(f\"Warning: Cluster {i} is empty during centroid calculation. Assigning a random data point as its centroid.\")\n",
        "                if len(data) > 0:\n",
        "                    # Use a random number generator seeded by self.seed for consistent fallback\n",
        "                    rng_fallback = np.random.RandomState(self.seed + i) # Add i for variety if multiple fallbacks\n",
        "                    centroids[i] = data[rng_fallback.choice(len(data))]\n",
        "                else: # No data points at all (edge case)\n",
        "                    centroids[i] = np.zeros(data_dim)\n",
        "        return centroids\n",
        "\n",
        "    def train(self, x, weights=None, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Implements the BKNC algorithm.\n",
        "        x: data matrix (n_samples, n_features)\n",
        "        weights: Not used by BKNC.\n",
        "        init_centroids: Not used by BKNC.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed) # Set seed for reproducibility for the entire train method\n",
        "        start_time = time.time()\n",
        "\n",
        "        if x.shape[1] != self.d_features:\n",
        "            raise ValueError(f\"Input data feature dimension {x.shape[1]} \"\n",
        "                             f\"does not match class initialized dimension {self.d_features}\")\n",
        "\n",
        "        X_m = x.T  # (n_features, n_samples) - X_m is the MATLAB-like X\n",
        "        n_features_internal, n_samples = X_m.shape # n_features_internal is self.d_features\n",
        "        c = self.k # Number of clusters\n",
        "\n",
        "        # Initialize Y (n_samples, c)\n",
        "        # _initialize_Y_bknc uses np.random, which is now seeded by self.seed\n",
        "        Y = self._initialize_Y_bknc(n_samples, c)\n",
        "\n",
        "        # Initialize R (c, c) as a random orthogonal matrix\n",
        "        # np.random.rand is also affected by the global seed set above\n",
        "        R = orth(np.random.rand(c, c))\n",
        "\n",
        "        obj_log = np.zeros(self.niter)\n",
        "\n",
        "        # F_loop initialization is also seeded\n",
        "        for iter_num in range(self.niter):\n",
        "            F_loop = orth(np.random.rand(n_samples, c))\n",
        "            G = Y @ R.T\n",
        "\n",
        "            for _ in range(10):\n",
        "                TempM_F = X_m @ F_loop\n",
        "                M_calc_F = 2 * X_m.T @ TempM_F + self.lambda_reformed * G\n",
        "                U_f, _, Vh_f = np.linalg.svd(M_calc_F, full_matrices=False)\n",
        "                F_loop = U_f @ Vh_f\n",
        "            F_current = F_loop\n",
        "\n",
        "            N_calc_R = F_current.T @ Y\n",
        "            U_r, _, Vh_r = np.linalg.svd(N_calc_R, full_matrices=False)\n",
        "            R = U_r @ Vh_r\n",
        "\n",
        "            P_calc_Y = R.T @ F_current.T\n",
        "            idx = np.argmax(P_calc_Y, axis=0)\n",
        "            Y = np.zeros((n_samples, c), dtype=int)\n",
        "            Y[np.arange(n_samples), idx] = 1\n",
        "\n",
        "            TempF_obj = X_m @ F_current\n",
        "            obj_log[iter_num] = np.trace(TempF_obj.T @ TempF_obj)\n",
        "\n",
        "            if self.verbose and (iter_num % 5 == 0 or iter_num == self.niter -1):\n",
        "                print(f\"Iter {iter_num+1}/{self.niter}, BKNC Obj: {obj_log[iter_num]:.4f}\")\n",
        "\n",
        "        self.runtime_ = time.time()\n",
        "\n",
        "        # Store BKNC results\n",
        "        self.F_ = F_current\n",
        "        self.R_ = R\n",
        "        self.Y_ = Y # This is the one-hot version of labels from the last iteration\n",
        "        self.labels_ = idx # finalInd in MATLAB (0-indexed labels)\n",
        "        self.obj_history_ = obj_log\n",
        "        self.final_obj_ = obj_log[-1]\n",
        "        self.obj = self.obj_history_ # Compatibility\n",
        "\n",
        "        # --- Calculate final centroids, SSE, and Balance Loss ---\n",
        "        # self.centroids are calculated based on original data `x` and final `self.labels_`\n",
        "        self.centroids = self._calculate_cluster_centroids(x, self.labels_, self.k, self.d_features)\n",
        "\n",
        "        # Calculate SSE\n",
        "        current_sse = 0\n",
        "        if self.labels_ is not None and self.centroids is not None:\n",
        "            for i in range(n_samples):\n",
        "                cluster_idx = self.labels_[i]\n",
        "                point = x[i, :]\n",
        "                centroid_val = self.centroids[cluster_idx, :]\n",
        "                current_sse += np.sum((point - centroid_val)**2) # Squared Euclidean distance\n",
        "        self.sse_ = current_sse\n",
        "\n",
        "        # Calculate Balance Loss\n",
        "        current_balance_loss = 0\n",
        "        if self.labels_ is not None:\n",
        "            cluster_sizes = np.bincount(self.labels_, minlength=self.k)\n",
        "            ideal_size = n_samples / self.k\n",
        "            current_balance_loss = np.sum((cluster_sizes - ideal_size)**2)\n",
        "        self.balance_loss_ = current_balance_loss\n",
        "\n",
        "        # Final runtime calculation\n",
        "        self.runtime_ = time.time() - start_time # Corrected runtime calculation\n",
        "\n",
        "        # Entropy & CV calculation\n",
        "        eps = 1e-10\n",
        "        l = n_samples\n",
        "        c = self.k\n",
        "        size0 = np.bincount(self.labels_, minlength=c)\n",
        "\n",
        "        entro = 0.0\n",
        "        cv = 0.0\n",
        "        for jj in range(c):\n",
        "            frac = size0[jj] / l\n",
        "            entro += frac * np.log((size0[jj]+eps)/l)\n",
        "            cv += np.sqrt((size0[jj] - l / c) ** 2)\n",
        "\n",
        "        self.normalized_entropy_ = -entro / np.log(c)\n",
        "        self.cluster_variance_ = (c / l) * cv\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"BKNC training completed in {self.runtime_:.4f} seconds.\")\n",
        "            print(f\"Final BKNC objective (trace): {self.final_obj_:.4f}\")\n",
        "            unique_labels_final, counts_final = np.unique(self.labels_, return_counts=True)\n",
        "            print(f\"Final cluster sizes: {dict(zip(unique_labels_final, counts_final))}\")\n",
        "            if self.centroids is not None:\n",
        "                print(f\"Shape of calculated centroids: {self.centroids.shape}\")\n",
        "            print(f\"Final SSE: {self.sse_:.4f}\")\n",
        "            print(f\"Final Balance Loss: {self.balance_loss_:.4f}\")\n",
        "\n",
        "# Helper functions (moved outside the class, or could be static methods)\n",
        "def initial_Y(X, c, rep, way=\"random\"):\n",
        "        N = X.shape[0]\n",
        "        Y = np.zeros((rep, N), dtype=np.int32)\n",
        "\n",
        "        if way == \"random\":\n",
        "            for rep_i in range(rep):\n",
        "                Y[rep_i] = np.random.randint(0, c, N)\n",
        "\n",
        "        elif way == \"k-means++\":\n",
        "            for rep_i in range(rep):\n",
        "                Y[rep_i] = KMeans(n_clusters=c, init=\"k-means++\", n_init=1, max_iter=1).fit(X).labels_\n",
        "\n",
        "        else:\n",
        "            assert 2 == 1\n",
        "\n",
        "        return Y\n",
        "def one_hot(y: np.ndarray, k: int):\n",
        "    n = len(y)\n",
        "    Y = np.zeros((n, k), dtype=np.float32)\n",
        "    Y[np.arange(n), y] = 1.0\n",
        "    return Y\n",
        "def compute_cluster_centers_cdkm(X, Y):\n",
        "    \"\"\"\n",
        "    X: (n, d)\n",
        "    Y: list of cluster label arrays, each of shape (n,)\n",
        "    \"\"\"\n",
        "    y = Y[0]  # shape (n,)\n",
        "    n, k = X.shape[0], np.max(y) + 1\n",
        "    Y0 = np.zeros((n, k), dtype=np.float64)\n",
        "    Y0[np.arange(n), y] = 1.0  # one-hot\n",
        "\n",
        "    weights = np.sum(Y0, axis=0)  # (k,)\n",
        "    weights[weights == 0] = 1e-10\n",
        "\n",
        "    centers = (Y0.T @ X) / weights[:, None]\n",
        "    return centers\n",
        "\n",
        "def get_centroid(data, label, K, n, d_features):\n",
        "    \"\"\"\n",
        "    Update centroids after the assignment phase.\n",
        "    data: (n, d_features)\n",
        "    label: (n,)\n",
        "    K: number of clusters\n",
        "    n: number of samples\n",
        "    d_features: number of features\n",
        "    \"\"\"\n",
        "    centroids = np.zeros((K, d_features))\n",
        "    for k_idx in range(K):\n",
        "        members = (label == k_idx)\n",
        "        if np.any(members):\n",
        "            # Np.sum on boolean array members gives count of True values\n",
        "            centroids[k_idx, :] = np.sum(data[members, :], axis=0) / np.sum(members)\n",
        "        else:\n",
        "            # Handle empty cluster: assign a random point from data\n",
        "            # This random choice is now affected by the seed set in train()\n",
        "            if n > 0 : # Ensure data is not empty\n",
        "                 centroids[k_idx, :] = data[np.random.choice(n), :]\n",
        "            # else: centroid remains zeros if data is empty (edge case)\n",
        "    return centroids\n",
        "\n",
        "\n",
        "def get_distance(data, centroids, K, n, d_features, size_cluster, lambda_param):\n",
        "    \"\"\"\n",
        "    Objective function term for assignment:\n",
        "    D(i,j) = distance(i-th data point, j-th centroid)^2 + lambda_param * size_of_jth_cluster\n",
        "    data: (n, d_features)\n",
        "    centroids: (K, d_features)\n",
        "    size_cluster: (K,) - current size of each cluster\n",
        "    lambda_param: balance weight\n",
        "    Returns: D_matrix (n, K)\n",
        "    \"\"\"\n",
        "    D_matrix = np.zeros((n, K))\n",
        "    for k_idx in range(K):\n",
        "        # Squared Euclidean distance\n",
        "        dist_sq = np.sum((data - centroids[k_idx, :])**2, axis=1)\n",
        "        D_matrix[:, k_idx] = dist_sq + lambda_param * size_cluster[k_idx]\n",
        "    return D_matrix\n",
        "\n",
        "\n",
        "def initial_centroid(x_data, K, n_samples):\n",
        "    \"\"\"\n",
        "    Initialize centroids randomly by choosing K unique points from the data.\n",
        "    x_data: (n_samples, d_features)\n",
        "    K: number of clusters\n",
        "    n_samples: number of samples\n",
        "    \"\"\"\n",
        "    if K > n_samples:\n",
        "        raise ValueError(\"K (number of clusters) cannot be greater than n_samples.\")\n",
        "    # This random choice is now affected by the seed set in train()\n",
        "    indices = np.random.choice(n_samples, K, replace=False)\n",
        "    return x_data[indices, :]\n",
        "\n",
        "# 优化后的数据加载函数\n",
        "def load_data_chunked(path, dtype='float32', chunksize=1000):\n",
        "    \"\"\"分块加载大数据集避免内存溢出\"\"\"\n",
        "    chunks = []\n",
        "    for chunk in pd.read_csv(path, header=None, chunksize=chunksize):\n",
        "        chunks.append(chunk.astype(dtype))\n",
        "    return np.concatenate(chunks, axis=0)\n",
        "\n",
        "def run_experiment(model_class, model_name, dataset_path, dimensions, n_clusters, n_runs=1):\n",
        "    \"\"\"运行实验并返回 CV 和 Entropy\"\"\"\n",
        "    try:\n",
        "        X_data = load_data_chunked(dataset_path)\n",
        "        for run in range(n_runs):\n",
        "            if model_name == \"FCFC\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, lambda_=0.1, seed=1234+run, verbose=False)\n",
        "            elif model_name == \"BCLS\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, lambda_=0.1, seed=1234+run, verbose=False)\n",
        "            elif model_name == \"Lloyd\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, seed=1234+run, verbose=False)\n",
        "            elif model_name == \"CDKM\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, seed=1234+run, verbose=False)\n",
        "            elif model_name == \"BKNC\":\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, lambda_=0.1, seed=1234+run, verbose=False)\n",
        "            else:\n",
        "                model = model_class(d=dimensions, k=n_clusters, niter=10, lambda_=0.1, seed=1234+run, verbose=False)\n",
        "\n",
        "            model.train(X_data) if hasattr(model, 'train') else model.fit(X_data)\n",
        "\n",
        "        # 返回三元组：方法名、CV、熵\n",
        "        return model.cluster_variance_, model.normalized_entropy_\n",
        "\n",
        "    finally:\n",
        "        if 'X_data' in locals(): del X_data\n",
        "        if 'model' in locals(): del model\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 模型列表\n",
        "    models = [\n",
        "        (Lloyd, \"Lloyd\"),\n",
        "        (CDKM, \"CDKM\"),\n",
        "        (BCLS, \"BCLS\"),\n",
        "        (FCFC, \"FCFC\"),\n",
        "        (BKNC, \"BKNC\"),\n",
        "        (MyKMeans, \"MyKMeans\")\n",
        "    ]\n",
        "\n",
        "    # 数据集路径和维度\n",
        "    datasets = [\n",
        "        (\"/content/sample_data/Huatuo_1024d_10k.csv\", 1024),\n",
        "        (\"/content/sample_data/LiveChat_1024d_10k.csv\", 1024),\n",
        "        (\"/content/sample_data/deep_96d_10k.csv\", 96),\n",
        "        (\"/content/sample_data/glove_300d_10k.csv\", 300),\n",
        "        (\"/content/sample_data/sift_128d_10k.csv\", 128)\n",
        "    ]\n",
        "\n",
        "    k = 10  # 固定聚类数\n",
        "    records = []  # 存储结果\n",
        "\n",
        "    for dataset_path, dim in datasets:\n",
        "        dataset_name = Path(dataset_path).stem.split(\"-\")[0]\n",
        "        for model_class, model_name in models:\n",
        "            print(f\"Running {model_name} on {dataset_name} (d={dim}, k={k})...\")\n",
        "            cv, entro = run_experiment(model_class, model_name, dataset_path, dim, k)\n",
        "            records.append([dataset_name, model_name, cv, entro])\n",
        "\n",
        "    # 保存为 CSV 文件\n",
        "    results_df = pd.DataFrame(records, columns=[\"Dataset\", \"Method\", \"CV\", \"Entropy\"])\n",
        "    results_df.to_csv('cv_entropy_results.csv', index=False)\n",
        "    print(\"\\nSaved CV & Entropy results to 'cv_entropy_results.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyYb8dLePwkx",
        "outputId": "1ef964d9-ee1e-4d85-b685-a747ca18fdad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Running Lloyd on Huatuo (d=1024, k=10)...\n",
            "Running CDKM on Huatuo (d=1024, k=10)...\n",
            "Running BCLS on Huatuo (d=1024, k=10)...\n",
            "Running FCFC on Huatuo (d=1024, k=10)...\n",
            "Running BKNC on Huatuo (d=1024, k=10)...\n",
            "Running MyKMeans on Huatuo (d=1024, k=10)...\n",
            "Running Lloyd on LiveChat (d=1024, k=10)...\n",
            "Running CDKM on LiveChat (d=1024, k=10)...\n",
            "Running BCLS on LiveChat (d=1024, k=10)...\n",
            "Running FCFC on LiveChat (d=1024, k=10)...\n",
            "Running BKNC on LiveChat (d=1024, k=10)...\n",
            "Running MyKMeans on LiveChat (d=1024, k=10)...\n",
            "Running Lloyd on deep_96d_10k (d=96, k=10)...\n",
            "Running CDKM on deep_96d_10k (d=96, k=10)...\n",
            "Running BCLS on deep_96d_10k (d=96, k=10)...\n",
            "Running FCFC on deep_96d_10k (d=96, k=10)...\n",
            "Running BKNC on deep_96d_10k (d=96, k=10)...\n",
            "Running MyKMeans on deep_96d_10k (d=96, k=10)...\n",
            "Running Lloyd on glove_300d_10k (d=300, k=10)...\n",
            "Running CDKM on glove_300d_10k (d=300, k=10)...\n",
            "Running BCLS on glove_300d_10k (d=300, k=10)...\n",
            "Running FCFC on glove_300d_10k (d=300, k=10)...\n",
            "Running BKNC on glove_300d_10k (d=300, k=10)...\n",
            "Running MyKMeans on glove_300d_10k (d=300, k=10)...\n",
            "Running Lloyd on sift_128d_10k (d=128, k=10)...\n",
            "Running CDKM on sift_128d_10k (d=128, k=10)...\n",
            "Running BCLS on sift_128d_10k (d=128, k=10)...\n",
            "Running FCFC on sift_128d_10k (d=128, k=10)...\n",
            "Running BKNC on sift_128d_10k (d=128, k=10)...\n",
            "Running MyKMeans on sift_128d_10k (d=128, k=10)...\n",
            "\n",
            "Saved CV & Entropy results to 'cv_entropy_results.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1️⃣ 读 CSV\n",
        "df = pd.read_csv('cv_entropy_results.csv')\n",
        "\n",
        "# 2️⃣ 提取原始 Dataset 和 Model 的顺序\n",
        "df['Dataset_raw'] = df['Dataset'].str.split('-').str[0].str.split('_').str[0]\n",
        "df['Model_raw'] = df['Method']\n",
        "\n",
        "# 3️⃣ 替换 Dataset 和 Model 名\n",
        "df['Dataset'] = df['Dataset_raw'].replace({\n",
        "    'deep': 'Deep',\n",
        "    'sift': 'SIFT',\n",
        "    'glove': 'GloVe'\n",
        "})\n",
        "df['Method'] = df['Model_raw'].replace({'MyKMeans': 'Tub-means'})\n",
        "\n",
        "# 4️⃣ 用 CSV 中 Dataset 出现顺序定义 Categorical\n",
        "dataset_order = df.drop_duplicates('Dataset_raw')['Dataset'].tolist()\n",
        "df['Dataset'] = pd.Categorical(df['Dataset'], categories=dataset_order, ordered=True)\n",
        "\n",
        "# 5️⃣ 保留原行顺序标记（保证 Model 顺序）\n",
        "df['row_order'] = np.arange(len(df))\n",
        "\n",
        "# 6️⃣ 按 Dataset、原行号 排序\n",
        "df = df.sort_values(['Dataset', 'row_order']).drop(columns=['row_order', 'Dataset_raw', 'Model_raw'])\n",
        "\n",
        "# ✅ 从这里开始是你原来的生成表格逻辑\n",
        "def format_row(dataset, model, cv, entro, last=False, multirow=False, multirow_count=None):\n",
        "    cv = \"{:.5f}\".format(cv)\n",
        "    entro = \"{:.5f}\".format(entro)\n",
        "\n",
        "    if multirow:\n",
        "        row = f\"\\\\multirow{{{multirow_count}}}{{*}}{{\\\\ {dataset}}}\\n& \\\\ {model} & {cv} & {entro} & 0 \\\\\\\\\"\n",
        "    elif last:\n",
        "        row = f\"& \\\\ {model} & {cv} & {entro} & 0 \\\\\\\\ \\\\midrule \\n\"\n",
        "    else:\n",
        "        row = f\"& \\\\ {model} & {cv} & {entro} & 0 \\\\\\\\\"\n",
        "    return row\n",
        "\n",
        "# 7️⃣ 分块生成\n",
        "lines = []\n",
        "for dataset, group in df.groupby('Dataset'):\n",
        "    models = group['Method'].tolist()\n",
        "    cv = group['CV'].tolist()\n",
        "    entro = group['Entropy'].tolist()\n",
        "\n",
        "    for i in range(len(models)):\n",
        "        is_first = (i == 0)\n",
        "        is_last = (i == len(models) - 1)  # 注意这里用实际长度\n",
        "        line = format_row(dataset, models[i], cv[i], entro[i],\n",
        "                          last=is_last, multirow=is_first, multirow_count=len(models) if is_first else None)\n",
        "        lines.append(line)\n",
        "\n",
        "# 8️⃣ 输出\n",
        "latex_table_body = \"\\n\".join(lines)\n",
        "print(latex_table_body)\n",
        "\n",
        "with open(\"latex_table_rows.tex\", \"w\") as f:\n",
        "    f.write(latex_table_body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWUvyBBgTX0r",
        "outputId": "38b0fbfa-5f8b-4471-d0ab-587b0564bb17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\multirow{6}{*}{\\ Huatuo}\n",
            "& \\ Lloyd & 2.42516 & 0.98232 & 0 \\\\\n",
            "& \\ CDKM & 3.25267 & 0.96677 & 0 \\\\\n",
            "& \\ BCLS & 4.75672 & 0.92561 & 0 \\\\\n",
            "& \\ FCFC & 18.00000 & -0.00000 & 0 \\\\\n",
            "& \\ BKNC & 1.63684 & 0.99057 & 0 \\\\\n",
            "& \\ Tub-means & 0.57654 & 0.99740 & 0 \\\\ \\midrule \n",
            "\n",
            "\\multirow{6}{*}{\\ LiveChat}\n",
            "& \\ Lloyd & 2.50875 & 0.97836 & 0 \\\\\n",
            "& \\ CDKM & 3.46105 & 0.96303 & 0 \\\\\n",
            "& \\ BCLS & 2.44696 & 0.97806 & 0 \\\\\n",
            "& \\ FCFC & 18.00000 & -0.00000 & 0 \\\\\n",
            "& \\ BKNC & 0.90911 & 0.99742 & 0 \\\\\n",
            "& \\ Tub-means & 0.17878 & 0.99990 & 0 \\\\ \\midrule \n",
            "\n",
            "\\multirow{6}{*}{\\ Deep}\n",
            "& \\ Lloyd & 3.28267 & 0.96603 & 0 \\\\\n",
            "& \\ CDKM & 2.62874 & 0.97487 & 0 \\\\\n",
            "& \\ BCLS & 12.00000 & 0.54968 & 0 \\\\\n",
            "& \\ FCFC & 18.00000 & -0.00000 & 0 \\\\\n",
            "& \\ BKNC & 1.03890 & 0.99533 & 0 \\\\\n",
            "& \\ Tub-means & 0.62294 & 0.99871 & 0 \\\\ \\midrule \n",
            "\n",
            "\\multirow{6}{*}{\\ GloVe}\n",
            "& \\ Lloyd & 2.42916 & 0.97282 & 0 \\\\\n",
            "& \\ CDKM & 2.66893 & 0.97861 & 0 \\\\\n",
            "& \\ BCLS & 11.15208 & 0.63001 & 0 \\\\\n",
            "& \\ FCFC & 10.18638 & 0.61696 & 0 \\\\\n",
            "& \\ BKNC & 0.94871 & 0.99594 & 0 \\\\\n",
            "& \\ Tub-means & 0.98290 & 0.99692 & 0 \\\\ \\midrule \n",
            "\n",
            "\\multirow{6}{*}{\\ SIFT}\n",
            "& \\ Lloyd & 1.75502 & 0.99068 & 0 \\\\\n",
            "& \\ CDKM & 1.24928 & 0.99488 & 0 \\\\\n",
            "& \\ BCLS & 13.72203 & 0.42643 & 0 \\\\\n",
            "& \\ FCFC & 1.41126 & 0.99323 & 0 \\\\\n",
            "& \\ BKNC & 1.11869 & 0.99540 & 0 \\\\\n",
            "& \\ Tub-means & 1.12469 & 0.99537 & 0 \\\\ \\midrule \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-233831544.py:44: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  for dataset, group in df.groupby('Dataset'):\n"
          ]
        }
      ]
    }
  ]
}